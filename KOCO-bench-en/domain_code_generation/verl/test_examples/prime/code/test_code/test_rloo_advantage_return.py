import unittest
import torch
import sys
import os
from unittest.mock import MagicMock

# Add the recipe directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'recipe'))

# Mock verl modules since they might not be available in test environment
class MockDataProto:
    def __init__(self):
        self.batch = {}

class MockVerlF:
    @staticmethod
    def masked_whiten(advantages, mask):
        # Implement masked whitening as expected by ground-truth code
        # Ensure advantages is float type for mean/std calculations
        advantages = advantages.float()
        masked_adv = advantages[mask.bool()]
        if len(masked_adv) > 0:
            mean = masked_adv.mean()
            std = masked_adv.std()
            if std > 1e-8:
                advantages_normalized = (advantages - mean) / std
            else:
                advantages_normalized = advantages - mean
            return advantages_normalized
        return advantages

# Create a mock verl module with proper structure
mock_verl = MagicMock()
mock_verl.DataProto = MockDataProto
mock_verl.utils = MagicMock()
mock_verl.utils.torch_functional = MockVerlF()

# Set up mocks before importing
sys.modules['verl'] = mock_verl
sys.modules['verl.utils'] = mock_verl.utils
sys.modules['verl.utils.torch_functional'] = MockVerlF()

# Import ground-truth function
from prime.prime_core_algos import compute_rloo_advantage_return


class TestRLOOAdvantageReturn(unittest.TestCase):
    """Test compute_rloo_advantage_return function - RLOO advantage estimation described in Section 4 of documentation"""
    
    def setUp(self):
        torch.manual_seed(42)
    
    def create_test_data(self, batch_size=4, seq_len=10, n_samples=2, include_acc=True):
        """Create standard test data that fully matches the format expected by ground-truth code"""
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.tensor([[0.1,0.3,1.0,-0.1,0.7,0.5,0.2,-0.45,0.1,0.2], [0.2,0.3,0.2,-0.4,0.9,-0.1,0.2,0.4,0.6,-0.8], [-0.1,0.3,0.6,-0.1,0.2,0.7,0.1,0.2,-0.4,0.6], [0.1,0.8,0.6,-0.1,0.2,0.5,0.1,0.2,-0.4,0.6]]),
            "prompts": torch.ones(batch_size, 5, dtype=torch.long),
            "attention_mask": torch.ones(batch_size, seq_len + 5, dtype=torch.long),
        }

        if include_acc:
            data.batch["acc"] = torch.tensor([1.0, 0.0, 0.8, 0.2])
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        
        # Create config with hierarchical structure matching ground-truth implementation
        config = MagicMock()
        config.algorithm.reward_dpo_coef = 1.0
        config.algorithm.reward_gt_coef = 0.5 if include_acc else 0.0
        
        return data, response_mask, config
    
    def test_compute_rloo_advantage_return_rm_only(self):
        """Test RLOO advantage computation using only rm_scores"""
        batch_size = 4
        seq_len = 10
        n_samples = 2
        
        data, response_mask, config = self.create_test_data(
            batch_size, seq_len, n_samples, include_acc=False
        )
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages)
        # print(returns)

        expected_advantages = torch.tensor([[ 1.871540665626525878906250000000,  2.133205175399780273437500000000,
          1.884304761886596679687500000000, -0.151573851704597473144531250000,
          0.620656013488769531250000000000, -0.649374842643737792968750000000,
         -1.408840775489807128906250000000, -1.402458548545837402343750000000,
          0.263260245323181152343750000000,  0.524924874305725097656250000000],
        [-0.808926701545715332031250000000, -0.802544653415679931640625000000,
         -1.051445245742797851562500000000, -1.045063138008117675781250000000,
          0.493014484643936157226562500000, -1.287581443786621093750000000000,
         -0.515351712703704833984375000000, -0.508969664573669433593750000000,
         -1.013152718544006347656250000000, -2.027901172637939453125000000000],
        [-0.106899671256542205810546875000,  0.748296976089477539062500000000,
          0.582363307476043701171875000000, -0.349418133497238159179687500000,
          0.505778551101684570312500000000,  0.595127463340759277343750000000,
         -0.591936409473419189453125000000, -0.247304961085319519042968750000,
         -0.157955989241600036621093750000,  1.463088274002075195312500000000],
        [ 1.169513225555419921875000000000,  1.514144778251647949218750000000,
          0.071798197925090789794921875000, -0.859983325004577636718750000000,
         -0.004786581732332706451416015625,  0.084562353789806365966796875000,
         -0.591936409473419189453125000000, -0.247304961085319519042968750000,
         -0.157955989241600036621093750000,  1.463088274002075195312500000000]])

        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[ 1.050000190734863281250000000000,  1.255000114440917968750000000000,
          1.060000061988830566406250000000, -0.534999907016754150390625000000,
          0.070000097155570983886718750000, -0.924999892711639404296875000000,
         -1.519999980926513671875000000000, -1.514999985694885253906250000000,
         -0.209999993443489074707031250000, -0.004999995231628417968750000000],
        [-1.050000071525573730468750000000, -1.045000076293945312500000000000,
         -1.240000128746032714843750000000, -1.235000133514404296875000000000,
         -0.030000060796737670898437500000, -1.425000071525573730468750000000,
         -0.820000052452087402343750000000, -0.815000057220458984375000000000,
         -1.210000038146972656250000000000, -2.005000114440917968750000000000],
        [-0.500000119209289550781250000000,  0.169999897480010986328125000000,
          0.039999902248382568359375000000, -0.690000116825103759765625000000,
         -0.020000100135803222656250000000,  0.049999922513961791992187500000,
         -0.879999995231628417968750000000, -0.610000014305114746093750000000,
         -0.539999961853027343750000000000,  0.730000019073486328125000000000],
        [ 0.499999880790710449218750000000,  0.769999921321868896484375000000,
         -0.360000073909759521484375000000, -1.090000152587890625000000000000,
         -0.420000076293945312500000000000, -0.350000053644180297851562500000,
         -0.879999995231628417968750000000, -0.610000014305114746093750000000,
         -0.539999961853027343750000000000,  0.730000019073486328125000000000]])
        # Verify output shape

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        
        # Verify whitening property: advantages should be whitened
        masked_advantages = advantages[response_mask]
        mean_val = masked_advantages.mean().item()
        std_val = masked_advantages.std().item()
        
        # Verify whitening effect
        self.assertAlmostEqual(mean_val, 0.0, places=5)
        self.assertAlmostEqual(std_val, 1.0, places=5)
    
    def test_compute_rloo_advantage_return_with_accuracy(self):
        """Test RLOO advantage computation using both rm_scores and accuracy"""
        batch_size = 4
        seq_len = 10
        n_samples = 2
        
        data, response_mask, config = self.create_test_data(
            batch_size, seq_len, n_samples, include_acc=True
        )
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages)
        # print(returns)

        expected_advantages = torch.tensor([[ 1.933372497558593750000000000000,  2.134944677352905273437500000000,
          1.943205356597900390625000000000,  0.374875277280807495117187500000,
          0.969759106636047363281250000000, -0.008603536523878574371337890625,
         -0.593654632568359375000000000000, -0.588738262653350830078125000000,
          0.694440901279449462890625000000,  0.896013081073760986328125000000],
        [-1.114792585372924804687500000000, -1.109876275062561035156250000000,
         -1.301615595817565917968750000000, -1.296699404716491699218750000000,
         -0.111848026514053344726562500000, -1.483522176742553710937500000000,
         -0.888638436794281005859375000000, -0.883722007274627685546875000000,
         -1.272117257118225097656250000000, -2.053824186325073242187500000000],
        [ 0.212633967399597167968750000000,  0.871430933475494384765625000000,
          0.743604660034179687500000000000,  0.025810962542891502380371093750,
          0.684607923030853271484375000000,  0.753437459468841552734375000000,
         -0.161011934280395507812500000000,  0.104473412036895751953125000000,
          0.173302963376045227050781250000,  1.422067284584045410156250000000],
        [ 0.605945646762847900390625000000,  0.871430993080139160156250000000,
         -0.239674299955368041992187500000, -0.957467973232269287109375000000,
         -0.298671036958694458007812500000, -0.229841515421867370605468750000,
         -0.750979363918304443359375000000, -0.485493928194046020507812500000,
         -0.416664421558380126953125000000,  0.832099914550781250000000000000]])

        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[ 1.550000190734863281250000000000,  1.755000114440917968750000000000,
          1.560000061988830566406250000000, -0.034999921917915344238281250000,
          0.570000112056732177734375000000, -0.424999892711639404296875000000,
         -1.019999980926513671875000000000, -1.014999985694885253906250000000,
          0.290000021457672119140625000000,  0.495000004768371582031250000000],
        [-1.550000071525573730468750000000, -1.545000076293945312500000000000,
         -1.740000128746032714843750000000, -1.735000133514404296875000000000,
         -0.530000090599060058593750000000, -1.925000071525573730468750000000,
         -1.320000052452087402343750000000, -1.315000057220458984375000000000,
         -1.710000038146972656250000000000, -2.505000114440917968750000000000],
        [-0.200000166893005371093750000000,  0.469999849796295166015625000000,
          0.339999854564666748046875000000, -0.390000164508819580078125000000,
          0.279999852180480957031250000000,  0.349999874830245971679687500000,
         -0.580000042915344238281250000000, -0.310000032186508178710937500000,
         -0.240000009536743164062500000000,  1.029999971389770507812500000000],
        [ 0.199999868869781494140625000000,  0.469999909400939941406250000000,
         -0.660000085830688476562500000000, -1.390000104904174804687500000000,
         -0.720000088214874267578125000000, -0.650000095367431640625000000000,
         -1.180000066757202148437500000000, -0.909999966621398925781250000000,
         -0.839999973773956298828125000000,  0.430000007152557373046875000000]])
        # Verify output shape

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        
        # Verify whitening effect
        masked_advantages = advantages[response_mask]
        self.assertAlmostEqual(masked_advantages.mean().item(), 0.0, places=4)
        self.assertAlmostEqual(masked_advantages.std().item(), 1.0, places=4)
    
    def test_rloo_mathematical_correctness(self):
        """Test mathematical correctness of RLOO algorithm"""
        batch_size = 4
        seq_len = 10
        n_samples = 2
        
        # Use deterministic data for verification
        torch.manual_seed(123)
        data, response_mask, config = self.create_test_data(
            batch_size, seq_len, n_samples, include_acc=False
        )
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        # Verify mathematical properties of whitening
        masked_advantages = advantages[response_mask]

        torch.set_printoptions(precision=30, sci_mode=False)
        # print(masked_advantages)

        expected_masked_advantages = torch.tensor([ 1.871540665626525878906250000000,  2.133205175399780273437500000000,
         1.884304761886596679687500000000, -0.151573851704597473144531250000,
         0.620656013488769531250000000000, -0.649374842643737792968750000000,
        -1.408840775489807128906250000000, -1.402458548545837402343750000000,
         0.263260245323181152343750000000,  0.524924874305725097656250000000,
        -0.808926701545715332031250000000, -0.802544653415679931640625000000,
        -1.051445245742797851562500000000, -1.045063138008117675781250000000,
         0.493014484643936157226562500000, -1.287581443786621093750000000000,
        -0.515351712703704833984375000000, -0.508969664573669433593750000000,
        -1.013152718544006347656250000000, -2.027901172637939453125000000000,
        -0.106899671256542205810546875000,  0.748296976089477539062500000000,
         0.582363307476043701171875000000, -0.349418133497238159179687500000,
         0.505778551101684570312500000000,  0.595127463340759277343750000000,
        -0.591936409473419189453125000000, -0.247304961085319519042968750000,
        -0.157955989241600036621093750000,  1.463088274002075195312500000000,
         1.169513225555419921875000000000,  1.514144778251647949218750000000,
         0.071798197925090789794921875000, -0.859983325004577636718750000000,
        -0.004786581732332706451416015625,  0.084562353789806365966796875000,
        -0.591936409473419189453125000000, -0.247304961085319519042968750000,
        -0.157955989241600036621093750000,  1.463088274002075195312500000000])
        torch.testing.assert_close(masked_advantages, expected_masked_advantages, atol=1e-30, rtol=1e-30)
        # self.assertAlmostEqual(masked_advantages.mean().item(), 0.0, places=30)
        # self.assertAlmostEqual(masked_advantages.std().item(), 1.0, places=30)
        
        # Verify reasonableness of return values
        self.assertTrue(torch.isfinite(returns).all())
        self.assertEqual(advantages.shape, returns.shape)
    
    def test_multi_source_reward_fusion(self):
        """Test multi-source reward fusion functionality"""
        batch_size = 4  # 3 groups × 2 samples per group
        seq_len = 10
        n_samples = 2
        
        # Create test data with proper structure
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.tensor([[0.1,0.3,1.0,-0.1,0.7,0.5,0.2,-0.45,0.1,0.2], [0.2,0.3,0.2,-0.4,0.9,-0.1,0.2,0.4,0.6,-0.8], [-0.1,0.3,0.6,-0.1,0.2,0.7,0.1,0.2,-0.4,0.6], [0.1,0.8,0.6,-0.1,0.2,0.5,0.1,0.2,-0.4,0.6]]),
            "acc": torch.tensor([1.0, 0.0, 0.8, 0.2]),
            "prompts": torch.ones(batch_size, 5, dtype=torch.long),
            "attention_mask": torch.ones(batch_size, seq_len + 5, dtype=torch.long),
        }
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        
        # Create config with both reward sources
        config = MagicMock()
        config.algorithm.reward_dpo_coef = 1.0
        config.algorithm.reward_gt_coef = 1.0
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages)
        # print(returns)

        expected_advantages = torch.tensor([[ 1.836434125900268554687500000000,  1.989082455635070800781250000000,
          1.843880414962768554687500000000,  0.656201839447021484375000000000,
          1.106700658798217773437500000000,  0.365797668695449829101562500000,
         -0.077254861593246459960937500000, -0.073531731963157653808593750000,
          0.898205220699310302734375000000,  1.050853610038757324218750000000],
        [-1.216532945632934570312500000000, -1.212809801101684570312500000000,
         -1.358011841773986816406250000000, -1.354288816452026367187500000000,
         -0.457014262676239013671875000000, -1.495767593383789062500000000000,
         -1.045268774032592773437500000000, -1.041545748710632324218750000000,
         -1.335673093795776367187500000000, -1.927650809288024902343750000000],
        [ 0.384413182735443115234375000000,  0.883312642574310302734375000000,
          0.786511242389678955078125000000,  0.242934226989746093750000000000,
          0.741833686828613281250000000000,  0.793957591056823730468750000000,
          0.101455338299274444580078125000,  0.302504390478134155273437500000,
          0.354628235101699829101562500000,  1.300303339958190917968750000000],
        [ 0.235487923026084899902343750000,  0.436536997556686401367187500000,
         -0.404890447854995727539062500000, -0.948467493057250976562500000000,
         -0.449568033218383789062500000000, -0.397444158792495727539062500000,
         -0.792096018791198730468750000000, -0.591046929359436035156250000000,
         -0.538923084735870361328125000000,  0.406752020120620727539062500000]])

        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[ 2.050000190734863281250000000000,  2.255000114440917968750000000000,
          2.060000181198120117187500000000,  0.465000092983245849609375000000,
          1.070000052452087402343750000000,  0.075000092387199401855468750000,
         -0.519999921321868896484375000000, -0.514999926090240478515625000000,
          0.790000021457672119140625000000,  0.995000004768371582031250000000],
        [-2.050000190734863281250000000000, -2.045000076293945312500000000000,
         -2.240000009536743164062500000000, -2.235000133514404296875000000000,
         -1.030000090599060058593750000000, -2.424999952316284179687500000000,
         -1.820000052452087402343750000000, -1.815000057220458984375000000000,
         -2.210000038146972656250000000000, -3.005000114440917968750000000000],
        [ 0.099999904632568359375000000000,  0.769999921321868896484375000000,
          0.639999926090240478515625000000, -0.090000092983245849609375000000,
          0.579999923706054687500000000000,  0.649999976158142089843750000000,
         -0.280000001192092895507812500000, -0.009999960660934448242187500000,
          0.060000061988830566406250000000,  1.330000042915344238281250000000],
        [-0.100000143051147460937500000000,  0.169999897480010986328125000000,
         -0.960000097751617431640625000000, -1.690000057220458984375000000000,
         -1.020000100135803222656250000000, -0.950000047683715820312500000000,
         -1.480000019073486328125000000000, -1.210000038146972656250000000000,
         -1.139999985694885253906250000000,  0.129999995231628417968750000000]])
        # Verify output shape

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        
        # Verify whitening property
        masked_advantages = advantages[response_mask]
        mean_val = masked_advantages.mean().item()
        std_val = masked_advantages.std().item()
        
        self.assertAlmostEqual(mean_val, 0.0, places=4)
        self.assertAlmostEqual(std_val, 1.0, places=4)
    
    def test_different_reward_coefficients(self):
        """Test the impact of different reward coefficients"""
        batch_size = 4
        seq_len = 10
        n_samples = 2
        
        data, response_mask, _ = self.create_test_data(
            batch_size, seq_len, n_samples, include_acc=True
        )
        
        # Test different coefficient combinations
        coeff_combinations = [
            (1.0, 0.0),  # Use only DPO reward
            (0.0, 1.0),  # Use only accuracy reward
            (1.0, 1.0),  # Equal weight for both rewards
            (2.0, 0.5),  # Higher weight for DPO reward
        ]

        
        results = []
        for dpo_coef, gt_coef in coeff_combinations:
            config = MagicMock()
            config.algorithm.reward_dpo_coef = dpo_coef
            config.algorithm.reward_gt_coef = gt_coef
            
            advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
            results.append((advantages, returns))
            
            # Verify that each combination produces valid output
            self.assertEqual(advantages.shape, (batch_size, seq_len))
            self.assertTrue(torch.isfinite(advantages).all())
            self.assertTrue(torch.isfinite(returns).all())
        
        # print(results)
        torch.set_printoptions(precision=30, sci_mode=False)
        # print(results)
        expected_results = [(torch.tensor([[ 1.871540665626525878906250000000,  2.133205175399780273437500000000,
          1.884304761886596679687500000000, -0.151573851704597473144531250000,
          0.620656013488769531250000000000, -0.649374842643737792968750000000,
         -1.408840775489807128906250000000, -1.402458548545837402343750000000,
          0.263260245323181152343750000000,  0.524924874305725097656250000000],
        [-0.808926701545715332031250000000, -0.802544653415679931640625000000,
         -1.051445245742797851562500000000, -1.045063138008117675781250000000,
          0.493014484643936157226562500000, -1.287581443786621093750000000000,
         -0.515351712703704833984375000000, -0.508969664573669433593750000000,
         -1.013152718544006347656250000000, -2.027901172637939453125000000000],
        [-0.106899671256542205810546875000,  0.748296976089477539062500000000,
          0.582363307476043701171875000000, -0.349418133497238159179687500000,
          0.505778551101684570312500000000,  0.595127463340759277343750000000,
         -0.591936409473419189453125000000, -0.247304961085319519042968750000,
         -0.157955989241600036621093750000,  1.463088274002075195312500000000],
        [ 1.169513225555419921875000000000,  1.514144778251647949218750000000,
          0.071798197925090789794921875000, -0.859983325004577636718750000000,
         -0.004786581732332706451416015625,  0.084562353789806365966796875000,
         -0.591936409473419189453125000000, -0.247304961085319519042968750000,
         -0.157955989241600036621093750000,  1.463088274002075195312500000000]]), torch.tensor([[ 1.050000190734863281250000000000,  1.255000114440917968750000000000,
          1.060000061988830566406250000000, -0.534999907016754150390625000000,
          0.070000097155570983886718750000, -0.924999892711639404296875000000,
         -1.519999980926513671875000000000, -1.514999985694885253906250000000,
         -0.209999993443489074707031250000, -0.004999995231628417968750000000],
        [-1.050000071525573730468750000000, -1.045000076293945312500000000000,
         -1.240000128746032714843750000000, -1.235000133514404296875000000000,
         -0.030000060796737670898437500000, -1.425000071525573730468750000000,
         -0.820000052452087402343750000000, -0.815000057220458984375000000000,
         -1.210000038146972656250000000000, -2.005000114440917968750000000000],
        [-0.500000119209289550781250000000,  0.169999897480010986328125000000,
          0.039999902248382568359375000000, -0.690000116825103759765625000000,
         -0.020000100135803222656250000000,  0.049999922513961791992187500000,
         -0.879999995231628417968750000000, -0.610000014305114746093750000000,
         -0.539999961853027343750000000000,  0.730000019073486328125000000000],
        [ 0.499999880790710449218750000000,  0.769999921321868896484375000000,
         -0.360000073909759521484375000000, -1.090000152587890625000000000000,
         -0.420000076293945312500000000000, -0.350000053644180297851562500000,
         -0.879999995231628417968750000000, -0.610000014305114746093750000000,
         -0.539999961853027343750000000000,  0.730000019073486328125000000000]])), (torch.tensor([[ 1.197423696517944335937500000000,  1.197423696517944335937500000000,
          1.197423696517944335937500000000,  1.197423696517944335937500000000,
          1.197423696517944335937500000000,  1.197423696517944335937500000000,
          1.197423696517944335937500000000,  1.197423696517944335937500000000,
          1.197423696517944335937500000000,  1.197423696517944335937500000000],
        [-1.197423696517944335937500000000, -1.197423696517944335937500000000,
         -1.197423696517944335937500000000, -1.197423696517944335937500000000,
         -1.197423696517944335937500000000, -1.197423696517944335937500000000,
         -1.197423696517944335937500000000, -1.197423696517944335937500000000,
         -1.197423696517944335937500000000, -1.197423696517944335937500000000],
        [ 0.718454241752624511718750000000,  0.718454241752624511718750000000,
          0.718454241752624511718750000000,  0.718454241752624511718750000000,
          0.718454241752624511718750000000,  0.718454241752624511718750000000,
          0.718454241752624511718750000000,  0.718454241752624511718750000000,
          0.718454241752624511718750000000,  0.718454241752624511718750000000],
        [-0.718454241752624511718750000000, -0.718454241752624511718750000000,
         -0.718454241752624511718750000000, -0.718454241752624511718750000000,
         -0.718454241752624511718750000000, -0.718454241752624511718750000000,
         -0.718454241752624511718750000000, -0.718454241752624511718750000000,
         -0.718454241752624511718750000000, -0.718454241752624511718750000000]]), torch.tensor([[ 1.000000000000000000000000000000,  1.000000000000000000000000000000,
          1.000000000000000000000000000000,  1.000000000000000000000000000000,
          1.000000000000000000000000000000,  1.000000000000000000000000000000,
          1.000000000000000000000000000000,  1.000000000000000000000000000000,
          1.000000000000000000000000000000,  1.000000000000000000000000000000],
        [-1.000000000000000000000000000000, -1.000000000000000000000000000000,
         -1.000000000000000000000000000000, -1.000000000000000000000000000000,
         -1.000000000000000000000000000000, -1.000000000000000000000000000000,
         -1.000000000000000000000000000000, -1.000000000000000000000000000000,
         -1.000000000000000000000000000000, -1.000000000000000000000000000000],
        [ 0.600000023841857910156250000000,  0.600000023841857910156250000000,
          0.600000023841857910156250000000,  0.600000023841857910156250000000,
          0.600000023841857910156250000000,  0.600000023841857910156250000000,
          0.600000023841857910156250000000,  0.600000023841857910156250000000,
          0.600000023841857910156250000000,  0.600000023841857910156250000000],
        [-0.600000023841857910156250000000, -0.600000023841857910156250000000,
         -0.600000023841857910156250000000, -0.600000023841857910156250000000,
         -0.600000023841857910156250000000, -0.600000023841857910156250000000,
         -0.600000023841857910156250000000, -0.600000023841857910156250000000,
         -0.600000023841857910156250000000, -0.600000023841857910156250000000]])), (torch.tensor([[ 1.836434125900268554687500000000,  1.989082455635070800781250000000,
          1.843880414962768554687500000000,  0.656201839447021484375000000000,
          1.106700658798217773437500000000,  0.365797668695449829101562500000,
         -0.077254861593246459960937500000, -0.073531731963157653808593750000,
          0.898205220699310302734375000000,  1.050853610038757324218750000000],
        [-1.216532945632934570312500000000, -1.212809801101684570312500000000,
         -1.358011841773986816406250000000, -1.354288816452026367187500000000,
         -0.457014262676239013671875000000, -1.495767593383789062500000000000,
         -1.045268774032592773437500000000, -1.041545748710632324218750000000,
         -1.335673093795776367187500000000, -1.927650809288024902343750000000],
        [ 0.384413182735443115234375000000,  0.883312642574310302734375000000,
          0.786511242389678955078125000000,  0.242934226989746093750000000000,
          0.741833686828613281250000000000,  0.793957591056823730468750000000,
          0.101455338299274444580078125000,  0.302504390478134155273437500000,
          0.354628235101699829101562500000,  1.300303339958190917968750000000],
        [ 0.235487923026084899902343750000,  0.436536997556686401367187500000,
         -0.404890447854995727539062500000, -0.948467493057250976562500000000,
         -0.449568033218383789062500000000, -0.397444158792495727539062500000,
         -0.792096018791198730468750000000, -0.591046929359436035156250000000,
         -0.538923084735870361328125000000,  0.406752020120620727539062500000]]), torch.tensor([[ 2.050000190734863281250000000000,  2.255000114440917968750000000000,
          2.060000181198120117187500000000,  0.465000092983245849609375000000,
          1.070000052452087402343750000000,  0.075000092387199401855468750000,
         -0.519999921321868896484375000000, -0.514999926090240478515625000000,
          0.790000021457672119140625000000,  0.995000004768371582031250000000],
        [-2.050000190734863281250000000000, -2.045000076293945312500000000000,
         -2.240000009536743164062500000000, -2.235000133514404296875000000000,
         -1.030000090599060058593750000000, -2.424999952316284179687500000000,
         -1.820000052452087402343750000000, -1.815000057220458984375000000000,
         -2.210000038146972656250000000000, -3.005000114440917968750000000000],
        [ 0.099999904632568359375000000000,  0.769999921321868896484375000000,
          0.639999926090240478515625000000, -0.090000092983245849609375000000,
          0.579999923706054687500000000000,  0.649999976158142089843750000000,
         -0.280000001192092895507812500000, -0.009999960660934448242187500000,
          0.060000061988830566406250000000,  1.330000042915344238281250000000],
        [-0.100000143051147460937500000000,  0.169999897480010986328125000000,
         -0.960000097751617431640625000000, -1.690000057220458984375000000000,
         -1.020000100135803222656250000000, -0.950000047683715820312500000000,
         -1.480000019073486328125000000000, -1.210000038146972656250000000000,
         -1.139999985694885253906250000000,  0.129999995231628417968750000000]])), (torch.tensor([[ 1.942705869674682617187500000000,  2.174755096435546875000000000000,
          1.954025268554687500000000000000,  0.148568332195281982421875000000,
          0.833396852016448974609375000000, -0.292891353368759155273437500000,
         -0.966400444507598876953125000000, -0.960740745067596435546875000000,
          0.516451299190521240234375000000,  0.748500645160675048828125000000],
        [-1.000359058380126953125000000000, -0.994699299335479736328125000000,
         -1.215429067611694335937500000000, -1.209769487380981445312500000000,
          0.154227897524833679199218750000, -1.424839377403259277343750000000,
         -0.740010976791381835937500000000, -0.734351217746734619140625000000,
         -1.181470751762390136718750000000, -2.081369400024414062500000000000],
        [ 0.074991442263126373291015625000,  0.833396553993225097656250000000,
          0.686243355274200439453125000000, -0.140078663825988769531250000000,
          0.618326485157012939453125000000,  0.697562813758850097656250000000,
         -0.355148613452911376953125000000, -0.049522675573825836181640625000,
          0.029713705182075500488281250000,  1.467287659645080566406250000000],
        [ 0.867355048656463623046875000000,  1.172981023788452148437500000000,
         -0.106120117008686065673828125000, -0.932442069053649902343750000000,
         -0.174036994576454162597656250000, -0.094800584018230438232421875000,
         -0.694732964038848876953125000000, -0.389107018709182739257812500000,
         -0.309870630502700805664062500000,  1.127703189849853515625000000000]]), torch.tensor([[ 2.600000381469726562500000000000,  3.010000228881835937500000000000,
          2.620000123977661132812500000000, -0.569999814033508300781250000000,
          0.640000224113464355468750000000, -1.349999785423278808593750000000,
         -2.539999961853027343750000000000, -2.529999971389770507812500000000,
          0.080000013113021850585937500000,  0.490000009536743164062500000000],
        [-2.600000143051147460937500000000, -2.590000152587890625000000000000,
         -2.980000257492065429687500000000, -2.970000267028808593750000000000,
         -0.560000121593475341796875000000, -3.350000143051147460937500000000,
         -2.140000104904174804687500000000, -2.130000114440917968750000000000,
         -2.920000076293945312500000000000, -4.510000228881835937500000000000],
        [-0.700000286102294921875000000000,  0.639999747276306152343750000000,
          0.379999756813049316406250000000, -1.080000281333923339843750000000,
          0.259999752044677734375000000000,  0.399999797344207763671875000000,
         -1.460000038146972656250000000000, -0.920000016689300537109375000000,
         -0.779999971389770507812500000000,  1.759999990463256835937500000000],
        [ 0.699999809265136718750000000000,  1.239999890327453613281250000000,
         -1.020000100135803222656250000000, -2.480000019073486328125000000000,
         -1.140000104904174804687500000000, -1.000000000000000000000000000000,
         -2.059999942779541015625000000000, -1.519999980926513671875000000000,
         -1.379999876022338867187500000000,  1.160000085830688476562500000000]]))]
        torch.testing.assert_close(results, expected_results, atol=1e-30, rtol=1e-30)
        # # Verify that different coefficient combinations produce different results
        # for i, (adv1, ret1) in enumerate(results):
        #     for j, (adv2, ret2) in enumerate(results[i+1:], i+1):
        #         # Different coefficients should produce different results (unless one coefficient is 0, they may be the same)
        #         if coeff_combinations[i] != coeff_combinations[j]:
        #             self.assertFalse(torch.allclose(adv1, adv2, atol=1e-6), 
        #                            f"Coefficients {coeff_combinations[i]} and {coeff_combinations[j]} should produce different results")
    
    def test_leave_one_out_baseline(self):
        """Test the concept of Leave-One-Out baseline estimation"""
        batch_size = 6  # 3 groups × 2 samples per group
        seq_len = 4
        n_samples = 2
        
        # Create data with clear grouping structure
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.tensor([
                [1.0, 1.0, 1.0, 1.0],  # Group 1, Sample 1
                [2.0, 2.0, 2.0, 2.0],  # Group 1, Sample 2
                [3.0, 3.0, 3.0, 3.0],  # Group 2, Sample 1
                [4.0, 4.0, 4.0, 4.0],  # Group 2, Sample 2
                [5.0, 5.0, 5.0, 5.0],  # Group 3, Sample 1
                [6.0, 6.0, 6.0, 6.0],  # Group 3, Sample 2
            ]),
            "prompts": torch.ones(batch_size, 5, dtype=torch.long),
            "attention_mask": torch.ones(batch_size, seq_len + 5, dtype=torch.long),
        }
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        config = MagicMock()
        config.algorithm.reward_dpo_coef = 1.0
        config.algorithm.reward_gt_coef = 0.0
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages)
        # print(returns)

        expected_advantages = torch.tensor([[-1.429840683937072753906250000000, -1.072380542755126953125000000000,
         -0.714920341968536376953125000000, -0.357460170984268188476562500000],
        [ 1.429840683937072753906250000000,  1.072380542755126953125000000000,
          0.714920341968536376953125000000,  0.357460170984268188476562500000],
        [-1.429840683937072753906250000000, -1.072380542755126953125000000000,
         -0.714920341968536376953125000000, -0.357460170984268188476562500000],
        [ 1.429840683937072753906250000000,  1.072380542755126953125000000000,
          0.714920341968536376953125000000,  0.357460170984268188476562500000],
        [-1.429840683937072753906250000000, -1.072380542755126953125000000000,
         -0.714920341968536376953125000000, -0.357460170984268188476562500000],
        [ 1.429840683937072753906250000000,  1.072380542755126953125000000000,
          0.714920341968536376953125000000,  0.357460170984268188476562500000]])

        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[-4., -3., -2., -1.],
        [ 4.,  3.,  2.,  1.],
        [-4., -3., -2., -1.],
        [ 4.,  3.,  2.,  1.],
        [-4., -3., -2., -1.],
        [ 4.,  3.,  2.,  1.]])

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        # RLOO should reduce variance, verify mathematical properties of output
        # self.assertEqual(advantages.shape, (batch_size, seq_len))
        # self.assertTrue(torch.isfinite(advantages).all())
        
        # Verify whitening effect
        masked_advantages = advantages[response_mask]
        self.assertAlmostEqual(masked_advantages.mean().item(), 0.0, places=4)
        self.assertAlmostEqual(masked_advantages.std().item(), 1.0, places=4)
    
    def test_accuracy_reward_allocation(self):
        """Test accuracy reward allocation to the last valid token position"""
        batch_size = 2
        seq_len = 6
        n_samples = 2  # Fix division by zero error: n_samples must be >1
        
        # Create sequences with different lengths (controlled by attention_mask)
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.zeros(batch_size, seq_len),  # Set RM scores to 0 to highlight accuracy reward
            "acc": torch.tensor([1.0, 0.5]),
            "prompts": torch.ones(batch_size, 2, dtype=torch.long),
            "attention_mask": torch.tensor([
                [1, 1, 1, 1, 1, 0],  # First sequence has valid length of 5
                [1, 1, 1, 1, 0, 0],  # Second sequence has valid length of 4
            ], dtype=torch.long),
        }
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        config = MagicMock()
        config.algorithm.reward_dpo_coef = 0.0  # Do not use RM scores
        config.algorithm.reward_gt_coef = 1.0   # Use only accuracy reward
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        # print(advantages)  
        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages)
        # print(returns)

        expected_advantages = torch.tensor([[ 1.371110320091247558593750000000,  1.371110320091247558593750000000,
          1.371110320091247558593750000000, -0.124646395444869995117187500000,
         -0.124646395444869995117187500000, -0.124646395444869995117187500000],
        [-1.620403170585632324218750000000, -1.620403170585632324218750000000,
         -0.124646395444869995117187500000, -0.124646395444869995117187500000,
         -0.124646395444869995117187500000, -0.124646395444869995117187500000]])

        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[ 0.5000,  0.5000,  0.5000,  0.0000,  0.0000,  0.0000],
        [-0.5000, -0.5000,  0.0000,  0.0000,  0.0000,  0.0000]])

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        # Verify accuracy reward allocation mechanism
        # self.assertEqual(advantages.shape, (batch_size, seq_len))
        # self.assertTrue(torch.isfinite(advantages).all())
        
        # Since only accuracy reward is used and allocated to the last valid position, a specific pattern should be observable
        # Specific verification depends on implementation details, here mainly verify output validity
        masked_advantages = advantages[response_mask]
        if len(masked_advantages) > 0:
            # If there are valid advantage values, verify whitening effect
            if masked_advantages.std() > 1e-8:
                self.assertAlmostEqual(masked_advantages.mean().item(), 0.0, places=3)
    
    def test_whitening_process(self):
        """Test correctness of whitening process"""
        batch_size = 4
        seq_len = 10
        n_samples = 2
        
        data, response_mask, config = self.create_test_data(
            batch_size, seq_len, n_samples, include_acc=False
        )
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages)
        # print(returns)

        expected_advantages = torch.tensor([[ 1.871540665626525878906250000000,  2.133205175399780273437500000000,
          1.884304761886596679687500000000, -0.151573851704597473144531250000,
          0.620656013488769531250000000000, -0.649374842643737792968750000000,
         -1.408840775489807128906250000000, -1.402458548545837402343750000000,
          0.263260245323181152343750000000,  0.524924874305725097656250000000],
        [-0.808926701545715332031250000000, -0.802544653415679931640625000000,
         -1.051445245742797851562500000000, -1.045063138008117675781250000000,
          0.493014484643936157226562500000, -1.287581443786621093750000000000,
         -0.515351712703704833984375000000, -0.508969664573669433593750000000,
         -1.013152718544006347656250000000, -2.027901172637939453125000000000],
        [-0.106899671256542205810546875000,  0.748296976089477539062500000000,
          0.582363307476043701171875000000, -0.349418133497238159179687500000,
          0.505778551101684570312500000000,  0.595127463340759277343750000000,
         -0.591936409473419189453125000000, -0.247304961085319519042968750000,
         -0.157955989241600036621093750000,  1.463088274002075195312500000000],
        [ 1.169513225555419921875000000000,  1.514144778251647949218750000000,
          0.071798197925090789794921875000, -0.859983325004577636718750000000,
         -0.004786581732332706451416015625,  0.084562353789806365966796875000,
         -0.591936409473419189453125000000, -0.247304961085319519042968750000,
         -0.157955989241600036621093750000,  1.463088274002075195312500000000]])
        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[ 1.050000190734863281250000000000,  1.255000114440917968750000000000,
          1.060000061988830566406250000000, -0.534999907016754150390625000000,
          0.070000097155570983886718750000, -0.924999892711639404296875000000,
         -1.519999980926513671875000000000, -1.514999985694885253906250000000,
         -0.209999993443489074707031250000, -0.004999995231628417968750000000],
        [-1.050000071525573730468750000000, -1.045000076293945312500000000000,
         -1.240000128746032714843750000000, -1.235000133514404296875000000000,
         -0.030000060796737670898437500000, -1.425000071525573730468750000000,
         -0.820000052452087402343750000000, -0.815000057220458984375000000000,
         -1.210000038146972656250000000000, -2.005000114440917968750000000000],
        [-0.500000119209289550781250000000,  0.169999897480010986328125000000,
          0.039999902248382568359375000000, -0.690000116825103759765625000000,
         -0.020000100135803222656250000000,  0.049999922513961791992187500000,
         -0.879999995231628417968750000000, -0.610000014305114746093750000000,
         -0.539999961853027343750000000000,  0.730000019073486328125000000000],
        [ 0.499999880790710449218750000000,  0.769999921321868896484375000000,
         -0.360000073909759521484375000000, -1.090000152587890625000000000000,
         -0.420000076293945312500000000000, -0.350000053644180297851562500000,
         -0.879999995231628417968750000000, -0.610000014305114746093750000000,
         -0.539999961853027343750000000000,  0.730000019073486328125000000000]])

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        # Verify that whitening ensures distributional properties of advantage function
        masked_advantages = advantages[response_mask]
        
        # Whitened advantages should have: mean≈0, standard deviation≈1
        mean_val = masked_advantages.mean().item()
        std_val = masked_advantages.std().item()
        
        self.assertAlmostEqual(mean_val, 0.0, places=5, 
                              msg="Whitened advantages should have zero mean")
        self.assertAlmostEqual(std_val, 1.0, places=5,
                              msg="Whitened advantages should have unit standard deviation")
        
        # Verify that whitening improves training stability: reduces gradient variance
        self.assertTrue(torch.isfinite(advantages).all())
        self.assertFalse(torch.isnan(advantages).any())
    
    def test_sample_grouping_logic(self):
        """Test sample grouping logic"""
        n_samples = 3
        batch_size = 9  # 3 groups × 3 samples per group
        seq_len = 4
        
        # Create data with clear grouping structure
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.arange(batch_size * seq_len).float().reshape(batch_size, seq_len),
            "prompts": torch.ones(batch_size, 2, dtype=torch.long),
            "attention_mask": torch.ones(batch_size, seq_len + 2, dtype=torch.long),
        }
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        config = MagicMock()
        config.algorithm.reward_dpo_coef = 1.0
        config.algorithm.reward_gt_coef = 0.0
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        # Verify output of grouping processing
        self.assertEqual(advantages.shape, (batch_size, seq_len))
        self.assertEqual(returns.shape, (batch_size, seq_len))
        
        # RLOO algorithm should perform leave-one-out baseline calculation within groups
        # Verify mathematical properties of output
        masked_advantages = advantages[response_mask]
        self.assertAlmostEqual(masked_advantages.mean().item(), 0.0, places=4)
        self.assertAlmostEqual(masked_advantages.std().item(), 1.0, places=4)

    def test_input_output_samples_rm_only(self):
        """Test specific input/output samples using only RM reward"""
        # Test case 1: Simple 2 groups × 2 samples case
        n_samples = 2
        batch_size = 4
        seq_len = 3
        
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.tensor([
                [1.0, 2.0, 3.0],  # Group 1, Sample 1
                [2.0, 1.0, 1.0],  # Group 1, Sample 2
                [3.0, 3.0, 3.0],  # Group 2, Sample 1
                [1.0, 1.0, 2.0]   # Group 2, Sample 2
            ]),
            "prompts": torch.ones(batch_size, 2, dtype=torch.long),
            "attention_mask": torch.ones(batch_size, seq_len + 2, dtype=torch.long),
        }
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        config = MagicMock()
        config.algorithm.reward_dpo_coef = 1.0
        config.algorithm.reward_gt_coef = 0.0
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        # print(advantages)   
        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages)
        # print(returns)
        expected_advantages = torch.tensor([[ 0.539734661579132080078125000000,  0.971522450447082519531250000000,
          0.755628645420074462890625000000],
        [-0.755628645420074462890625000000, -0.971522450447082519531250000000,
         -0.539734661579132080078125000000],
        [ 1.511257052421569824218750000000,  0.971522450447082519531250000000,
          0.431787818670272827148437500000],
        [-1.727151036262512207031250000000, -0.971522450447082519531250000000,
         -0.215893819928169250488281250000]])

        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[ 1.999999523162841796875000000000,  3.333333015441894531250000000000,
          2.666666507720947265625000000000],
        [-2.000000476837158203125000000000, -2.666666984558105468750000000000,
         -1.333333492279052734375000000000],
        [ 4.999999523162841796875000000000,  3.333333015441894531250000000000,
          1.666666507720947265625000000000],
        [-5.000000476837158203125000000000, -2.666666984558105468750000000000,
         -0.333333492279052734375000000000]])

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        # Verify basic properties
        # self.assertEqual(advantages.shape, (batch_size, seq_len))
        # self.assertEqual(returns.shape, (batch_size, seq_len))
        # self.assertTrue(torch.isfinite(advantages).all())
        # self.assertTrue(torch.isfinite(returns).all())
        
        # Verify RLOO processing:
        # Group 1: samples [0,1], Group 2: samples [2,3]
        # RLOO should compute leave-one-out baseline for each sample
        # For example, baseline for sample 0 is the mean of sample 1, baseline for sample 1 is the mean of sample 0
        
        # Verify whitening effect
        masked_advantages = advantages[response_mask]
        mean_val = masked_advantages.mean().item()
        std_val = masked_advantages.std().item()
        
        # After whitening should satisfy mean≈0, standard deviation≈1
        self.assertAlmostEqual(mean_val, 0.0, places=3)
        self.assertAlmostEqual(std_val, 1.0, places=3)
        
        # Verify returns calculation: should be cumulative sum of rewards (from right to left)
        # Since cumulative return calculation involves complex flipping and accumulation, mainly verify its mathematical properties
        for i in range(batch_size):
            # Each row's returns should be decreasing (from left to right, since accumulated from right)
            row_returns = returns[i, response_mask[i]]
            if len(row_returns) > 1:
                # Verify basic reasonableness of returns (don't check specific values since they've been whitened)
                self.assertTrue(torch.isfinite(row_returns).all())

    def test_input_output_samples_accuracy_reward(self):
        """Test specific input/output samples for accuracy reward allocation"""
        # Test case: Accuracy reward allocated to the last valid token position
        n_samples = 2
        batch_size = 4
        seq_len = 4
        
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.zeros(batch_size, seq_len),  # Set RM scores to 0 to highlight accuracy effect
            "acc": torch.tensor([1.0, 0.5, 0.8, 0.2]),
            "prompts": torch.ones(batch_size, 2, dtype=torch.long),  # prompt length is 2
            "attention_mask": torch.tensor([
                [1, 1, 1, 1, 1, 1],  # First sample: prompt(2) + response(4) = 6, all valid
                [1, 1, 1, 1, 1, 0],  # Second sample: valid length 5, response length 3
                [1, 1, 1, 1, 0, 0],  # Third sample: valid length 4, response length 2
                [1, 1, 1, 0, 0, 0]   # Fourth sample: valid length 3, response length 1
            ], dtype=torch.long),
        }
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        config = MagicMock()
        config.algorithm.reward_dpo_coef = 0.0  # Do not use RM reward
        config.algorithm.reward_gt_coef = 1.0   # Use only accuracy reward
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)

        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages)
        # print(returns)
        expected_advantages = torch.tensor([[ 1.006382703781127929687500000000,  1.006382703781127929687500000000,
          1.006382703781127929687500000000,  1.006382703781127929687500000000],
        [-1.327258467674255371093750000000, -1.327258467674255371093750000000,
         -1.327258467674255371093750000000, -0.160437822341918945312500000000],
        [ 1.239746809005737304687500000000,  1.239746809005737304687500000000,
         -0.160437822341918945312500000000, -0.160437822341918945312500000000],
        [-1.560622572898864746093750000000, -0.160437822341918945312500000000,
         -0.160437822341918945312500000000, -0.160437822341918945312500000000]])

        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[ 0.5000,  0.5000,  0.5000,  0.5000],
        [-0.5000, -0.5000, -0.5000,  0.0000],
        [ 0.6000,  0.6000,  0.0000,  0.0000],
        [-0.6000,  0.0000,  0.0000,  0.0000]])

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        
        
        # Verify whitening effect
        masked_advantages = advantages[response_mask]
        if len(masked_advantages) > 0 and masked_advantages.std() > 1e-8:
            self.assertAlmostEqual(masked_advantages.mean().item(), 0.0, places=3)
            self.assertAlmostEqual(masked_advantages.std().item(), 1.0, places=3)

    def test_input_output_samples_multi_source_fusion(self):
        """Test specific input/output samples for multi-source reward fusion"""
        # Test case: Use both RM scores and accuracy reward simultaneously
        n_samples = 2
        batch_size = 4
        seq_len = 3
        
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.tensor([
                [0.5, 0.3, 0.2],  # Group 1, Sample 1
                [0.1, 0.2, 0.4],  # Group 1, Sample 2  
                [0.8, 0.6, 0.4],  # Group 2, Sample 1
                [0.2, 0.3, 0.5]   # Group 2, Sample 2
            ]),
            "acc": torch.tensor([0.9, 0.3, 0.8, 0.4]),
            "prompts": torch.ones(batch_size, 1, dtype=torch.long),  # prompt length is 1
            "attention_mask": torch.ones(batch_size, seq_len + 1, dtype=torch.long),
        }
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        config = MagicMock()
        config.algorithm.reward_dpo_coef = 2.0   # RM reward coefficient
        config.algorithm.reward_gt_coef = 3.0    # Accuracy reward coefficient
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)

        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages)
        # print(returns)
        expected_advantages = torch.tensor([[ 1.224922418594360351562500000000,  0.788648664951324462890625000000,
          0.755089163780212402343750000000],
        [-1.191362977027893066406250000000, -0.822208285331726074218750000000,
         -0.654410600662231445312500000000],
        [ 1.426279783248901367187500000000,  0.755089282989501953125000000000,
          0.486613154411315917968750000000],
        [-1.392720103263854980468750000000, -0.855767786502838134765625000000,
         -0.520172536373138427734375000000]])

        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[ 2.399999380111694335937500000000,  1.533332824707031250000000000000,
          1.466666221618652343750000000000],
        [-2.400000333786010742187500000000, -1.666666984558105468750000000000,
         -1.333333492279052734375000000000],
        [ 2.799999713897705078125000000000,  1.466666460037231445312500000000,
          0.933333158493041992187500000000],
        [-2.800000429153442382812500000000, -1.733333587646484375000000000000,
         -1.066666841506958007812500000000]])

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        # Verify output of multi-source fusion
        # self.assertEqual(advantages.shape, (batch_size, seq_len))
        # self.assertEqual(returns.shape, (batch_size, seq_len))
        
        # Manually verify fusion logic concept:
        # - RM reward: rm_scores at each position * reward_dpo_coef = rm_scores * 2.0
        # - Accuracy reward: allocated to last valid position, value is acc * reward_gt_coef = acc * 3.0
        # - Fusion: add two reward tensors together
        # - RLOO processing: compute leave-one-out baseline within each group
        # - Whitening: standardization processing
        
        # Verify mathematical properties of output
        self.assertTrue(torch.isfinite(advantages).all())
        self.assertTrue(torch.isfinite(returns).all())
        
        # Verify whitening effect
        masked_advantages = advantages[response_mask]
        if len(masked_advantages) > 0:
            mean_val = masked_advantages.mean().item()
            std_val = masked_advantages.std().item()
            
            # Since there is multi-source reward fusion, there should be sufficient variance for whitening
            if std_val > 1e-6:
                self.assertAlmostEqual(mean_val, 0.0, places=3)
                self.assertAlmostEqual(std_val, 1.0, places=3)

    def test_rloo_baseline_calculation_input_output(self):
        """Test specific input/output samples for RLOO baseline calculation"""
        # Test case: Verify the impact of Leave-One-Out baseline calculation
        n_samples = 3
        batch_size = 6  # 2 groups × 3 samples per group
        seq_len = 2
        
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.tensor([
                [1.0, 1.0],  # Group 1: [1.0, 1.0] average reward: 1.0
                [2.0, 2.0],  # Group 1: [2.0, 2.0] average reward: 2.0
                [3.0, 3.0],  # Group 1: [3.0, 3.0] average reward: 3.0
                [4.0, 4.0],  # Group 2: [4.0, 4.0] average reward: 4.0
                [5.0, 5.0],  # Group 2: [5.0, 5.0] average reward: 5.0
                [6.0, 6.0],  # Group 2: [6.0, 6.0] average reward: 6.0
            ]),
            "prompts": torch.ones(batch_size, 1, dtype=torch.long),
            "attention_mask": torch.ones(batch_size, seq_len + 1, dtype=torch.long),
        }
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        config = MagicMock()
        config.algorithm.reward_dpo_coef = 1.0
        config.algorithm.reward_gt_coef = 0.0
        
        advantages, returns = compute_rloo_advantage_return(data, response_mask, n_samples, config)
        
        torch.set_printoptions(precision=30, sci_mode=False)    
        # print(advantages)
        # print(returns)
        expected_advantages = torch.tensor([[-1.483239650726318359375000000000, -0.741619825363159179687500000000],
        [ 0.000000000000000000000000000000,  0.000000000000000000000000000000],
        [ 1.483239650726318359375000000000,  0.741619825363159179687500000000],
        [-1.483239650726318359375000000000, -0.741619825363159179687500000000],
        [ 0.000000000000000000000000000000,  0.000000000000000000000000000000],
        [ 1.483239650726318359375000000000,  0.741619825363159179687500000000]])

        torch.testing.assert_close(advantages, expected_advantages, atol=1e-30, rtol=1e-30)

        expected_returns = torch.tensor([[-3.0000, -1.5000],
        [ 0.0000,  0.0000],
        [ 3.0000,  1.5000],
        [-3.0000, -1.5000],
        [ 0.0000,  0.0000],
        [ 3.0000,  1.5000]])

        torch.testing.assert_close(returns, expected_returns, atol=1e-30, rtol=1e-30)
        # Verify RLOO processing concept:
        # Group 1 (samples 0,1,2):
        #   - Baseline for sample 0 = average of (sample 1 + sample 2) / 2
        #   - Baseline for sample 1 = average of (sample 0 + sample 2) / 2  
        #   - Baseline for sample 2 = average of (sample 0 + sample 1) / 2
        # Group 2 (samples 3,4,5):
        #   - Similar leave-one-out calculation
        
        # Since RLOO reduces variance, verify output stability
        # self.assertEqual(advantages.shape, (batch_size, seq_len))
        # self.assertEqual(returns.shape, (batch_size, seq_len))
        # self.assertTrue(torch.isfinite(advantages).all())
        # self.assertTrue(torch.isfinite(returns).all())
        
        # Verify whitening effect
        masked_advantages = advantages[response_mask]
        self.assertAlmostEqual(masked_advantages.mean().item(), 0.0, places=3)
        self.assertAlmostEqual(masked_advantages.std().item(), 1.0, places=3)
        
        # Verify cumulative property of returns: each position contains cumulative reward from that position to end of sequence
        for i in range(batch_size):
            row_returns = returns[i, :]
            # Returns should be finite values
            self.assertTrue(torch.isfinite(row_returns).all())

    def test_zero_coefficient_input_output(self):
        """Test specific input/output samples for zero coefficient case"""
        # Test case: When coefficient is zero, corresponding reward calculation should be skipped
        n_samples = 2
        batch_size = 4
        seq_len = 3
        
        data = MockDataProto()
        data.batch = {
            "rm_scores": torch.tensor([
                [1.0, 2.0, 3.0],
                [2.0, 3.0, 1.0],
                [3.0, 1.0, 2.0],
                [1.0, 3.0, 2.0]
            ]),
            "acc": torch.tensor([0.8, 0.6, 0.9, 0.4]),
            "prompts": torch.ones(batch_size, 1, dtype=torch.long),
            "attention_mask": torch.ones(batch_size, seq_len + 1, dtype=torch.long),
        }
        
        response_mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
        
        # Test case 1: Use only RM reward
        config1 = MagicMock()
        config1.algorithm.reward_dpo_coef = 1.0
        config1.algorithm.reward_gt_coef = 0.0
        
        advantages1, returns1 = compute_rloo_advantage_return(data, response_mask, n_samples, config1)
        
        # Test case 2: Use only accuracy reward  
        config2 = MagicMock()
        config2.algorithm.reward_dpo_coef = 0.0
        config2.algorithm.reward_gt_coef = 1.0
        
        advantages2, returns2 = compute_rloo_advantage_return(data, response_mask, n_samples, config2)
        
        # Test case 3: Both coefficients are zero
        config3 = MagicMock()
        config3.algorithm.reward_dpo_coef = 0.0
        config3.algorithm.reward_gt_coef = 0.0
        
        advantages3, returns3 = compute_rloo_advantage_return(data, response_mask, n_samples, config3)
        
        torch.set_printoptions(precision=30, sci_mode=False)
        # print(advantages1)
        # print(returns1)
        # print(advantages2)
        # print(returns2)
        # print(advantages3)
        # print(returns3)

        expected_advantages1 = torch.tensor([[-0.124646395444869995117187500000,  1.371110320091247558593750000000,
          1.371110320091247558593750000000],
        [-0.124646395444869995117187500000, -0.124646395444869995117187500000,
         -1.620403170585632324218750000000],
        [-0.124646395444869995117187500000, -1.620403170585632324218750000000,
         -0.124646395444869995117187500000],
        [-0.124646395444869995117187500000,  1.371110320091247558593750000000,
         -0.124646395444869995117187500000]])

        torch.testing.assert_close(advantages1, expected_advantages1, atol=1e-30, rtol=1e-30)

        expected_returns1 = torch.tensor([[ 0.,  2.,  2.],
        [ 0.,  0., -2.],
        [ 0., -2.,  0.],
        [ 0.,  2.,  0.]])

        torch.testing.assert_close(returns1, expected_returns1, atol=1e-30, rtol=1e-30)

        expected_advantages2 = torch.tensor([[ 0.502865254878997802734375000000,  0.502865254878997802734375000000,
          0.502865254878997802734375000000],
        [-0.502865493297576904296875000000, -0.502865493297576904296875000000,
         -0.502865493297576904296875000000],
        [ 1.257163405418395996093750000000,  1.257163405418395996093750000000,
          1.257163405418395996093750000000],
        [-1.257163286209106445312500000000, -1.257163286209106445312500000000,
         -1.257163286209106445312500000000]])

        torch.testing.assert_close(advantages2, expected_advantages2, atol=1e-30, rtol=1e-30)

        expected_returns2 = torch.tensor([[ 0.199999928474426269531250000000,  0.199999928474426269531250000000,
          0.199999928474426269531250000000],
        [-0.200000047683715820312500000000, -0.200000047683715820312500000000,
         -0.200000047683715820312500000000],
        [ 0.500000000000000000000000000000,  0.500000000000000000000000000000,
          0.500000000000000000000000000000],
        [-0.499999940395355224609375000000, -0.499999940395355224609375000000,
         -0.499999940395355224609375000000]])

        torch.testing.assert_close(returns2, expected_returns2, atol=1e-30, rtol=1e-30)

        expected_advantages3 = torch.tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])

        torch.testing.assert_close(advantages3, expected_advantages3, atol=1e-30, rtol=1e-30)

        expected_returns3 = torch.tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])

        torch.testing.assert_close(returns3, expected_returns3, atol=1e-30, rtol=1e-30)

        # Verify that all cases produce valid output
        # for adv, ret in [(advantages1, returns1), (advantages2, returns2), (advantages3, returns3)]:
        #     self.assertEqual(adv.shape, (batch_size, seq_len))
        #     self.assertEqual(ret.shape, (batch_size, seq_len))
        #     self.assertTrue(torch.isfinite(adv).all())
        #     self.assertTrue(torch.isfinite(ret).all())
        
        # # Verify that different coefficient combinations produce different results
        # self.assertFalse(torch.allclose(advantages1, advantages2, atol=1e-6))
        # self.assertFalse(torch.allclose(advantages1, advantages3, atol=1e-6))
        
        # Verify special properties of zero coefficient case
        # When both coefficients are 0, rewards should all be 0, but still go through RLOO and whitening processing
        masked_advantages3 = advantages3[response_mask]
        if len(masked_advantages3) > 0:
            # Even if rewards are 0, whitening processing should still work (if there is sufficient variance)
            if masked_advantages3.std() > 1e-8:
                self.assertAlmostEqual(masked_advantages3.mean().item(), 0.0, places=4)


if __name__ == "__main__":
    unittest.main()
