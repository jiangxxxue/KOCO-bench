# VisualQuality-R1 Test Code

This directory contains test code for the core algorithm functions of the VisualQuality-R1 project. These tests are used to verify whether the code generated by LLM meets the requirements.

## Test File Description

### 1. test_fidelity_reward.py
Test `fidelity_reward` function - Calculate ranking fidelity reward between a single pair of samples

**Test Content:**
- Basic fidelity reward calculation
- Cases where prediction ranking is correct/incorrect
- Equal quality cases (gt = 0.5)
- Impact of variance on reward
- Tensor input processing
- Edge cases (very small variance, zero variance)
- Different ground truth ranking relationships (gt = 0.0, 0.5, 1.0)
- Symmetry verification

**Key Test Points:**
- Reward value range should be within [0, 2.0]
- Reward close to 2.0 when prediction is correct
- Lower reward when prediction is incorrect
- All outputs should be finite

### 2. test_accuracy_reward.py
Test `accuracy_reward` function - Batch calculate fidelity rewards for all samples

**Test Content:**
- Basic accuracy reward calculation
- Cases where prediction is correct
- Impact of variance in different generation results
- Parsing answers in different formats
- Pairwise comparison logic
- Error handling (unparseable answers)
- Single sample cases

**Key Test Points:**
- Able to extract scores from `<answer>` tags
- Able to handle answer text in different formats
- Pairwise comparison of all samples
- Calculate mean and variance for each sample
- Use fidelity_reward to calculate ranking consistency

### 3. test_format_reward.py
Test `format_reward` function - Check if generated responses meet specified format requirements

**Test Content:**
- Correctly formatted responses (containing `<think>` and `<answer>` tags)
- Missing think tag
- Missing answer tag
- Incorrect tag order
- Extra content before/after tags
- Multi-line content handling
- Empty tags
- Nested tags
- Special characters and Chinese text handling
- Batch processing
- Handling of different whitespace characters

**Key Test Points:**
- Correct format returns 1.0
- Incorrect format returns 0.0
- Use regex `fullmatch` for strict matching
- Allow whitespace between `<think>` and `<answer>`

### 4. test_generate_and_score_completions.py
Test `VLMGRPOTrainer._generate_and_score_completions` function - Generate multiple responses and calculate rewards

**Test Content:**
- Input data preparation logic
- Image loading and resizing (minimum 28×28 pixels)
- Completion generation simulation
- EOS token masking logic
- Reward calculation simulation
- Advantage calculation
- Multimodal input extraction
- Output data structure

**Key Test Points:**
- Images smaller than 28×28 need to be resized
- Content after EOS token should be masked
- Rewards come from sum of multiple reward functions
- Advantages computed by normalizing rewards (grouped by prompt)
- Output contains prompt_ids, completion_ids, advantages, etc.

### 5. test_compute_loss.py
Test `VLMGRPOTrainer.compute_loss` function - Calculate loss function for GRPO training

**Test Content:**
- Policy ratio calculation
- Ratio clipping logic
- PPO loss calculation
- KL divergence penalty calculation
- Masked loss calculation
- Final loss aggregation
- Clipped ratio metrics
- Impact of advantages on loss

**Key Test Points:**
- Policy ratio = exp(log_prob - old_log_prob)
- Ratio clipped to [1-ε_low, 1+ε_high]
- PPO loss = -min(ratio × advantage, clipped_ratio × advantage)
- KL penalty = β × KL(ref || current)
- Loss considers completion_mask

### 6. test_repeat_random_sampler.py
Test `RepeatRandomSampler.__iter__` function - Custom sampler implementation

**Test Content:**
- Basic sampling functionality
- mini_repeat_count parameter
- repeat_count parameter
- Multi-GPU sampling
- Prefix-based sampling (KADID-10K dataset)
- Mixed dataset sampling
- Sampler length calculation
- Deterministic sampling with seed

**Key Test Points:**
- Each GPU's batch can come from different datasets
- Support sampling grouped by image prefix
- Each index repeated mini_repeat_count times
- Each batch repeated repeat_count times
- Same seed produces same sampling sequence

## Running Tests

### Run All Tests (with Detailed Statistics)
Recommended method, generates detailed statistical report:
```bash
cd open-r1/test_examples/VisualQuality-R1/code/test_code
python3 run_tests_with_stats.py
```

This script will:
- Run all test files
- Display pass/fail status and execution time for each test
- Generate detailed statistical report including:
  - Overall statistics (pass rate, failure count, error count, etc.)
  - Statistics grouped by test file
  - Top 10 slowest tests
  - Detailed error information for failed tests
- Save report to `test_report.txt` file

### Run All Tests (Simple Version)
```bash
cd open-r1/test_examples/VisualQuality-R1/code/test_code
./run_all_tests.sh
```

### Run Individual Tests
```bash
cd open-r1/test_examples/VisualQuality-R1/code/test_code
python3 test_fidelity_reward.py
python3 test_accuracy_reward.py
python3 test_format_reward.py
python3 test_generate_and_score_completions.py
python3 test_compute_loss.py
python3 test_repeat_random_sampler.py
```

### Run Specific Test Case
```bash
python3 test_fidelity_reward.py TestFidelityReward.test_fidelity_reward_basic
```

### View Test Report
After running `run_tests_with_stats.py`, a `test_report.txt` file will be generated containing complete test results and statistics:
```bash
cat test_report.txt
```

## Test Design Principles

1. **Direct Import Testing**: Test code directly imports functions from original code rather than copying
2. **Independence**: Each test case is independent and can be run separately
3. **Coverage**: Tests cover normal cases, edge cases, and error cases
4. **Verifiability**: Test results are clear and easy to determine pass or fail
5. **Reference PURE**: Test code style references verl/test_examples/PURE/code/test_code

## Notes

1. Test code assumes source code is located in `../src/open-r1-multimodal/src/` directory
2. Some tests use mock objects to avoid complex dependencies
3. Tests mainly focus on logical correctness of functions, not performance
4. For tests involving randomness, fixed random seeds are used to ensure reproducibility

## Test Result Interpretation

- **PASS**: Test passed, function implementation meets requirements
- **FAIL**: Test failed, function implementation has issues

When tests fail, specific error information is displayed, including:
- Expected value vs actual value
- Failed assertions
- Error stack trace

## Integration with LLM Code Generation

The main purpose of these test codes is to verify LLM-generated code:

1. During training data construction, we remove function implementations described in 03_algorithm_and_core_methods.md
2. Let LLM learn the open-r1 framework and VisualQuality-R1 base code
3. Let LLM generate implementations of these core functions
4. Use these test codes to verify if LLM-generated code is correct

Passing tests means LLM successfully understood the requirements and generated correct implementation.
