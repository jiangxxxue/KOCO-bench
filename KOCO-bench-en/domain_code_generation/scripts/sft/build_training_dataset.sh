#!/bin/bash
# æ„å»ºæ¡†æ¶è®­ç»ƒæ•°æ®é›† - æ”¯æŒå¤šä¸ªæ¡†æ¶/repo
#
# ä½¿ç”¨æ–¹æ³•:
#   é»˜è®¤æ„å»º verl æ¡†æ¶:
#     ./scripts/build_verl_training_dataset.sh
#
#   æ„å»ºå…¶ä»–æ¡†æ¶:
#     FRAMEWORK=tensorrt_model_optimizer ./scripts/build_verl_training_dataset.sh
#
#   æŒ‡å®šç‰¹å®šçš„ repo åç§°:
#     FRAMEWORK=verl REPO_NAME=custom-repo ./scripts/build_verl_training_dataset.sh
#
#   è®¾ç½®æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰:
#     MAX_FILE_SIZE=2097152 ./scripts/build_verl_training_dataset.sh

set -e

# ========================================
# é…ç½®å˜é‡
# ========================================

# æ¡†æ¶åç§°
FRAMEWORK="${FRAMEWORK:-verl}"

# Repo åç§°ï¼ˆçŸ¥è¯†åº“ä¸­çš„ç›®å½•åï¼‰
REPO_NAME="${REPO_NAME:-${FRAMEWORK}-main}"

FRAMEWORK=verl
REPO_NAME=verl-main

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
SCRIPT_DIR="${PROJECT_ROOT}/scripts"

# æºç›®å½•ï¼šçŸ¥è¯†åº“
SOURCE_DIR="${PROJECT_ROOT}/${FRAMEWORK}/knowledge_corpus/${REPO_NAME}"

# è¾“å‡ºç›®å½•
OUTPUT_DIR="${PROJECT_ROOT}/scripts/data/${FRAMEWORK}"
OUTPUT_FILE="${OUTPUT_DIR}/${FRAMEWORK}_training_dataset.jsonl"

# å‚æ•°
MAX_FILE_SIZE="${MAX_FILE_SIZE:-1048576}"  # 1MB
TOKENIZER_PATH="${TOKENIZER_PATH:-/workspace/data/models/Qwen2.5-Coder-7B-Instruct}"

# ========================================
# æ‰§è¡Œæ„å»º
# ========================================

echo "========================================================"
echo "æ„å»ºè®­ç»ƒæ•°æ®é›†"
echo "========================================================"
echo "æ¡†æ¶: ${FRAMEWORK}"
echo "Repo: ${REPO_NAME}"
echo "æºç›®å½•: ${SOURCE_DIR}"
echo "è¾“å‡ºæ–‡ä»¶: ${OUTPUT_FILE}"
echo "æœ€å¤§æ–‡ä»¶å¤§å°: ${MAX_FILE_SIZE} bytes"
echo "Tokenizeræ¨¡å‹: ${TOKENIZER_PATH}"
echo "========================================================"
echo ""

# æ£€æŸ¥æºç›®å½•
if [ ! -d "$SOURCE_DIR" ]; then
    echo "âŒ é”™è¯¯: æºç›®å½•ä¸å­˜åœ¨: $SOURCE_DIR"
    echo ""
    echo "æç¤º: è¯·ç¡®ä¿ä»¥ä¸‹è·¯å¾„å­˜åœ¨:"
    echo "  ${PROJECT_ROOT}/${FRAMEWORK}/knowledge_corpus/${REPO_NAME}"
    echo ""
    echo "æˆ–è€…ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šå…¶ä»–æ¡†æ¶/repo:"
    echo "  FRAMEWORK=your_framework REPO_NAME=your_repo ./scripts/build_verl_training_dataset.sh"
    exit 1
fi

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$OUTPUT_DIR"

# è¿è¡Œæ•°æ®é›†æ„å»ºå™¨
cd "${SCRIPT_DIR}/sft"
python3 finetune_dataset_builder.py \
    --source-dir "$SOURCE_DIR" \
    --output-file "$OUTPUT_FILE" \
    --format jsonl \
    --max-file-size "$MAX_FILE_SIZE" \
    --tokenizer-path "$TOKENIZER_PATH"

if [ $? -eq 0 ]; then
    echo ""
    echo "========================================================"
    echo "âœ… æ•°æ®é›†æ„å»ºå®Œæˆï¼"
    echo "========================================================"
    echo "æ•°æ®æ–‡ä»¶: ${OUTPUT_FILE}"
    echo "ç»Ÿè®¡æ–‡ä»¶: ${OUTPUT_FILE%.jsonl}.stats.json"
    echo ""
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if [ -f "${OUTPUT_FILE%.jsonl}.stats.json" ]; then
        echo "ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:"
        cat "${OUTPUT_FILE%.jsonl}.stats.json" | python3 -c "
import json, sys
stats = json.load(sys.stdin)
print(f\"  æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
print(f\"  å¤„ç†æˆåŠŸ: {stats['processed_files']}")
print(f\"  è·³è¿‡æ–‡ä»¶: {stats['skipped_files']}")
print(f\"  æ€»å­—ç¬¦æ•°: {stats['total_size_chars']:,}")
print(f\"  æ€»è¡Œæ•°: {stats['total_lines']:,}")
if 'total_tokens' in stats and stats['total_tokens'] > 0:
    print(f\"  æ€»Tokenæ•°: {stats['total_tokens']:,}")
    print(f\"  å¹³å‡æ¯æ–‡ä»¶Tokenæ•°: {stats.get('average_tokens_per_file', 0):.1f}")
print(f\"  æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:\")
for ftype, count in sorted(stats['file_types'].items(), key=lambda x: x[1], reverse=True):
    print(f\"    {ftype}: {count}\")
"
    fi
    
    echo ""
    echo "========================================================"
else
    echo ""
    echo "âŒ æ•°æ®é›†æ„å»ºå¤±è´¥"
    exit 1
fi

