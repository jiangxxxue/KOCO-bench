# æ¨ç†æœåŠ¡å™¨ä½¿ç”¨è¯´æ˜

## ğŸ“‹ æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªåŸºäºæœåŠ¡å™¨-å®¢æˆ·ç«¯æ¶æ„çš„ä»£ç ç”Ÿæˆç³»ç»Ÿï¼Œè§£å†³äº†åŸæ¥æ¯æ¬¡éƒ½è¦é‡æ–°åŠ è½½æ¨¡å‹çš„é—®é¢˜ã€‚

### ä¼˜åŠ¿
- âœ… **æ¨¡å‹åªåŠ è½½ä¸€æ¬¡**ï¼šæœåŠ¡å™¨å¯åŠ¨åä¸€ç›´è¿è¡Œï¼Œä¸éœ€è¦é‡å¤åŠ è½½æ¨¡å‹
- âœ… **æé«˜æ•ˆç‡**ï¼šæ‰¹é‡ç”Ÿæˆæ—¶å¤§å¹…æå‡é€Ÿåº¦
- âœ… **èµ„æºå¤ç”¨**ï¼šå¤šä¸ªä»»åŠ¡å¯ä»¥å…±äº«åŒä¸€ä¸ªæ¨¡å‹æœåŠ¡
- âœ… **ä¾¿äºç®¡ç†**ï¼šç‹¬ç«‹çš„å¯åŠ¨/åœæ­¢è„šæœ¬ï¼Œæ˜“äºæ§åˆ¶

### ç³»ç»Ÿç»„æˆ

1. **inference_server.py** - æ¨ç†æœåŠ¡å™¨ï¼ˆåå°è¿è¡Œï¼ŒåŠ è½½æ¨¡å‹å¹¶æä¾› APIï¼‰
2. **inference_client.py** - æ¨ç†å®¢æˆ·ç«¯ï¼ˆè¯·æ±‚æœåŠ¡å™¨ç”Ÿæˆä»£ç ï¼‰
3. **start_inference_server.sh** - å¯åŠ¨æœåŠ¡å™¨è„šæœ¬
4. **stop_inference_server.sh** - åœæ­¢æœåŠ¡å™¨è„šæœ¬
5. **run_batch_code_generation_with_server.sh** - æ–°çš„æ‰¹é‡ç”Ÿæˆè„šæœ¬ï¼ˆä½¿ç”¨æœåŠ¡å™¨ï¼‰

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: å¯åŠ¨æ¨ç†æœåŠ¡å™¨

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®å¯åŠ¨
bash scripts/inference/start_inference_server.sh

# æˆ–è€…æŒ‡å®šæ¨¡å‹è·¯å¾„
MODEL_PATH=../models/your-model bash scripts/inference/start_inference_server.sh

# æˆ–è€…æŒ‡å®šç«¯å£
SERVER_PORT=8001 MODEL_PATH=../models/your-model bash scripts/inference/start_inference_server.sh
```

**å¯åŠ¨å‚æ•°ï¼ˆç¯å¢ƒå˜é‡ï¼‰**ï¼š
- `MODEL_PATH`: æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: `../models/qwen2.5-coder-7b-modelopt-sft`ï¼‰
- `SERVER_PORT`: æœåŠ¡å™¨ç«¯å£ï¼ˆé»˜è®¤: `8000`ï¼‰
- `SERVER_HOST`: æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: `0.0.0.0`ï¼‰
- `MAX_CONTEXT_LEN`: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé»˜è®¤: `4096`ï¼‰
- `CUDA_VISIBLE_DEVICES`: GPU è®¾å¤‡ï¼ˆé»˜è®¤: `0,1,2,3`ï¼‰

**æ³¨æ„äº‹é¡¹**ï¼š
- é¦–æ¬¡å¯åŠ¨éœ€è¦åŠ è½½æ¨¡å‹ï¼Œå¤§çº¦éœ€è¦ 1-2 åˆ†é’Ÿ
- å¯åŠ¨åä¼šè‡ªåŠ¨è¿›è¡Œå¥åº·æ£€æŸ¥
- æ—¥å¿—ä¿å­˜åœ¨ `../logs/inference_server.log`
- PID ä¿å­˜åœ¨ `../logs/inference_server.pid`

### æ­¥éª¤ 2: æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€

```bash
# æ–¹æ³• 1: ä½¿ç”¨ curl æ£€æŸ¥å¥åº·çŠ¶æ€
curl http://localhost:8000/health

# æ–¹æ³• 2: æŸ¥çœ‹æ—¥å¿—
tail -f ../logs/inference_server.log

# æ–¹æ³• 3: æ£€æŸ¥è¿›ç¨‹
cat ../logs/inference_server.pid
ps aux | grep inference_server
```

### æ­¥éª¤ 3: æ‰¹é‡ç”Ÿæˆä»£ç 

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
bash scripts/run_batch_code_generation_with_server.sh

# æˆ–è€…æŒ‡å®šæ¡†æ¶å’Œå‚æ•°
FRAMEWORK=verl \
NUM_COMPLETIONS=4 \
TEMPERATURE=0.2 \
bash scripts/run_batch_code_generation_with_server.sh
```

**ç”Ÿæˆå‚æ•°ï¼ˆç¯å¢ƒå˜é‡ï¼‰**ï¼š
- `FRAMEWORK`: æ¡†æ¶åç§°ï¼ˆé»˜è®¤: `verl`ï¼‰
- `MODEL_NAME`: æ¨¡å‹åç§°ï¼ˆé»˜è®¤: `qwen2.5-coder-7b-verl-ntp`ï¼‰
- `SERVER_URL`: æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: `http://localhost:8000`ï¼‰
- `NUM_COMPLETIONS`: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆæ•°é‡ï¼ˆé»˜è®¤: `1`ï¼‰
- `MAX_TOKENS`: æœ€å¤§ç”Ÿæˆ tokensï¼ˆé»˜è®¤: `2048`ï¼‰
- `TEMPERATURE`: é‡‡æ ·æ¸©åº¦ï¼ˆé»˜è®¤: `0.7`ï¼‰
- `TOP_P`: Top-p é‡‡æ ·ï¼ˆé»˜è®¤: `0.95`ï¼‰
- `BATCH_SIZE`: æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤: `1`ï¼‰
- `SKIP_EXISTING`: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼ˆé»˜è®¤: `false`ï¼‰

### æ­¥éª¤ 4: åœæ­¢æœåŠ¡å™¨

```bash
# æ­£å¸¸åœæ­¢æœåŠ¡å™¨
bash scripts/inference/stop_inference_server.sh

# æˆ–è€…ç›´æ¥ kill è¿›ç¨‹
kill $(cat ../logs/inference_server.pid)
```

---

## ğŸ“– è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨æµç¨‹

```bash
# 1. å¯åŠ¨æœåŠ¡å™¨
bash scripts/start_inference_server.sh

# ç­‰å¾…æç¤º "æ¨ç†æœåŠ¡å™¨å‡†å¤‡å°±ç»ªï¼"

# 2. è¿è¡Œæ‰¹é‡ç”Ÿæˆ
bash scripts/run_batch_code_generation_with_server.sh

# 3. æŸ¥çœ‹ç»“æœ
ls -lh ../data/verl/qwen2.5-coder-7b-verl-ntp/*_output.jsonl

# 4. å®Œæˆååœæ­¢æœåŠ¡å™¨
bash scripts/stop_inference_server.sh
```

### ç¤ºä¾‹ 2: ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹

```bash
# 1. å¯åŠ¨æœåŠ¡å™¨ï¼ˆä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ï¼‰
MODEL_PATH=../models/qwen2.5-coder-7b-modelopt-sft \
bash scripts/start_inference_server.sh

# 2. è¿è¡Œæ‰¹é‡ç”Ÿæˆï¼ˆåŒ¹é…æ¨¡å‹åç§°ï¼‰
MODEL_NAME=qwen2.5-coder-7b-modelopt-sft \
bash scripts/run_batch_code_generation_with_server.sh
```

### ç¤ºä¾‹ 3: ç”Ÿæˆå¤šä¸ªè¡¥å…¨ç”¨äºè¯„ä¼°

```bash
# 1. ç¡®ä¿æœåŠ¡å™¨å·²å¯åŠ¨
curl http://localhost:8000/health

# 2. ç”Ÿæˆæ¯ä¸ªæ ·æœ¬ 4 ä¸ªè¡¥å…¨ï¼Œä½¿ç”¨è¾ƒä½æ¸©åº¦
NUM_COMPLETIONS=4 \
TEMPERATURE=0.2 \
MAX_TOKENS=2048 \
bash scripts/run_batch_code_generation_with_server.sh
```

### ç¤ºä¾‹ 4: ä½¿ç”¨æ‰¹å¤„ç†åŠ é€Ÿ

```bash
# æ¯æ¬¡è¯·æ±‚å¤„ç† 4 ä¸ªæ ·æœ¬ï¼ˆå¦‚æœæ•°æ®è¾ƒå¤šå¯ä»¥åŠ é€Ÿï¼‰
BATCH_SIZE=4 \
bash scripts/run_batch_code_generation_with_server.sh
```

### ç¤ºä¾‹ 5: å•ç‹¬ä½¿ç”¨å®¢æˆ·ç«¯

```bash
# å¯¹å•ä¸ªæ–‡ä»¶ç”Ÿæˆä»£ç 
python scripts/inference_client.py \
    --server_url http://localhost:8000 \
    --input_file ./../data/verl/algorithm_methods_data_prime.jsonl \
    --output_file ./output/prime_output.jsonl \
    --num_completions 2 \
    --temperature 0.2
```

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### ä½¿ç”¨ä¸åŒçš„ç«¯å£

```bash
# å¯åŠ¨æœåŠ¡å™¨åœ¨ 8001 ç«¯å£
SERVER_PORT=8001 bash scripts/start_inference_server.sh

# å®¢æˆ·ç«¯è¿æ¥åˆ° 8001 ç«¯å£
SERVER_URL=http://localhost:8001 \
bash scripts/run_batch_code_generation_with_server.sh
```

### å¤šä¸ªæœåŠ¡å™¨åŒæ—¶è¿è¡Œ

```bash
# æœåŠ¡å™¨ 1: ç«¯å£ 8000ï¼Œä½¿ç”¨ GPU 0,1
CUDA_VISIBLE_DEVICES=0,1 \
SERVER_PORT=8000 \
MODEL_PATH=../models/model1 \
bash scripts/start_inference_server.sh

# æœåŠ¡å™¨ 2: ç«¯å£ 8001ï¼Œä½¿ç”¨ GPU 2,3
CUDA_VISIBLE_DEVICES=2,3 \
SERVER_PORT=8001 \
MODEL_PATH=../models/model2 \
bash scripts/start_inference_server.sh

# ä½¿ç”¨ä¸åŒçš„æœåŠ¡å™¨ç”Ÿæˆ
SERVER_URL=http://localhost:8000 bash scripts/run_batch_code_generation_with_server.sh
SERVER_URL=http://localhost:8001 bash scripts/run_batch_code_generation_with_server.sh
```

### è¿œç¨‹æœåŠ¡å™¨

```bash
# åœ¨æœåŠ¡å™¨ A å¯åŠ¨æ¨ç†æœåŠ¡
SERVER_HOST=0.0.0.0 SERVER_PORT=8000 bash scripts/start_inference_server.sh

# åœ¨æœåŠ¡å™¨ B è¯·æ±‚ç”Ÿæˆ
SERVER_URL=http://server-a-ip:8000 \
bash scripts/run_batch_code_generation_with_server.sh
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: æœåŠ¡å™¨å¯åŠ¨å¤±è´¥

**æ£€æŸ¥æ—¥å¿—**ï¼š
```bash
tail -50 ./logs/inference_server.log
```

**å¸¸è§åŸå› **ï¼š
- GPU å†…å­˜ä¸è¶³ï¼šå‡å°‘ GPU æ•°é‡æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹
- æ¨¡å‹è·¯å¾„é”™è¯¯ï¼šæ£€æŸ¥ `MODEL_PATH` æ˜¯å¦æ­£ç¡®
- ç«¯å£è¢«å ç”¨ï¼šæ›´æ¢ç«¯å£æˆ–åœæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹

### é—®é¢˜ 2: å®¢æˆ·ç«¯è¿æ¥å¤±è´¥

**æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€**ï¼š
```bash
# æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
curl http://localhost:8000/health

# æ£€æŸ¥è¿›ç¨‹
ps aux | grep inference_server
```

**å¸¸è§åŸå› **ï¼š
- æœåŠ¡å™¨æœªå¯åŠ¨ï¼šå…ˆè¿è¡Œ `start_inference_server.sh`
- ç«¯å£ä¸åŒ¹é…ï¼šç¡®ä¿ `SERVER_URL` å’Œ `SERVER_PORT` ä¸€è‡´
- é˜²ç«å¢™é˜»æ­¢ï¼šæ£€æŸ¥é˜²ç«å¢™è®¾ç½®

### é—®é¢˜ 3: ç”Ÿæˆé€Ÿåº¦æ…¢

**ä¼˜åŒ–å»ºè®®**ï¼š
- å¢åŠ  `BATCH_SIZE`ï¼ˆä¾‹å¦‚è®¾ç½®ä¸º 4 æˆ– 8ï¼‰
- ä½¿ç”¨å¤šä¸ª GPUï¼ˆé€šè¿‡ `CUDA_VISIBLE_DEVICES`ï¼‰
- å‡å°‘ `MAX_TOKENS`
- é™ä½ `NUM_COMPLETIONS`

### é—®é¢˜ 4: æœåŠ¡å™¨å†…å­˜æº¢å‡º

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å°‘ GPU æ•°é‡
- é™ä½ `MAX_CONTEXT_LEN`
- ä½¿ç”¨é‡åŒ–æ¨¡å‹
- å‡å° `BATCH_SIZE`

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### åŸæ–¹æ¡ˆ vs æ–°æ–¹æ¡ˆ

| é¡¹ç›® | åŸæ–¹æ¡ˆ | æ–°æ–¹æ¡ˆï¼ˆæœåŠ¡å™¨ï¼‰ | æå‡ |
|------|--------|-----------------|------|
| æ¨¡å‹åŠ è½½æ¬¡æ•° | N æ¬¡ï¼ˆæ¯ä¸ªæ–‡ä»¶ï¼‰ | 1 æ¬¡ | N å€ |
| æ€»è€—æ—¶ï¼ˆ10 ä¸ªæ–‡ä»¶ï¼‰ | ~30 åˆ†é’Ÿ | ~5 åˆ†é’Ÿ | 6 å€ |
| GPU åˆ©ç”¨ç‡ | ä½ï¼ˆåŠ è½½æ—¶é—´é•¿ï¼‰ | é«˜ | - |
| èµ„æºå¤ç”¨ | å¦ | æ˜¯ | - |

---

## ğŸ”„ ä¸åŸè„šæœ¬å¯¹æ¯”

### åŸè„šæœ¬ï¼ˆrun_batch_code_generation_improved.shï¼‰
- âŒ æ¯ä¸ªæ–‡ä»¶éƒ½é‡æ–°åŠ è½½æ¨¡å‹
- âŒ æ•ˆç‡ä½ä¸‹
- âœ… ä½¿ç”¨ç®€å•ï¼ˆå•ä¸ªè„šæœ¬ï¼‰

### æ–°è„šæœ¬ï¼ˆrun_batch_code_generation_with_server.shï¼‰
- âœ… æ¨¡å‹åªåŠ è½½ä¸€æ¬¡
- âœ… æ•ˆç‡é«˜
- âœ… æ”¯æŒå¤šä»»åŠ¡å…±äº«
- âš ï¸ éœ€è¦å…ˆå¯åŠ¨æœåŠ¡å™¨

**å»ºè®®**ï¼š
- å•æ¬¡ç”Ÿæˆå°‘é‡æ–‡ä»¶ï¼šå¯ä»¥ä½¿ç”¨åŸè„šæœ¬
- æ‰¹é‡ç”Ÿæˆå¤šä¸ªæ–‡ä»¶ï¼š**å¼ºçƒˆæ¨èä½¿ç”¨æ–°è„šæœ¬**
- é¢‘ç¹ç”Ÿæˆä»£ç ï¼š**å¼ºçƒˆæ¨èä½¿ç”¨æ–°è„šæœ¬**

---

## ğŸ“ å®Œæ•´å·¥ä½œæµç¤ºä¾‹

```bash
# ========================================
# å®Œæ•´çš„ä»£ç ç”Ÿæˆå’Œè¯„ä¼°æµç¨‹
# ========================================

# 1. å¯åŠ¨æ¨ç†æœåŠ¡å™¨
echo "å¯åŠ¨æ¨ç†æœåŠ¡å™¨..."
MODEL_PATH=../models/qwen2.5-coder-7b-modelopt-sft \
bash scripts/start_inference_server.sh

# 2. ç­‰å¾…æœåŠ¡å™¨å°±ç»ªï¼ˆæ£€æŸ¥å¥åº·çŠ¶æ€ï¼‰
echo "æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€..."
curl http://localhost:8000/health

# 3. æ‰¹é‡ç”Ÿæˆä»£ç ï¼ˆç”Ÿæˆ 4 ä¸ªè¡¥å…¨ç”¨äºè¯„ä¼°ï¼‰
echo "æ‰¹é‡ç”Ÿæˆä»£ç ..."
FRAMEWORK=verl \
MODEL_NAME=qwen2.5-coder-7b-modelopt-sft \
NUM_COMPLETIONS=4 \
TEMPERATURE=0.2 \
MAX_TOKENS=2048 \
bash scripts/run_batch_code_generation_with_server.sh

# 4. è¿è¡Œæ‰§è¡Œè¯„ä¼°
echo "è¿è¡Œæ‰§è¡Œè¯„ä¼°..."
bash scripts/run_batch_execution_evaluation_pure.sh

# 5. å®Œæˆååœæ­¢æœåŠ¡å™¨
echo "åœæ­¢æœåŠ¡å™¨..."
bash scripts/stop_inference_server.sh

echo "å®Œæˆï¼"
```

---

## ğŸ†˜ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. **æŸ¥çœ‹æ—¥å¿—**ï¼š`tail -f ./logs/inference_server.log`
2. **æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€**ï¼š`curl http://localhost:8000/health`
3. **æŸ¥çœ‹è¿›ç¨‹**ï¼š`ps aux | grep inference_server`
4. **æ¸…ç†é‡å¯**ï¼š
   ```bash
   bash scripts/stop_inference_server.sh
   bash scripts/start_inference_server.sh
   ```

---

## ğŸ“š API æ–‡æ¡£

æœåŠ¡å™¨å¯åŠ¨åï¼Œå¯ä»¥è®¿é—®äº¤äº’å¼ API æ–‡æ¡£ï¼š
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### ä¸»è¦ API ç«¯ç‚¹

#### GET /health
å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8000/health
```

å“åº”ï¼š
```json
{
  "status": "healthy",
  "model": "qwen2.5-coder-7b-modelopt-sft",
  "device": "cuda:0"
}
```

#### POST /generate
ç”Ÿæˆä»£ç è¡¥å…¨

```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompts": ["def hello():\n    "],
    "num_completions": 2,
    "max_tokens": 512,
    "temperature": 0.2,
    "top_p": 0.95
  }'
```

å“åº”ï¼š
```json
{
  "completions": [
    ["print('Hello, World!')", "return 'Hello'"]
  ],
  "model": "qwen2.5-coder-7b-modelopt-sft",
  "status": "success"
}
```

