# VLM-R1 Algorithm Core Function Descriptions

# FUNCTION: default_accuracy_reward

## Function Overview
Default accuracy reward function that supports multiple verification methods: symbolic verification, numerical matching, multiple-choice matching, and fuzzy matching. First attempts symbolic verification, then selects appropriate verification method based on answer type if that fails.

## Function Signature
def default_accuracy_reward(content, sol, **kwargs):

## Input Parameters
- `content`: Complete response text generated by the model, may contain `<think>` and `<answer>` tags
- `sol`: Standard answer text, may contain `<answer>` tags
- `**kwargs`: Additional parameters (unused)

## Detailed Description
The function first attempts symbolic verification, parsing student answer and ground truth answer as mathematical expressions for equivalence verification. If symbolic verification fails, the function selects different verification methods based on the ground truth answer type.

For answers containing numbers, it calls the numerical reward function for exact matching, and uses Levenshtein distance for fuzzy matching if matching fails. For multiple-choice questions, it calls a function to extract option letters for exact comparison. For pure text answers, it uses Levenshtein distance for fuzzy matching.

Returns 0 when all verification methods fail.

## Output
- `reward`: Float, range 0.0-1.0

## Function Implementation
code/src/open-r1-multimodal/src/open_r1/grpo_jsonl.py:line 787-830

## Test Code
code/test_code/test_default_accuracy_reward.py

---

# FUNCTION: iou_reward

## Function Overview
Calculates IoU (Intersection over Union) between predicted bounding box and ground truth bounding box, used for accuracy evaluation in Referring Expression Comprehension (REC) tasks. Supports automatic scaling adjustment of bounding box coordinates.

## Function Signature
def iou_reward(completions, solution, **kwargs):

## Input Parameters
- `completions`: List of model generation results, each element is a list of dictionaries containing `{"role": "assistant", "content": "..."}`
- `solution`: List of standard answers, each element is a JSON string containing bounding box coordinates
- `**kwargs`: Must contain the following fields:
  - `image_grid_thw`: Image grid size information (for coordinate scaling)
  - `image_path`: List of image paths (for obtaining original image dimensions)

## Detailed Description
The function uses regular expressions to extract bounding box coordinates from the answer tag in completion, and extracts annotated bounding boxes in JSON format from ground truth answer. Coordinate format is [x1, y1, x2, y2], where (x1, y1) is the top-left corner coordinate and (x2, y2) is the bottom-right corner coordinate.

Obtains original image dimensions and model input dimensions (grid count multiplied by 14 pixels), scales predicted bounding box proportionally from input dimensions to original dimensions. Calculates intersection over union between predicted box and ground truth box as reward.

The function logs detailed information including reward value, image path, question description, model output, and ground truth answer in debug mode.

## Output
- `rewards`: List of floats, each element is an IoU value between 0.0-1.0

## Function Implementation
code/src/open-r1-multimodal/src/open_r1/vlm_modules/qwen_module.py:line 108-176

## Test Code
code/test_code/test_iou_reward.py

---

# FUNCTION: detection_score

## Function Overview
Calculates comprehensive score for object detection tasks, considering position accuracy, label accuracy, and completeness (missed detections/false positives). Uses weighted sum to fuse multiple evaluation dimensions.

## Function Signature
def detection_score(content, sol, iou_threshold=0.5, alpha=0.7, beta=0.0, gamma=0.3):

## Input Parameters
- `content`: Response text generated by model, containing JSON-formatted predicted bounding box list
- `sol`: Standard answer text, containing JSON-formatted ground truth bounding box list
- `iou_threshold`: IoU threshold, default 0.5, used to determine if predicted box matches ground truth box
- `alpha`: Position accuracy weight, default 0.7
- `beta`: Label accuracy weight, default 0.0
- `gamma`: Completeness weight, default 0.3

## Detailed Description
The function extracts JSON code blocks from content and sol, parses them into predicted bounding box list and ground truth bounding box list, each bounding box contains coordinate and label fields. Calculates IoU matrix between all predicted boxes and ground truth boxes. Uses greedy matching algorithm, selecting box pair with maximum IoU each round for matching, requiring IoU to exceed threshold. When matching, compares if labels are consistent, setting IoU of that match to 0 if labels are incorrect. After matching, removes matched boxes from unmatched list until no match satisfying threshold can be found.

Calculates three scores: position score, label score, and completeness score. Completeness score is 1 minus the average of missed detection rate and false positive rate. Final score is weighted average of three scores, with sum of weights normalized.

The function supports handling edge cases such as empty ground truth boxes or empty predicted boxes.

## Output
- `score`: Float, range 0.0-1.0, representing comprehensive score for object detection

## Function Implementation
code/src/open-r1-multimodal/src/open_r1/grpo_jsonl.py:line 435-569

## Test Code
code/test_code/test_detection_score.py

---

# FUNCTION: cosine_reward

## Function Overview
Cosine-based length reward that encourages the model to generate concise answers. For correct answers, shorter length → higher reward; for incorrect answers, shorter length → greater penalty.

## Function Signature
def cosine_reward(content, tokenizer, acc_reward, **kwargs):

## Input Parameters
- `content`: Response text generated by model
- `tokenizer`: Tokenizer, used to calculate text length
- `acc_reward`: Accuracy reward value, used to determine if answer is correct
- `**kwargs`: Additional parameters (unused)

## Detailed Description
The function uses tokenizer to calculate token length of generated content. Determines answer correctness based on whether accuracy reward is greater than or equal to 0.7. When answer is correct, sets reward range from 0.5 to 1.0, with shortest length corresponding to highest reward. When answer is incorrect, sets reward range from 0.0 to -0.5, with shortest length corresponding to lowest penalty.

Uses cosine function to calculate final reward, formula is maximum value minus range multiplied by normalized cosine term. Cosine term is (1 - cos(generated token length × π / cosine function period length)) / 2, providing smooth transition from 0 to 1. Cosine function period length is 1024.

## Output
- `reward`: Float, range 0.5-1.0 for correct answers, range -0.5-0.0 for incorrect answers

## Function Implementation
code/src/open-r1-multimodal/src/open_r1/grpo_jsonl.py:line 571-596

## Test Code
code/test_code/test_cosine_reward.py

---

# FUNCTION: repetition_reward

## Function Overview
Detects repetitive patterns in generated content and penalizes repetitive content. Supports JSON format data (detecting repeated bounding boxes) and pure text data (detecting repeated phrases).

## Function Signature
def repetition_reward(content, **kwargs):

## Input Parameters
- `content`: Response text generated by model
- `**kwargs`: Additional parameters (unused)

## Detailed Description
The function first attempts to extract JSON code blocks, sequentially matching explicit json markers, general code block markers, and JSON array patterns containing bbox_2d and label fields. When parsing JSON, first tries direct parsing, uses repair tool if that fails, and treats as plain text if still failing.

For successfully parsed JSON lists, combines bbox_2d and label of each object into string `f"{bbox_2d}_{label}"`, uses 1-gram to generate all n-grams through sliding window. For plain text or JSON processing failure cases, uses 6-gram, requires at least 6 words, converts text to lowercase and tokenizes before generating sliding window n-grams.

Counts number of unique n-grams and total n-grams, calculates repetition rate as 1 minus unique count divided by total count. Reward is repetition rate multiplied by maximum penalty value (set to -1.0). Returns 0 for empty content, text length less than n-gram size, or total n-grams equal to 0.

## Output
- `reward`: Float, range -1.0-0.0, higher repetition rate → greater penalty

## Function Implementation
code/src/open-r1-multimodal/src/open_r1/grpo_jsonl.py:line 598-689

## Test Code
code/test_code/test_repetition_reward.py

---

# FUNCTION: format_reward

## Function Overview
Checks if generated content conforms to predefined format requirements, i.e., whether it contains complete `<think>...</think><answer>...</answer>` structure.

## Function Signature
def format_reward(completions, **kwargs):

## Input Parameters
- `completions`: List of model generation results, each element is a list of dictionaries containing `{"role": "assistant", "content": "..."}`
- `**kwargs`: Additional parameters (unused)

## Detailed Description
The function uses exact matching to verify if the output format of content extracted from each completion is "think tag followed by answer tag, with possible whitespace in between, no extra content allowed". Matching needs to support cross-line content, allowing newlines inside tags.

The function returns binary reward list, with 1.0 for positions with exact format match, otherwise 0.0. In debug mode, logs each completion's content and format matching result to dedicated format log file. Log file name is `{LOG_PATH}_format.txt`.

## Output
- `rewards`: List of floats, each element is 0.0 or 1.0

## Function Implementation
code/src/open-r1-multimodal/src/open_r1/grpo_jsonl.py:line 888-903

## Test Code
code/test_code/test_format_reward.py

---

# FUNCTION: format_reward_rec

## Function Overview
REC task-specific format reward function that checks if output contains `<think>` tag, `<answer>` tag, and JSON-formatted bounding box coordinates.

## Function Signature
def format_reward_rec(completions, **kwargs):

## Input Parameters
- `completions`: List of model generation results, each element is a list of dictionaries containing `{"role": "assistant", "content": "..."}`
- `**kwargs`: Additional parameters (unused)

## Detailed Description
The function needs to verify if content contains think tag, optional whitespace, answer tag, and answer tag contains JSON object wrapped in curly braces, with object containing bounding box coordinate format [number, number, number, number] consisting of four numbers. Matching supports cross-line content. Uses search rather than exact match, allowing extra content before and after pattern.

The function returns binary reward list, with 1.0 for positions containing required format, otherwise 0.0. In debug mode, logs each completion's content and format check result to dedicated format log file. Log file name is `{LOG_PATH}_format.txt`.

## Output
- `rewards`: List of floats, each element is 0.0 or 1.0

## Function Implementation
code/src/open-r1-multimodal/src/open_r1/vlm_modules/qwen_module.py:line 88-105

## Test Code
code/test_code/test_format_reward_rec.py
