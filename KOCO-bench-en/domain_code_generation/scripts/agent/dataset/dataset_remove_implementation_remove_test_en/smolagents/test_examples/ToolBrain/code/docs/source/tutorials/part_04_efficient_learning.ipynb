{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5530a7-296d-4bdc-bdd3-2e8e345b4b27",
   "metadata": {},
   "source": [
    "# Part 4: Efficient Training with ToolBrain\n",
    "\n",
    "Fine-tuning Large Language Models is a resource-intensive process, often requiring significant GPU memory and long training times. ToolBrain integrates several state-of-the-art optimization techniques to make this process dramatically more efficient. By using them, you can train larger models on smaller GPUs and complete your training runs much faster.\n",
    "\n",
    "This tutorial covers the three key efficiency techniques available in ToolBrain: **LoRA**, **BitsAndBytes**, and **Unsloth**.\n",
    "\n",
    "## 1. LoRA: Low-Rank Adaptation\n",
    "\n",
    "**What it is:** Instead of fine-tuning all the billions of parameters in an LLM, LoRA (Low-Rank Adaptation) freezes the base model and injects small, trainable \"adapter\" layers into its architecture. During training, only these tiny adapter layers are updated.\n",
    "\n",
    "**Why it's useful:**\n",
    "-   **Massive Memory Savings:** Since you are only training a tiny fraction of the parameters (e.g., <1%), the memory required for gradients and optimizer states is drastically reduced.\n",
    "-   **Fast & Portable:** The final output of the training is just the small adapter weights (a few megabytes), not a full multi-gigabyte model. This makes saving, loading, and sharing your fine-tuned model incredibly fast and easy.\n",
    "-   **No Performance Loss:** For most tasks, LoRA can match the performance of full fine-tuning.\n",
    "\n",
    "**How to use it in ToolBrain:**\n",
    "\n",
    "ToolBrain uses the `peft` library from Hugging Face to handle LoRA. You simply define a `LoraConfig` object and pass it to the appropriate configuration in your `Brain`.\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Define the LoRA configuration\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # Target attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# This config is then passed into the Brain's algorithm configuration\n",
    "# (Example from the email search agent)\n",
    "GRPO_CONFIG = {\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"lora_config\": LORA_CONFIG, # Pass the config here\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "ToolBrain will automatically apply this configuration to the model.\n",
    "\n",
    "## 2. BitsAndBytes: 4-bit or 8-bit Quantization (GPU only)\n",
    "\n",
    "**What it is:** Quantization is the process of reducing the numerical precision of the model's weights. BitsAndBytes is a library that allows you to load a model with its weights quantized to 4-bit precision instead of the standard 16-bit or 32-bit.\n",
    "\n",
    "**Why it's useful:**\n",
    "-   **Drastic Memory Reduction:** Loading a model in 4-bit precision can reduce its memory footprint by up to 75%. This often makes the difference between a model fitting on your GPU or not.\n",
    "\n",
    "**How to use it in ToolBrain:**\n",
    "\n",
    "First, we need to load the model in quantized 4-bit or 8-bit\n",
    "\n",
    "```python\n",
    "\n",
    "from smolagents import tool, TransformersModel, CodeAgent\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 0. set bitsandbytes config for low precision inference\n",
    "nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "\n",
    "# 1. Create agent\n",
    "model = TransformersModel(\n",
    "    model_id=\"Qwen/Qwen2.5-0.5B-Instruct\",  # use a bigger model for better results\n",
    "    max_new_tokens=128,\n",
    "    model_kwargs={\"quantization_config\": nf4_config},\n",
    ")\n",
    "```\n",
    "\n",
    "Then to enable bitsandbytes in Brain, use with a simple boolean flag in the `Brain` constructor.\n",
    "\n",
    "```python\n",
    "brain = Brain(\n",
    "    agent,\n",
    "    algorithm=\"GRPO\",\n",
    "    use_bitsandbytes=True, # Enable 4-bit quantization\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "The complete example on how to use bitsandbytes can be found [here](https://github.com/ToolBrain/ToolBrain/blob/main/examples/12_hello_world_bitsandbytes.py)\n",
    "\n",
    "## 3. FP16 training (GPU only)\n",
    "\n",
    "Training large models with full floating-point precision (FP32) can quickly lead to out-of-memory (OOM) errors. To mitigate this, we support both training and inference in FP16. Follow the steps below to enable FP16:\n",
    "\n",
    "1. load the model in fp16\n",
    "\n",
    "```python\n",
    "\n",
    "# 1. Create agent\n",
    "model = TransformersModel(\n",
    "    model_id=\"Qwen/Qwen2.5-0.5B-Instruct\",  # use a bigger model for better results\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "```\n",
    "\n",
    "2. enable fp16 in Brain, use with a simple boolean flag in the `Brain` constructor.\n",
    "\n",
    "```python\n",
    "brain = Brain(\n",
    "    agent,\n",
    "    algorithm=\"GRPO\",\n",
    "    fp16=True, # FP16 training\n",
    "    ...\n",
    ")\n",
    "```\n",
    "The complete example on how to use fp16 finetuning can be found [here](https://github.com/ToolBrain/ToolBrain/blob/main/examples/13_hello_world_fp16.py)\n",
    "## 4. Unsloth: Faster Training Speed (GPU only)\n",
    "\n",
    "**What it is:** Unsloth is a powerful optimization library that rewrites the model's underlying code (e.g., the attention mechanism) to be significantly faster and more memory-efficient, especially when training with LoRA.\n",
    "\n",
    "**Why it's useful:**\n",
    "-   **Up to 2x Faster Training:** Unsloth can double your training speed.\n",
    "-   **Reduced Memory Usage:** It further reduces memory consumption, allowing you to use larger batch sizes.\n",
    "\n",
    "**How to use it in ToolBrain:**\n",
    "\n",
    "Unsloth is enabled when you first create your agent or model, as it needs to patch the model upon loading.\n",
    "\n",
    "```python\n",
    "from toolbrain import create_agent\n",
    "\n",
    "# Enable Unsloth when creating the agent\n",
    "agent = create_agent(\n",
    "    model_id=\"Qwen/Qwen2.5-14B-Instruct\",\n",
    "    tools=[...],\n",
    "    use_unsloth=True # Enable Unsloth here\n",
    ")\n",
    "```\n",
    "\n",
    "## Putting It All Together: The Ultimate Efficiency Setup\n",
    "\n",
    "The true power of these techniques comes from combining them. Here is how you would configure a `Brain` for maximum efficiency, using the email search agent from a previous tutorial as a template.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from peft import LoraConfig\n",
    "from toolbrain import Brain, create_agent\n",
    "from examples.07_email_search_agent import email_tools, custom_rewards\n",
    "\n",
    "# 1. Define LoRA Config\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], ...\n",
    ")\n",
    "\n",
    "# 2. Create agent with Unsloth enabled\n",
    "print(\"Initializing agent with Unsloth...\")\n",
    "agent = create_agent(\n",
    "    model_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    tools=[email_tools.search_emails, email_tools.read_email],\n",
    "    use_unsloth=True # Unsloth enabled\n",
    ")\n",
    "\n",
    "# 3. Configure Brain with BitsAndBytes and pass LoRA config\n",
    "print(\"Configuring Brain with BitsAndBytes and LoRA...\")\n",
    "brain = Brain(\n",
    "    agent,\n",
    "    algorithm=\"GRPO\",\n",
    "    learning_rate=1e-5,\n",
    "    use_bitsandbytes=True, # BitsAndBytes enabled\n",
    "    config={\"lora_config\": LORA_CONFIG} # Pass LoRA config\n",
    ")\n",
    "\n",
    "print(\"âœ… Efficient training setup complete!\")\n",
    "\n",
    "# Now, when you call brain.train(), it will run with all optimizations.\n",
    "# brain.train(training_data, num_iterations=4)\n",
    "```\n",
    "\n",
    "By using LoRA, BitsAndBytes, and Unsloth together, you can fine-tune large, powerful models on consumer-grade GPUs in a fraction of the time it would otherwise take, making advanced agent training accessible to more developers and researchers than ever before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450950b-0cae-4ccc-ad82-c48a810dd573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d95e43-94b6-4395-ac07-06caf900c764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
