#!/usr/bin/env python3
"""
çº¯å‡€æ¨¡å¼æ‰§è¡Œè¯„ä¼°è„šæœ¬ - ä¿®æ”¹æ–‡ä»¶ -> è¿è¡Œæµ‹è¯• -> è¿˜åŸæ–‡ä»¶
"""

import json
import sys
import os
import re
import argparse
import subprocess
import shutil
import tempfile
from typing import List, Dict, Any, Tuple
import numpy as np
import itertools
from datetime import datetime


class PureCodeReplacer:
    """çº¯å‡€ä»£ç æ›¿æ¢å¼•æ“ - ç›´æ¥ä¿®æ”¹æ–‡ä»¶"""
    
    def parse_location(self, location: str) -> Tuple[int, int]:
        """è§£ælocationå­—ç¬¦ä¸²ï¼Œè¿”å›èµ·å§‹è¡Œå’Œç»“æŸè¡Œ"""
        pattern = r".*:line\s+(\d+)-(\d+)"
        match = re.search(pattern, location)
        if not match:
            raise ValueError(f"Invalid location format: {location}")
        return int(match.group(1)), int(match.group(2))
    
    def extract_code_from_markdown(self, completion: str) -> str:
        """ä» markdown æ ¼å¼ä¸­æå–çº¯ä»£ç """
        if "```python" in completion:
            start = completion.find("```python") + len("```python")
            end = completion.find("```", start)
            if end == -1:
                end = len(completion)
            code_block = completion[start:end].strip()
        elif "```" in completion:
            start = completion.find("```") + len("```")
            end = completion.find("```", start)
            if end == -1:
                end = len(completion)
            code_block = completion[start:end].strip()
        else:
            code_block = completion.strip()
        
        return code_block
    
    def normalize_indentation(self, code: str) -> str:
        """å»é™¤ä»£ç çš„åŸºç¡€ç¼©è¿›"""
        lines = code.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]
        if not non_empty_lines:
            return code
        
        min_indent = min(len(line) - len(line.lstrip()) for line in non_empty_lines)
        
        normalized_lines = []
        for line in lines:
            if line.strip():
                normalized_lines.append(line[min_indent:])
            else:
                normalized_lines.append('')
        
        return '\n'.join(normalized_lines)
    
    def apply_indentation(self, code: str, base_indent: int) -> str:
        """ç»™ä»£ç æ·»åŠ æŒ‡å®šçš„åŸºç¡€ç¼©è¿›"""
        lines = code.split('\n')
        indented_lines = []
        for line in lines:
            if line.strip():
                indented_lines.append(' ' * base_indent + line)
            else:
                indented_lines.append('')
        return '\n'.join(indented_lines)
    
    def replace_function_in_file(self, file_path: str, location: str, completion: str) -> bool:
        """åœ¨æ–‡ä»¶ä¸­æ›¿æ¢å‡½æ•°å®ç°"""
        try:
            # è¯»å–åŸæ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                source_code = f.read()
            
            start_line, end_line = self.parse_location(location)
            lines = source_code.split('\n')
            
            # æå–ä»£ç 
            extracted_code = self.extract_code_from_markdown(completion)
            normalized_code = self.normalize_indentation(extracted_code)
            
            # è·å–åŸæ–‡ä»¶çš„åŸºç¡€ç¼©è¿›
            original_first_line = lines[start_line - 1]
            base_indent = len(original_first_line) - len(original_first_line.lstrip())
            
            # æ·»åŠ ç¼©è¿›
            indented_code = self.apply_indentation(normalized_code, base_indent)
            
            # æ›¿æ¢ä»£ç 
            indented_lines = indented_code.split('\n')
            lines[start_line-1:end_line] = indented_lines
            
            # å†™å›æ–‡ä»¶
            modified_code = '\n'.join(lines)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(modified_code)
            
            return True
            
        except Exception as e:
            print(f"Error replacing function: {e}")
            import traceback
            traceback.print_exc()
            return False


class PureTestExecutor:
    """çº¯å‡€æµ‹è¯•æ‰§è¡Œå™¨ - æ”¯æŒ unittest å’Œ pytest"""
    
    def __init__(self, source_dir: str, log_dir: str = None):
        self.source_dir = source_dir
        self.log_dir = log_dir
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        if self.log_dir and not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir, exist_ok=True)
    
    def _detect_test_framework(self, test_file_path: str) -> str:
        """æ£€æµ‹æµ‹è¯•æ–‡ä»¶ä½¿ç”¨çš„æµ‹è¯•æ¡†æ¶
        
        Args:
            test_file_path: æµ‹è¯•æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
            
        Returns:
            'pytest' æˆ– 'unittest'
        """
        try:
            with open(test_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æ£€æŸ¥æ˜¯å¦å¯¼å…¥äº† pytest
            if 'import pytest' in content or 'from pytest' in content:
                return 'pytest'
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† pytest çš„è£…é¥°å™¨
            if '@pytest.' in content:
                return 'pytest'
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† pytest çš„ fixture
            if 'def test_' in content and ('@pytest.fixture' in content or 'pytest.raises' in content):
                return 'pytest'
            
            # é»˜è®¤ä½¿ç”¨ unittest
            return 'unittest'
            
        except Exception as e:
            print(f"    Warning: Failed to detect test framework, defaulting to unittest: {e}")
            return 'unittest'
    
    def run_test_file(self, test_file_path: str, function_name: str = "") -> Tuple[bool, float]:
        """è¿è¡Œæµ‹è¯•æ–‡ä»¶ - è‡ªåŠ¨æ£€æµ‹ unittest æˆ– pytest
        
        Args:
            test_file_path: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            function_name: å‡½æ•°åï¼ˆç”¨äºç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼‰
        
        Returns:
            Tuple[bool, float]: (æ˜¯å¦å…¨éƒ¨é€šè¿‡, é€šè¿‡æ¯”ä¾‹)
        """
        try:
            full_test_path = os.path.join(self.source_dir, test_file_path)
            if not os.path.exists(full_test_path):
                print(f"Test file not found: {full_test_path}")
                return False, 0.0
            
            # æ£€æµ‹æµ‹è¯•æ¡†æ¶
            framework = self._detect_test_framework(full_test_path)
            print(f"    ğŸ” æ£€æµ‹åˆ°æµ‹è¯•æ¡†æ¶: {framework}")
            
            # æ ¹æ®æ¡†æ¶é€‰æ‹©è¿è¡Œå‘½ä»¤
            if framework == 'pytest':
                # ä½¿ç”¨ pytest è¿è¡Œï¼Œ-v æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼Œ-s æ˜¾ç¤ºè¾“å‡º
                cmd = [sys.executable, '-m', 'pytest', full_test_path, '-v', '--tb=short']
            else:
                # ä½¿ç”¨ unittest æ–¹å¼è¿è¡Œï¼ˆç›´æ¥æ‰§è¡Œæ–‡ä»¶ï¼‰
                cmd = [sys.executable, full_test_path]
            
            # è¿è¡Œæµ‹è¯•
            result = subprocess.run(
                cmd,
                cwd=self.source_dir,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            # ä¿å­˜æ—¥å¿—æ–‡ä»¶
            if self.log_dir:
                self._save_test_log(result, test_file_path, function_name, framework)
            
            # æ£€æŸ¥è¿”å›ç ï¼ˆæ³¨æ„ï¼šå³ä½¿æ‰€æœ‰æµ‹è¯•éƒ½è·³è¿‡ï¼Œè¿”å›ç ä¹Ÿæ˜¯0ï¼‰
            returncode_ok = result.returncode == 0
            
            # è§£ææµ‹è¯•ç»“æœï¼Œä¼ å…¥æ¡†æ¶ç±»å‹ä»¥ä¾¿æ­£ç¡®è§£æ
            pass_ratio = self._parse_test_output(result.stdout, result.stderr, returncode_ok, framework)
            
            # åˆ¤æ–­æ˜¯å¦çœŸæ­£å…¨éƒ¨é€šè¿‡ï¼šé€šè¿‡ç‡å¿…é¡»å¤§äº0ï¼ˆæ’é™¤å…¨éƒ¨è·³è¿‡çš„æƒ…å†µï¼‰
            all_passed = returncode_ok and pass_ratio > 0.0
            
            return all_passed, pass_ratio
            
        except subprocess.TimeoutExpired as e:
            print(f"Test timeout")
            # ä¿å­˜è¶…æ—¶æ—¥å¿—
            if self.log_dir:
                self._save_timeout_log(test_file_path, function_name)
            return False, 0.0
        except Exception as e:
            print(f"Error running test: {e}")
            import traceback
            traceback.print_exc()
            return False, 0.0
    
    def _save_test_log(self, result: subprocess.CompletedProcess, test_file_path: str, function_name: str, framework: str = "unknown"):
        """ä¿å­˜æµ‹è¯•è¿è¡Œæ—¥å¿—
        
        Args:
            result: subprocess è¿è¡Œç»“æœ
            test_file_path: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            function_name: å‡½æ•°å
            framework: æµ‹è¯•æ¡†æ¶ç±»å‹ (pytest/unittest)
        """
        try:
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ç²¾ç¡®åˆ°æ¯«ç§’
            
            # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼šå‡½æ•°å_æ—¶é—´æˆ³.log
            if function_name:
                # æ¸…ç†å‡½æ•°åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                safe_function_name = re.sub(r'[^\w\-_.]', '_', function_name)
                log_filename = f"{safe_function_name}_{timestamp}.log"
            else:
                test_basename = os.path.basename(test_file_path).replace('.py', '')
                log_filename = f"{test_basename}_{timestamp}.log"
            
            log_filepath = os.path.join(self.log_dir, log_filename)
            
            # å†™å…¥æ—¥å¿—å†…å®¹
            with open(log_filepath, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å‡½æ•°å: {function_name}\n")
                f.write(f"æµ‹è¯•æ–‡ä»¶: {test_file_path}\n")
                f.write(f"æµ‹è¯•æ¡†æ¶: {framework}\n")
                f.write(f"è¿”å›ç : {result.returncode}\n")
                f.write("=" * 80 + "\n\n")
                
                f.write("ã€STDOUTã€‘\n")
                f.write("-" * 80 + "\n")
                f.write(result.stdout if result.stdout else "(æ— è¾“å‡º)\n")
                f.write("\n")
                
                f.write("ã€STDERRã€‘\n")
                f.write("-" * 80 + "\n")
                f.write(result.stderr if result.stderr else "(æ— é”™è¯¯)\n")
                f.write("\n")
                
                f.write("=" * 80 + "\n")
                f.write("æ—¥å¿—ç»“æŸ\n")
            
            print(f"    ğŸ“ æ—¥å¿—å·²ä¿å­˜: {log_filename}")
            
        except Exception as e:
            print(f"    âš ï¸  ä¿å­˜æ—¥å¿—å¤±è´¥: {e}")
    
    def _save_timeout_log(self, test_file_path: str, function_name: str):
        """ä¿å­˜è¶…æ—¶æ—¥å¿—"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            safe_function_name = re.sub(r'[^\w\-_.]', '_', function_name) if function_name else 'unknown'
            log_filename = f"{safe_function_name}_{timestamp}_TIMEOUT.log"
            log_filepath = os.path.join(self.log_dir, log_filename)
            
            with open(log_filepath, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å‡½æ•°å: {function_name}\n")
                f.write(f"æµ‹è¯•æ–‡ä»¶: {test_file_path}\n")
                f.write(f"çŠ¶æ€: è¶…æ—¶ (TIMEOUT)\n")
                f.write("=" * 80 + "\n")
            
            print(f"    ğŸ“ è¶…æ—¶æ—¥å¿—å·²ä¿å­˜: {log_filename}")
        except Exception as e:
            print(f"    âš ï¸  ä¿å­˜è¶…æ—¶æ—¥å¿—å¤±è´¥: {e}")
    
    def _parse_test_output(self, stdout: str, stderr: str, all_passed: bool, framework: str = "unittest") -> float:
        """è§£ææµ‹è¯•è¾“å‡ºï¼Œæå–é€šè¿‡ç‡
        
        æ”¯æŒå¤šç§æµ‹è¯•æ¡†æ¶çš„è¾“å‡ºæ ¼å¼ï¼š
        - unittest: "Ran X tests", "FAILED (failures=Y)"
        - pytest: "X passed", "Y failed", "X passed in Xs"
        - å­—ç¬¦ç»Ÿè®¡: ä¼˜å…ˆç»Ÿè®¡ E/F/. å­—ç¬¦ï¼ˆæœ€å‡†ç¡®ï¼‰
        
        æ³¨æ„ï¼šè·³è¿‡çš„æµ‹è¯•ï¼ˆskippedï¼‰ä¸è®¡å…¥é€šè¿‡
        
        Args:
            stdout: æ ‡å‡†è¾“å‡º
            stderr: æ ‡å‡†é”™è¯¯è¾“å‡º
            all_passed: è¿”å›ç æ˜¯å¦ä¸º0
            framework: æµ‹è¯•æ¡†æ¶ç±»å‹ (pytest/unittest)
        
        Returns:
            float: é€šè¿‡çš„æµ‹è¯•ç”¨ä¾‹æ¯”ä¾‹ (0.0-1.0)
        """
        combined_output = stdout + stderr
        
        # å¦‚æœæ˜¯ pytestï¼Œä¼˜å…ˆä½¿ç”¨ pytest ç‰¹å®šçš„è§£ææ–¹æ³•
        if framework == 'pytest':
            ratio = self._parse_pytest_output(combined_output, all_passed)
            if ratio is not None:
                return ratio
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æµ‹è¯•éƒ½è¢«è·³è¿‡
        # ä¾‹å¦‚: "Ran 11 tests in 0.000s\n\nOK (skipped=11)"
        import re
        ran_match = re.search(r'Ran (\d+) test', combined_output)
        skipped_match = re.search(r'skipped=(\d+)', combined_output)
        
        if ran_match and skipped_match:
            total_tests = int(ran_match.group(1))
            skipped_tests = int(skipped_match.group(1))
            
            # å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½è¢«è·³è¿‡ï¼Œè¿”å›0.0
            if total_tests == skipped_tests:
                return 0.0
        
        # å¦‚æœå…¨éƒ¨é€šè¿‡ï¼ˆä¸”ä¸æ˜¯å…¨éƒ¨è·³è¿‡ï¼‰ï¼Œè¿”å›1.0
        if all_passed:
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰è·³è¿‡çš„æµ‹è¯•
            if skipped_match and ran_match:
                total_tests = int(ran_match.group(1))
                skipped_tests = int(skipped_match.group(1))
                # å¦‚æœæœ‰éƒ¨åˆ†è·³è¿‡ï¼Œè®¡ç®—å®é™…é€šè¿‡ç‡
                if skipped_tests > 0 and total_tests > skipped_tests:
                    # æœ‰éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œéƒ¨åˆ†è·³è¿‡
                    passed_tests = total_tests - skipped_tests
                    return passed_tests / total_tests
            return 1.0
        
        # æ–¹æ³•1ï¼šä¼˜å…ˆä½¿ç”¨å­—ç¬¦ç»Ÿè®¡ï¼ˆunittest å®æ—¶è¾“å‡ºçš„ E/F/.ï¼‰
        # è¿™æ˜¯æœ€å‡†ç¡®çš„æ–¹æ³•ï¼Œå› ä¸ºæ¯ä¸ªæµ‹è¯•éƒ½ä¼šè¾“å‡ºä¸€ä¸ªå­—ç¬¦
        error_count = combined_output.count('E')
        failure_count = combined_output.count('F')
        passed_count = combined_output.count('.')
        
        # å¦‚æœæ‰¾åˆ°äº†å­—ç¬¦ç»Ÿè®¡æ ‡è®°ï¼Œä¼˜å…ˆä½¿ç”¨è¿™ä¸ª
        if error_count > 0 or failure_count > 0 or passed_count > 0:
            total_from_chars = error_count + failure_count + passed_count
            if total_from_chars > 0:
                # éªŒè¯ï¼šæ£€æŸ¥å­—ç¬¦ç»Ÿè®¡æ˜¯å¦åˆç†ï¼ˆé¿å…åŒ¹é…åˆ°ä»£ç ä¸­çš„å­—ç¬¦ï¼‰
                # é€šå¸¸æµ‹è¯•è¾“å‡ºçš„ E/F/. ä¼šè¿ç»­å‡ºç°
                char_pattern = re.search(r'[EF.]{3,}', combined_output)
                if char_pattern:
                    # åªç»Ÿè®¡è¿ç»­å‡ºç°çš„æµ‹è¯•æ ‡è®°å­—ç¬¦
                    test_markers = char_pattern.group(0)
                    error_count = test_markers.count('E')
                    failure_count = test_markers.count('F')
                    passed_count = test_markers.count('.')
                    total_from_chars = len(test_markers)
                    
                    pass_ratio = passed_count / total_from_chars
                    return max(0.0, min(1.0, pass_ratio))
        
        # æ–¹æ³•2ï¼šè§£æ unittest æ–‡æœ¬æ ¼å¼
        # ä¾‹å¦‚: "Ran 5 tests in 0.001s" å’Œ "FAILED (failures=2, errors=1, skipped=1)"
        ran_match = re.search(r'Ran (\d+) test', combined_output)
        if ran_match:
            total_tests = int(ran_match.group(1))
            
            # åŒ¹é…å¤±è´¥æ•°é‡ã€é”™è¯¯æ•°é‡ã€è·³è¿‡æ•°é‡
            failures = 0
            errors = 0
            skipped = 0
            
            failure_match = re.search(r'failures=(\d+)', combined_output)
            if failure_match:
                failures = int(failure_match.group(1))
            
            error_match = re.search(r'errors=(\d+)', combined_output)
            if error_match:
                errors = int(error_match.group(1))
            
            skipped_match = re.search(r'skipped=(\d+)', combined_output)
            if skipped_match:
                skipped = int(skipped_match.group(1))
            
            if total_tests > 0:
                # é€šè¿‡çš„æµ‹è¯• = æ€»æ•° - å¤±è´¥ - é”™è¯¯ - è·³è¿‡
                # è·³è¿‡çš„æµ‹è¯•ä¸åº”è¯¥ç®—ä½œé€šè¿‡
                passed_tests = total_tests - failures - errors - skipped
                
                # ç¡®ä¿é€šè¿‡ç‡åœ¨ [0.0, 1.0] èŒƒå›´å†…
                passed_tests = max(0, min(passed_tests, total_tests))
                return passed_tests / total_tests
        
        # å°è¯•è§£æ pytest æ ¼å¼
        # ä¾‹å¦‚: "3 passed, 2 failed in 0.12s"
        pytest_match = re.search(r'(\d+) passed(?:, (\d+) failed)?', combined_output)
        if pytest_match:
            passed = int(pytest_match.group(1))
            failed = int(pytest_match.group(2)) if pytest_match.group(2) else 0
            total = passed + failed
            if total > 0:
                # ç¡®ä¿é€šè¿‡ç‡åœ¨ [0.0, 1.0] èŒƒå›´å†…
                pass_ratio = passed / total
                return max(0.0, min(1.0, pass_ratio))
        
        # å¦‚æœæ— æ³•è§£æä½†æµ‹è¯•å¤±è´¥äº†ï¼Œè¿”å›0.0
        return 0.0
    
    def _parse_pytest_output(self, output: str, all_passed: bool) -> float:
        """è§£æ pytest çš„è¾“å‡ºæ ¼å¼
        
        Pytest è¾“å‡ºç¤ºä¾‹:
        - "3 passed in 0.12s"
        - "1 failed, 2 passed in 0.12s"
        - "3 passed, 1 skipped in 0.12s"
        - "collected 3 items" ... "test_file.py::test_name PASSED"
        
        Args:
            output: pytest çš„è¾“å‡º
            all_passed: è¿”å›ç æ˜¯å¦ä¸º0
            
        Returns:
            float or None: é€šè¿‡ç‡ï¼Œå¦‚æœæ— æ³•è§£æè¿”å› None
        """
        import re
        
        # æ–¹æ³•1: è§£ææœ€åçš„æ±‡æ€»è¡Œ "X passed, Y failed in Zs"
        # åŒ¹é… "5 passed in 0.12s" æˆ– "2 failed, 3 passed in 0.12s"
        summary_pattern = r'(?:(\d+) failed)?(?:, )?(?:(\d+) passed)?(?:, )?(?:(\d+) skipped)?(?:, )?(?:(\d+) error)?.*in ([\d.]+)s'
        summary_match = re.search(summary_pattern, output)
        
        if summary_match:
            failed = int(summary_match.group(1)) if summary_match.group(1) else 0
            passed = int(summary_match.group(2)) if summary_match.group(2) else 0
            skipped = int(summary_match.group(3)) if summary_match.group(3) else 0
            errors = int(summary_match.group(4)) if summary_match.group(4) else 0
            
            # è®¡ç®—æ€»æµ‹è¯•æ•°ï¼ˆä¸åŒ…æ‹¬è·³è¿‡çš„ï¼‰
            total = passed + failed + errors
            
            # å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½è¢«è·³è¿‡
            if total == 0 and skipped > 0:
                return 0.0
            
            if total > 0:
                return passed / total
        
        # æ–¹æ³•2: ç»Ÿè®¡æµ‹è¯•ç»“æœæ ‡è®° PASSED / FAILED / ERROR / SKIPPED
        passed_count = len(re.findall(r'\bPASSED\b', output))
        failed_count = len(re.findall(r'\bFAILED\b', output))
        error_count = len(re.findall(r'\bERROR\b', output))
        skipped_count = len(re.findall(r'\bSKIPPED\b', output))
        
        total_from_markers = passed_count + failed_count + error_count
        
        if total_from_markers > 0:
            return passed_count / total_from_markers
        
        # æ–¹æ³•3: æ£€æŸ¥ collected è¡Œ
        # ä¾‹å¦‚: "collected 3 items"
        collected_match = re.search(r'collected (\d+) items?', output)
        if collected_match:
            total_tests = int(collected_match.group(1))
            
            # å¦‚æœ returncode ä¸º 0 ä¸”æ²¡æœ‰æ‰¾åˆ°å¤±è´¥ä¿¡æ¯ï¼Œè¯´æ˜å…¨éƒ¨é€šè¿‡
            if all_passed and total_tests > 0:
                # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨è¢«è·³è¿‡
                if skipped_count == total_tests:
                    return 0.0
                return 1.0
        
        # å¦‚æœè¿”å›ç ä¸º0ä¸”æ‰¾åˆ°äº† passedï¼Œè®¤ä¸ºå…¨éƒ¨é€šè¿‡
        if all_passed and 'passed' in output.lower():
            return 1.0
        
        # æ— æ³•è§£æ
        return None


def estimate_pass_at_k(num_samples, num_correct, k: int) -> np.ndarray:
    """Estimates pass@k"""
    def estimator(n: int, c: int, k: int) -> float:
        if n - c < k:
            return 1.0
        return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))

    if isinstance(num_samples, int):
        num_samples_it = itertools.repeat(num_samples, len(num_correct))
    else:
        assert len(num_samples) == len(num_correct)
        num_samples_it = iter(num_samples)

    return np.array([estimator(int(n), int(c), k) for n, c in zip(num_samples_it, num_correct)])


class ResultCollector:
    """ç»“æœæ”¶é›†å™¨"""
    
    def load_jsonl_data(self, file_path: str) -> List[Dict[str, Any]]:
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
        return data
    
    def save_jsonl_data(self, data: List[Dict[str, Any]], file_path: str):
        with open(file_path, 'w', encoding='utf-8') as f:
            for record in data:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
    
    def calculate_pass_at_k(self, data_records: List[Dict[str, Any]], k_values: List[int] = [1, 2, 3, 4]) -> Dict[str, float]:
        total_samples = []
        correct_samples = []
        
        for record in data_records:
            if 'results' in record and record['results']:
                num_samples = len(record['results'])
                num_correct = sum(record['results'])
                total_samples.append(num_samples)
                correct_samples.append(num_correct)
        
        if not total_samples:
            return {}
        
        total_samples = np.array(total_samples)
        correct_samples = np.array(correct_samples)
        
        pass_at_k = {}
        for k in k_values:
            if (total_samples >= k).all():
                pass_k_scores = estimate_pass_at_k(total_samples, correct_samples, k)
                pass_at_k[f"pass@{k}"] = pass_k_scores.mean()
        
        return pass_at_k
    
    def calculate_avg_pass_ratio(self, data_records: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å¹³å‡é€šè¿‡ç‡ - æ›´ç»†ç²’åº¦çš„æŒ‡æ ‡
        
        å¯¹äºæ¯ä¸ªä»»åŠ¡(task_id/function_name)ï¼Œè®¡ç®—å…¶æ‰€æœ‰æ ·æœ¬çš„é€šè¿‡ç‡çš„å¹³å‡å€¼
        ç„¶åå¯¹æ‰€æœ‰ä»»åŠ¡çš„å¹³å‡é€šè¿‡ç‡å†æ±‚å¹³å‡
        
        Args:
            data_records: åŒ…å«æµ‹è¯•ç»“æœçš„è®°å½•åˆ—è¡¨
            
        Returns:
            å¹³å‡é€šè¿‡ç‡ (0.0-1.0)
        """
        group = {}
        for record in data_records:
            # ä½¿ç”¨ function_name ä½œä¸º task_id
            task_id = record.get('function_name', '')
            if not task_id:
                continue
            
            if task_id not in group:
                group[task_id] = []
                
            # ä¼˜å…ˆä½¿ç”¨ pass_ratios å­—æ®µï¼ˆæ–°ç‰ˆæœ¬çš„ç»†ç²’åº¦æ•°æ®ï¼‰
            if 'pass_ratios' in record and record['pass_ratios']:
                group[task_id].extend(record['pass_ratios'])
            # å…¶æ¬¡ä½¿ç”¨ passed å­—æ®µï¼ˆå…¼å®¹æ—§æ•°æ®æ ¼å¼ï¼‰
            elif 'passed' in record:
                group[task_id].append(record['passed'])
            # æœ€åä½¿ç”¨ results å­—æ®µï¼ˆå¸ƒå°”å€¼åˆ—è¡¨ï¼‰ï¼Œè®¡ç®—é€šè¿‡ç‡
            elif 'results' in record and record['results']:
                # è®¡ç®—è¿™ä¸ª completion çš„é€šè¿‡ç‡
                pass_ratio = sum(record['results']) / len(record['results']) if len(record['results']) > 0 else 0.0
                group[task_id].append(pass_ratio)
        
        if not group:
            return 0.0
        
        # å¯¹æ¯ä¸ª task_id è®¡ç®—å¹³å‡é€šè¿‡ç‡
        task_avg_pass_ratios = []
        for task_id, pass_ratios in group.items():
            task_avg = np.mean(pass_ratios)
            task_avg_pass_ratios.append(task_avg)
        
        # è¿”å›æ‰€æœ‰ä»»åŠ¡çš„å¹³å‡é€šè¿‡ç‡
        return np.mean(task_avg_pass_ratios)


def main():
    parser = argparse.ArgumentParser(description='Pure mode execution evaluation - modify file, run test, restore')
    parser.add_argument('--source_dir', required=True, help='Source code directory path')
    parser.add_argument('--input_file', required=True, help='Input file path')
    parser.add_argument('--output_file', required=True, help='Output file path')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ”¬ PURE MODE - Modify file, run test, restore")
    print("=" * 60)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆä¸è¾“å‡ºæ–‡ä»¶åŒç›®å½•ï¼‰
    output_dir = os.path.dirname(args.output_file)
    log_dir = os.path.join(output_dir, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    print(f"ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")
    print()
    
    code_replacer = PureCodeReplacer()
    test_executor = PureTestExecutor(args.source_dir, log_dir=log_dir)
    result_collector = ResultCollector()
    
    print(f"Loading data from {args.input_file}...")
    data_records = result_collector.load_jsonl_data(args.input_file)
    print(f"Processing {len(data_records)} records...")
    
    for i, record in enumerate(data_records):
        print(f"Processing record {i+1}/{len(data_records)}: {record['function_name']}")
        
        # Validate that required fields are present
        location = record.get('implementation_location', '')
        test_code_path = record.get('test_code_path', '')
        
        if not location or not location.strip():
            print(f"  âŒ Error: 'implementation_location' field is empty or missing")
            print(f"  This field is required to locate the source file to modify")
            record['results'] = [False] * len(record.get('completions', []))
            record['pass_ratios'] = [0.0] * len(record.get('completions', []))
            continue
        
        if not test_code_path or not test_code_path.strip():
            print(f"  âŒ Error: 'test_code_path' field is empty or missing")
            print(f"  This field is required to locate the test file to run")
            record['results'] = [False] * len(record.get('completions', []))
            record['pass_ratios'] = [0.0] * len(record.get('completions', []))
            continue
        
        source_file = location.split(':')[0]
        
        if source_file.startswith('code/'):
            source_file = source_file[5:]
        
        source_file_path = os.path.join(args.source_dir, source_file)
        
        # Check if source_file_path is a directory (indicates invalid path)
        if os.path.isdir(source_file_path):
            print(f"  âŒ Error: Source path is a directory, not a file: {source_file_path}")
            print(f"  Check that 'implementation_location' contains a valid file path")
            record['results'] = [False] * len(record.get('completions', []))
            record['pass_ratios'] = [0.0] * len(record.get('completions', []))
            continue
        
        if not os.path.exists(source_file_path):
            print(f"  âŒ Source file not found: {source_file_path}")
            record['results'] = [False] * len(record.get('completions', []))
            record['pass_ratios'] = [0.0] * len(record.get('completions', []))
            continue
        
        if test_code_path.startswith('code/'):
            test_code_path = test_code_path[5:]
        
        # å¤‡ä»½åŸæ–‡ä»¶
        backup_path = source_file_path + '.backup'
        shutil.copy2(source_file_path, backup_path)
        
        results = []
        pass_ratios = []  # æ–°å¢ï¼šè®°å½•æ¯ä¸ªcompletionçš„é€šè¿‡ç‡
        
        try:
            for j, completion in enumerate(record['completions']):
                print(f"  Testing completion {j+1}/{len(record['completions'])}")
                
                try:
                    # 1. ä¿®æ”¹æºæ–‡ä»¶
                    success = code_replacer.replace_function_in_file(
                        source_file_path,
                        record['implementation_location'],
                        completion
                    )
                    
                    if not success:
                        results.append(False)
                        pass_ratios.append(0.0)
                        print(f"    Result: FAIL (replace failed)")
                        # è¿˜åŸæ–‡ä»¶
                        shutil.copy2(backup_path, source_file_path)
                        continue
                    
                    # 2. è¿è¡Œæµ‹è¯•
                    test_passed, pass_ratio = test_executor.run_test_file(test_code_path, record['function_name'])
                    
                    results.append(test_passed)
                    pass_ratios.append(pass_ratio)
                    print(f"    Result: {'PASS' if test_passed else 'FAIL'} (é€šè¿‡ç‡: {pass_ratio:.2%})")
                    
                    # 3. è¿˜åŸæ–‡ä»¶ï¼ˆä¸ºä¸‹ä¸€ä¸ª completion å‡†å¤‡ï¼‰
                    shutil.copy2(backup_path, source_file_path)
                    
                except Exception as e:
                    print(f"    Error: {e}")
                    results.append(False)
                    pass_ratios.append(0.0)
                    # ç¡®ä¿è¿˜åŸæ–‡ä»¶
                    shutil.copy2(backup_path, source_file_path)
        
        finally:
            # ç¡®ä¿æœ€åè¿˜åŸæ–‡ä»¶
            if os.path.exists(backup_path):
                shutil.copy2(backup_path, source_file_path)
                os.remove(backup_path)
        
        record['results'] = results
        record['pass_ratios'] = pass_ratios  # æ–°å¢ï¼šä¿å­˜é€šè¿‡ç‡ä¿¡æ¯
    
    print(f"Saving results to {args.output_file}...")
    result_collector.save_jsonl_data(data_records, args.output_file)
    
    # è®¡ç®—æŒ‡æ ‡
    print("\nCalculating pass@k metrics...")
    pass_at_k_results = result_collector.calculate_pass_at_k(data_records)
    
    print("Calculating AvgPassRatio metric...")
    avg_pass_ratio = result_collector.calculate_avg_pass_ratio(data_records)
    
    total_tests = sum(len(record['completions']) for record in data_records)
    total_passed = sum(sum(record['results']) for record in data_records)
    
    print(f"\nExecution completed!")
    print(f"Total tests: {total_tests}")
    print(f"Passed: {total_passed}")
    print(f"Failed: {total_tests - total_passed}")
    print(f"Success rate: {total_passed/total_tests*100:.1f}%")
    
    if pass_at_k_results:
        print("\nPass@k Results:")
        for metric, value in pass_at_k_results.items():
            print(f"{metric}: {value:.4f}")
    
    print(f"\nAvgPassRatio: {avg_pass_ratio:.4f}")
    
    if pass_at_k_results:
        metrics_file = args.output_file.replace('_result.jsonl', '_result.metrics.json')
        print(f"Saving metrics to {metrics_file}...")
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump({
                'total_functions': len(data_records),
                'total_tests': total_tests,
                'total_passed': total_passed,
                'overall_success_rate': total_passed/total_tests if total_tests > 0 else 0.0,
                'pass_at_k': pass_at_k_results,
                'avg_pass_ratio': avg_pass_ratio
            }, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    main()
