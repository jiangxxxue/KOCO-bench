#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
aggregate_metrics.py - èšåˆå¤šä¸ªæµ‹è¯•å®ä¾‹çš„è¯„ä¼°æŒ‡æ ‡

è®¡ç®—å¤šä¸ªæµ‹è¯•å®ä¾‹çš„ç»¼åˆ pass@1 å’Œ avg_pass_ratio
"""

import json
import argparse
from pathlib import Path
from typing import List, Dict, Any


def load_metrics(file_path: str) -> Dict[str, Any]:
    """åŠ è½½å•ä¸ª metrics æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def discover_test_examples(model_dir: str) -> List[str]:
    """
    è‡ªåŠ¨å‘ç°ç›®å½•ä¸‹çš„æ‰€æœ‰æµ‹è¯•å®ä¾‹
    
    é€šè¿‡æ‰«æ *result.metrics.json æ–‡ä»¶æ¥æå–æµ‹è¯•å®ä¾‹åç§°
    
    Args:
        model_dir: æ¨¡å‹è¾“å‡ºç›®å½•è·¯å¾„
    
    Returns:
        æµ‹è¯•å®ä¾‹åç§°åˆ—è¡¨
    """
    import re
    
    model_path = Path(model_dir)
    examples = []
    
    # æŸ¥æ‰¾æ‰€æœ‰ metrics æ–‡ä»¶
    pattern = "*result.metrics.json"
    for metrics_file in model_path.glob(pattern):
        filename = metrics_file.name
        
        # å°è¯•åŒ¹é… algorithm_methods_data_{example}_result.metrics.json æ ¼å¼
        match = re.match(r'algorithm_methods_data_(.+?)_result\.metrics\.json', filename)
        if match:
            example_name = match.group(1)
            examples.append(example_name)
            continue
        
        # å°è¯•åŒ¹é…å…¶ä»–æ ¼å¼: {prefix}_{example}_result.metrics.json
        # å»æ‰ _result.metrics.json åç¼€ï¼Œå–æœ€åä¸€ä¸ªä¸‹åˆ’çº¿åçš„éƒ¨åˆ†
        base_name = filename.replace('_result.metrics.json', '')
        if '_' in base_name:
            # å–æœ€åä¸€ä¸ªä¸‹åˆ’çº¿åçš„éƒ¨åˆ†ä½œä¸º example åç§°
            parts = base_name.rsplit('_', 1)
            if len(parts) == 2:
                example_name = parts[1]
                examples.append(example_name)
    
    # å»é‡å¹¶æ’åº
    examples = sorted(set(examples))
    
    return examples


def aggregate_metrics(
    model_dir: str,
    test_examples: List[str] = None,
    framework: str = None,
) -> Dict[str, Any]:
    """
    èšåˆå¤šä¸ªæµ‹è¯•å®ä¾‹çš„æŒ‡æ ‡
    
    Args:
        model_dir: æ¨¡å‹è¾“å‡ºç›®å½•è·¯å¾„
        test_examples: æµ‹è¯•å®ä¾‹åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨å‘ç°ç›®å½•ä¸‹æ‰€æœ‰å®ä¾‹ï¼‰
        framework: æ¡†æ¶åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºæ–‡ä»¶ååŒ¹é…ï¼‰
    
    Returns:
        èšåˆåçš„æŒ‡æ ‡å­—å…¸
    """
    model_path = Path(model_dir)
    
    # å¦‚æœæœªæŒ‡å®š test_examplesï¼Œåˆ™è‡ªåŠ¨å‘ç°
    if test_examples is None or len(test_examples) == 0:
        test_examples = discover_test_examples(model_dir)
        if not test_examples:
            raise ValueError(f"åœ¨ç›®å½• {model_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½• metrics æ–‡ä»¶")
        print(f"ğŸ“‹ è‡ªåŠ¨å‘ç° {len(test_examples)} ä¸ªæµ‹è¯•å®ä¾‹: {', '.join(test_examples)}")
    
    # æ”¶é›†æ‰€æœ‰å®ä¾‹çš„æŒ‡æ ‡
    all_metrics = []
    total_functions = 0
    total_tests = 0
    total_passed = 0
    weighted_pass_at_1 = 0.0  # ä¿®æ”¹ï¼šä½¿ç”¨åŠ æƒ pass@1
    weighted_avg_pass_ratio = 0.0
    
    missing_files = []
    
    for example in test_examples:
        # æ„å»º metrics æ–‡ä»¶è·¯å¾„
        if framework:
            metrics_file = model_path / f"algorithm_methods_data_{example}_result.metrics.json"
        else:
            # å°è¯•æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            pattern = f"*{example}*result.metrics.json"
            matches = list(model_path.glob(pattern))
            if matches:
                metrics_file = matches[0]
            else:
                metrics_file = model_path / f"algorithm_methods_data_{example}_result.metrics.json"
        
        # åŠ è½½æŒ‡æ ‡
        if not metrics_file.exists():
            print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° {example} çš„ metrics æ–‡ä»¶: {metrics_file}")
            missing_files.append(example)
            continue
        
        try:
            metrics = load_metrics(str(metrics_file))
            all_metrics.append({
                'example': example,
                'metrics': metrics
            })
            
            # ç´¯åŠ ç»Ÿè®¡
            num_funcs = metrics.get('total_functions', 0)
            total_functions += num_funcs
            total_tests += metrics.get('total_tests', 0)
            total_passed += metrics.get('total_passed', 0)
            
            # ä¿®æ”¹ï¼šåŠ æƒå¹³å‡ pass@1ï¼ˆæŒ‰å‡½æ•°æ•°é‡åŠ æƒï¼‰
            pass_at_1 = metrics.get('pass_at_k', {}).get('pass@1', 0.0)
            weighted_pass_at_1 += pass_at_1 * num_funcs
            
            # åŠ æƒå¹³å‡ avg_pass_ratioï¼ˆæŒ‰å‡½æ•°æ•°é‡åŠ æƒï¼‰
            avg_ratio = metrics.get('avg_pass_ratio', 0.0)
            weighted_avg_pass_ratio += avg_ratio * num_funcs
            
            print(f"âœ“ {example}: {num_funcs} å‡½æ•°, "
                  f"pass@1={pass_at_1:.4f}, "
                  f"avg_pass_ratio={avg_ratio:.4f}")
        
        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½ {example} çš„ metrics: {e}")
            missing_files.append(example)
    
    if not all_metrics:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ metrics æ–‡ä»¶")
    
    # è®¡ç®—ç»¼åˆæŒ‡æ ‡
    # ä¿®æ”¹ï¼špass@1 ä½¿ç”¨åŠ æƒå¹³å‡ï¼ˆè€Œä¸æ˜¯ç®€å•çš„ total_passed / total_functionsï¼‰
    aggregate_pass_at_1 = weighted_pass_at_1 / total_functions if total_functions > 0 else 0.0
    
    # avg_pass_ratio: åŠ æƒå¹³å‡
    aggregate_avg_pass_ratio = weighted_avg_pass_ratio / total_functions if total_functions > 0 else 0.0
    
    result = {
        'model_dir': str(model_path),
        'test_examples': test_examples,
        'valid_examples': [m['example'] for m in all_metrics],
        'missing_examples': missing_files,
        'aggregate_metrics': {
            'total_functions': total_functions,
            'total_tests': total_tests,
            'total_passed': total_passed,
            'pass@1': aggregate_pass_at_1,
            'avg_pass_ratio': aggregate_avg_pass_ratio,
        },
        'individual_metrics': all_metrics
    }
    
    return result


def print_summary(result: Dict[str, Any]):
    """æ‰“å°æ±‡æ€»ç»“æœ"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ç»¼åˆæŒ‡æ ‡æ±‡æ€»")
    print("=" * 70)
    print(f"æ¨¡å‹ç›®å½•: {result['model_dir']}")
    print(f"æµ‹è¯•å®ä¾‹: {', '.join(result['test_examples'])}")
    
    if result['missing_examples']:
        print(f"âš ï¸  ç¼ºå¤±å®ä¾‹: {', '.join(result['missing_examples'])}")
    
    print("\n" + "-" * 70)
    print("ç»¼åˆç»“æœ:")
    print("-" * 70)
    
    agg = result['aggregate_metrics']
    print(f"æ€»å‡½æ•°æ•°:     {agg['total_functions']}")
    print(f"æ€»æµ‹è¯•æ•°:     {agg['total_tests']}")
    print(f"é€šè¿‡å‡½æ•°æ•°:   {agg['total_passed']}")
    print(f"pass@1:       {agg['pass@1']:.4f} ({agg['pass@1']*100:.2f}%)")
    print(f"avg_pass_ratio: {agg['avg_pass_ratio']:.4f}")
    
    print("\n" + "-" * 70)
    print("å„å®ä¾‹è¯¦æƒ…:")
    print("-" * 70)
    for item in result['individual_metrics']:
        example = item['example']
        m = item['metrics']
        print(f"\n{example}:")
        print(f"  å‡½æ•°æ•°: {m.get('total_functions', 0)}")
        print(f"  é€šè¿‡æ•°: {m.get('total_passed', 0)}")
        print(f"  pass@1: {m.get('pass_at_k', {}).get('pass@1', 0.0):.4f}")
        print(f"  avg_pass_ratio: {m.get('avg_pass_ratio', 0.0):.4f}")
    
    print("\n" + "=" * 70)


def main():
    parser = argparse.ArgumentParser(
        description="èšåˆå¤šä¸ªæµ‹è¯•å®ä¾‹çš„è¯„ä¼°æŒ‡æ ‡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

  # è‡ªåŠ¨å‘ç°æ‰€æœ‰æµ‹è¯•å®ä¾‹ï¼ˆæ¨èï¼‰
  python aggregate_metrics.py \\
    --model_dir scripts/data/verl/qwen2.5-coder-32b-instruct-simple

  # æŒ‡å®šç‰¹å®šæµ‹è¯•å®ä¾‹
  python aggregate_metrics.py \\
    --model_dir scripts/data/verl/qwen2.5-coder-32b-instruct-simple \\
    --test_examples prime ARES LUFFY PURE

  # æŒ‡å®šæ¡†æ¶åç§°
  python aggregate_metrics.py \\
    --model_dir scripts/data/verl/qwen2.5-coder-7b-lora \\
    --test_examples prime ARES LUFFY PURE \\
    --framework verl

  # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
  python aggregate_metrics.py \\
    --model_dir scripts/data/verl/qwen2.5-coder-32b-instruct-simple \\
    --output aggregate_result.json

è¾“å‡ºè¯´æ˜:
  - pass@1: æ‰€æœ‰å®ä¾‹ä¸­é€šè¿‡çš„å‡½æ•°æ•° / æ€»å‡½æ•°æ•°
  - avg_pass_ratio: æ‰€æœ‰å®ä¾‹çš„ avg_pass_ratio æŒ‰å‡½æ•°æ•°åŠ æƒå¹³å‡
        """
    )
    
    parser.add_argument(
        '--model_dir',
        type=str,
        required=True,
        help='æ¨¡å‹è¾“å‡ºç›®å½•è·¯å¾„'
    )
    parser.add_argument(
        '--test_examples',
        type=str,
        nargs='+',
        default=None,
        help='æµ‹è¯•å®ä¾‹åç§°åˆ—è¡¨ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ã€‚ä¸æŒ‡å®šåˆ™è‡ªåŠ¨å‘ç°ç›®å½•ä¸‹æ‰€æœ‰å®ä¾‹'
    )
    parser.add_argument(
        '--framework',
        type=str,
        default=None,
        help='æ¡†æ¶åç§°ï¼ˆå¯é€‰ï¼‰'
    )
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰'
    )
    
    args = parser.parse_args()
    
    try:
        # èšåˆæŒ‡æ ‡
        result = aggregate_metrics(
            model_dir=args.model_dir,
            test_examples=args.test_examples,
            framework=args.framework,
        )
        
        # æ‰“å°æ±‡æ€»
        print_summary(result)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if args.output:
            output_path = Path(args.output)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return 0
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())

