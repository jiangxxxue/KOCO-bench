# LoRA æ¨ç†æœåŠ¡å™¨ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

LoRA æ¨ç†æœåŠ¡å™¨æ”¯æŒåŠ è½½**åŸºç¡€æ¨¡å‹ + LoRA adapter**ï¼Œæä¾›ä»£ç ç”ŸæˆæœåŠ¡ã€‚

### âš ï¸ é‡è¦è¯´æ˜

**LoRA æ¨¡å‹åŠ è½½æ–¹å¼ï¼š**
- LoRA è®­ç»ƒåç”Ÿæˆçš„æ˜¯ **adapter æƒé‡**ï¼Œä¸æ˜¯å®Œæ•´æ¨¡å‹
- æ¨ç†æ—¶éœ€è¦ï¼š**åŸºç¡€æ¨¡å‹** + **LoRA adapter** ä¸€èµ·åŠ è½½
- ä½¿ç”¨ PEFT åº“å®ç°åœ¨çº¿åŠ è½½

**ä¸¤ç§ä½¿ç”¨æ–¹æ¡ˆï¼š**
1. **æ–¹æ¡ˆ Aï¼ˆæ¨èï¼‰**: ä½¿ç”¨ LoRA æœåŠ¡å™¨ - åœ¨çº¿åŠ è½½ base model + adapter
2. **æ–¹æ¡ˆ B**: å…ˆåˆå¹¶æƒé‡ - ä½¿ç”¨ `merge_lora.py` åˆå¹¶åå½“æ™®é€šæ¨¡å‹ç”¨

---

## ğŸš€ æ–¹æ¡ˆ A: ä½¿ç”¨ LoRA æ¨ç†æœåŠ¡å™¨

### æ­¥éª¤ 1: å¯åŠ¨ LoRA æ¨ç†æœåŠ¡å™¨

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®å¯åŠ¨ï¼ˆéœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰
BASE_MODEL_PATH=/home/user/models/Qwen2.5-Coder-7B-Instruct \
LORA_ADAPTER_PATH=../models/qwen2.5-coder-7b-verl-lora \
bash scripts/lora/start_inference_server_lora.sh

# æˆ–è€…æŒ‡å®šç«¯å£
BASE_MODEL_PATH=/home/user/models/Qwen2.5-Coder-7B-Instruct \
LORA_ADAPTER_PATH=../models/qwen2.5-coder-7b-verl-lora \
SERVER_PORT=8001 \
bash scripts/lora/start_inference_server_lora.sh
```

**å¯åŠ¨å‚æ•°ï¼ˆç¯å¢ƒå˜é‡ï¼‰**ï¼š
- `BASE_MODEL_PATH`: **åŸºç¡€æ¨¡å‹è·¯å¾„**ï¼ˆå¿…é¡»ï¼Œä¾‹å¦‚ Qwen2.5-Coder-7B-Instructï¼‰
- `LORA_ADAPTER_PATH`: **LoRA adapter è·¯å¾„**ï¼ˆå¿…é¡»ï¼Œè®­ç»ƒè¾“å‡ºçš„ç›®å½•ï¼‰
- `SERVER_PORT`: æœåŠ¡å™¨ç«¯å£ï¼ˆé»˜è®¤: `8001`ï¼Œé¿å…å’Œ SFT æœåŠ¡å™¨å†²çªï¼‰
- `SERVER_HOST`: æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: `0.0.0.0`ï¼‰
- `MAX_CONTEXT_LEN`: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé»˜è®¤: `4096`ï¼‰
- `TORCH_DTYPE`: æ•°æ®ç±»å‹ï¼ˆé»˜è®¤: `bfloat16`ï¼‰
- `CUDA_VISIBLE_DEVICES`: GPU è®¾å¤‡ï¼ˆé»˜è®¤: `0`ï¼‰

**æ³¨æ„äº‹é¡¹**ï¼š
- é¦–æ¬¡å¯åŠ¨éœ€è¦åŠ è½½åŸºç¡€æ¨¡å‹å’Œ adapterï¼Œå¤§çº¦éœ€è¦ 2-3 åˆ†é’Ÿ
- ç¡®ä¿å®‰è£…äº† `peft` åº“: `pip install peft`
- æ—¥å¿—ä¿å­˜åœ¨ `../logs/inference_server_lora.log`
- PID ä¿å­˜åœ¨ `../logs/inference_server_lora.pid`

### æ­¥éª¤ 2: æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€

```bash
# æ–¹æ³• 1: ä½¿ç”¨ curl æ£€æŸ¥å¥åº·çŠ¶æ€
curl http://localhost:8001/health

# æ–¹æ³• 2: æŸ¥çœ‹æ—¥å¿—
tail -f ../logs/inference_server_lora.log

# æ–¹æ³• 3: æ£€æŸ¥è¿›ç¨‹
cat ../logs/inference_server_lora.pid
ps aux | grep inference_server_lora
```

### æ­¥éª¤ 3: æ‰¹é‡ç”Ÿæˆä»£ç 

```bash
# ä½¿ç”¨ LoRA æœåŠ¡å™¨æ‰¹é‡ç”Ÿæˆ
FRAMEWORK=verl \
MODEL_NAME=qwen2.5-coder-7b-verl-lora \
bash scripts/lora/run_batch_code_generation_with_lora_server.sh

# ç”Ÿæˆå¤šä¸ªè¡¥å…¨
FRAMEWORK=verl \
MODEL_NAME=qwen2.5-coder-7b-verl-lora \
NUM_COMPLETIONS=4 \
TEMPERATURE=0.8 \
bash scripts/lora/run_batch_code_generation_with_lora_server.sh
```

### æ­¥éª¤ 4: åœæ­¢æœåŠ¡å™¨

```bash
# æ­£å¸¸åœæ­¢æœåŠ¡å™¨
bash scripts/lora/stop_inference_server_lora.sh

# æˆ–è€…ç›´æ¥ kill è¿›ç¨‹
kill $(cat ../logs/inference_server_lora.pid)
```

---

## ğŸ”€ æ–¹æ¡ˆ B: åˆå¹¶æƒé‡åä½¿ç”¨

å¦‚æœä¸æƒ³ä½¿ç”¨æœåŠ¡å™¨æ¶æ„ï¼Œå¯ä»¥å…ˆåˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹ï¼š

```bash
# åˆå¹¶ LoRA adapter åˆ°åŸºç¡€æ¨¡å‹
python merge_lora.py \
    --base_model /path/to/base/model \
    --lora_adapter ../models/qwen2.5-coder-7b-verl-lora \
    --output_dir ../models/qwen2.5-coder-7b-verl-merged

# ç„¶ååƒæ™®é€šæ¨¡å‹ä¸€æ ·ä½¿ç”¨
# å¯ä»¥ç”¨ inference ç›®å½•ä¸‹çš„æœåŠ¡å™¨æˆ– apicall æ–¹å¼
```

---

## ğŸ“– è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: å®Œæ•´çš„ LoRA è®­ç»ƒå’Œæ¨ç†æµç¨‹

```bash
# 1. è®­ç»ƒ LoRA adapter
bash scripts/lora/run_finetuning_lora.sh

# 2. å¯åŠ¨ LoRA æ¨ç†æœåŠ¡å™¨
BASE_MODEL_PATH=/home/user/models/Qwen2.5-Coder-7B-Instruct \
LORA_ADAPTER_PATH=../models/qwen2.5-coder-7b-verl-lora \
bash scripts/lora/start_inference_server_lora.sh

# 3. ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨ï¼ˆæŸ¥çœ‹æ—¥å¿—ï¼‰
tail -f ../logs/inference_server_lora.log

# 4. æ‰¹é‡ç”Ÿæˆä»£ç ï¼ˆåœ¨å¦ä¸€ä¸ªç»ˆç«¯ï¼‰
FRAMEWORK=verl \
MODEL_NAME=qwen2.5-coder-7b-verl-lora \
bash scripts/lora/run_batch_code_generation_with_lora_server.sh

# 5. åœæ­¢æœåŠ¡å™¨
bash scripts/lora/stop_inference_server_lora.sh
```

### ç¤ºä¾‹ 2: æ‰‹åŠ¨è°ƒç”¨å®¢æˆ·ç«¯

```bash
# ä½¿ç”¨å®¢æˆ·ç«¯è„šæœ¬ç›´æ¥è°ƒç”¨
python inference_client_lora.py \
    --server_url http://localhost:8001 \
    --input_file ../data/verl/algorithm_methods_data_ARES.jsonl \
    --model_name qwen2.5-coder-7b-verl-lora \
    --num_completions 1 \
    --max_tokens 2048
```

---

## ğŸ”§ API è°ƒç”¨ç¤ºä¾‹

### å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8001/health
```

è¿”å›ç¤ºä¾‹ï¼š
```json
{
  "status": "healthy",
  "model": "/path/to/base/model + ../models/qwen2.5-coder-7b-verl-lora",
  "base_model": "/path/to/base/model",
  "lora_adapter": "../models/qwen2.5-coder-7b-verl-lora",
  "device": "cuda:0"
}
```

### ä»£ç ç”Ÿæˆ

```bash
curl -X POST http://localhost:8001/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompts": ["def fibonacci(n):\n    "],
    "num_completions": 1,
    "max_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.95
  }'
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: LoRA å’Œ SFT æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

- **SFT (Supervised Fine-Tuning)**: å…¨å‚æ•°å¾®è°ƒï¼Œç”Ÿæˆå®Œæ•´æ¨¡å‹
- **LoRA**: å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œåªè®­ç»ƒå°‘é‡ adapter æƒé‡
  - ä¼˜ç‚¹ï¼šæ˜¾å­˜å ç”¨å°‘ï¼Œè®­ç»ƒå¿«ï¼Œé€‚åˆå¤šä»»åŠ¡
  - ç¼ºç‚¹ï¼šæ¨ç†æ—¶éœ€è¦é¢å¤–åŠ è½½ adapter

### Q2: LoRA adapter åœ¨å“ªé‡Œï¼Ÿ

LoRA è®­ç»ƒåï¼Œadapter æƒé‡ä¿å­˜åœ¨è¾“å‡ºç›®å½•ä¸­ï¼š
```
../models/qwen2.5-coder-7b-verl-lora/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors  # LoRA æƒé‡
â””â”€â”€ ...
```

### Q3: ä¸ºä»€ä¹ˆè¦ç”¨æœåŠ¡å™¨æ¶æ„ï¼Ÿ

- åŸºç¡€æ¨¡å‹åŠ è½½ä¸€æ¬¡ï¼Œå¤šæ¬¡å¤ç”¨
- é¿å…æ¯æ¬¡æ¨ç†éƒ½é‡æ–°åŠ è½½æ¨¡å‹
- å¤§å¹…æå‡æ‰¹é‡ç”Ÿæˆæ•ˆç‡

### Q4: LoRA æœåŠ¡å™¨å’Œæ™®é€šæœåŠ¡å™¨èƒ½åŒæ—¶è¿è¡Œå—ï¼Ÿ

å¯ä»¥ï¼ä¸¤è€…ä½¿ç”¨ä¸åŒçš„ç«¯å£ï¼š
- SFT/æ™®é€šæœåŠ¡å™¨: ç«¯å£ `8000`
- LoRA æœåŠ¡å™¨: ç«¯å£ `8001`ï¼ˆé»˜è®¤ï¼‰

### Q5: å¦‚ä½•é€‰æ‹©ä½¿ç”¨å“ªç§æ–¹æ¡ˆï¼Ÿ

**ä½¿ç”¨ LoRA æœåŠ¡å™¨ï¼ˆæ–¹æ¡ˆ Aï¼‰**é€‚åˆï¼š
- éœ€è¦å¿«é€Ÿåˆ‡æ¢ä¸åŒçš„ adapter
- å¤šä¸ª LoRA æ¨¡å‹å¤ç”¨åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹
- æƒ³èŠ‚çœç£ç›˜ç©ºé—´ï¼ˆä¸éœ€è¦å¤šä¸ªå®Œæ•´æ¨¡å‹ï¼‰

**åˆå¹¶æƒé‡ï¼ˆæ–¹æ¡ˆ Bï¼‰**é€‚åˆï¼š
- åªæœ‰ä¸€ä¸ª LoRA æ¨¡å‹é•¿æœŸä½¿ç”¨
- ä¸æƒ³ä¾èµ– PEFT åº“
- å¸Œæœ›æ¨ç†é€Ÿåº¦æ›´å¿«ï¼ˆåˆå¹¶åå°‘ä¸€æ¬¡æƒé‡åŠ è½½ï¼‰

---

## ğŸ†˜ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: æœåŠ¡å™¨å¯åŠ¨å¤±è´¥

```bash
# æŸ¥çœ‹æ—¥å¿—
tail -50 ../logs/inference_server_lora.log

# å¸¸è§åŸå› ï¼š
# 1. åŸºç¡€æ¨¡å‹è·¯å¾„é”™è¯¯
# 2. LoRA adapter è·¯å¾„é”™è¯¯
# 3. ç¼ºå°‘ peft åº“: pip install peft
# 4. æ˜¾å­˜ä¸è¶³
```

### é—®é¢˜ 2: ç”Ÿæˆå¤±è´¥

```bash
# æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
curl http://localhost:8001/health

# æŸ¥çœ‹å®¢æˆ·ç«¯æ—¥å¿—
cat /tmp/gen_lora_*.log

# é‡å¯æœåŠ¡å™¨
bash scripts/lora/stop_inference_server_lora.sh
bash scripts/lora/start_inference_server_lora.sh
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LoRA åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [PEFT åº“æ–‡æ¡£](https://github.com/huggingface/peft)
- [Qwen2.5 æ¨¡å‹æ–‡æ¡£](https://github.com/QwenLM/Qwen2.5)

