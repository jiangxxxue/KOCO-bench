#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
merge_lora.py â€” å°† LoRA adapter åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹

ç”¨é€”ï¼š
1. å°† LoRA adapter æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ï¼Œç”Ÿæˆå®Œæ•´çš„æ¨¡å‹
2. åˆå¹¶åçš„æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨ transformers åŠ è½½ï¼Œæ— éœ€ peft åº“
3. ä¾¿äºéƒ¨ç½²å’Œåˆ†å‘

ä½¿ç”¨ç¤ºä¾‹ï¼š
python merge_lora.py \
  --base_model /path/to/Qwen2.5-Coder-7B-Instruct \
  --lora_adapter ./models/qwen2.5-coder-7b-verl-lora \
  --output_dir ./models/qwen2.5-coder-7b-verl-merged
"""

import os
import sys
import torch
import argparse
from pathlib import Path

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


def merge_lora_weights(
    base_model_path: str,
    lora_adapter_path: str,
    output_dir: str,
    device: str = "auto",
    torch_dtype: str = "auto",
):
    """
    åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_adapter_path: LoRA adapter è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        device: è®¾å¤‡
        torch_dtype: æ•°æ®ç±»å‹
    """
    print("="*60)
    print("ğŸ”§ å¼€å§‹åˆå¹¶ LoRA æƒé‡")
    print("="*60)
    
    # å¤„ç† torch_dtype
    if torch_dtype == "auto":
        dtype = "auto"
    elif torch_dtype == "bfloat16":
        dtype = torch.bfloat16
    elif torch_dtype == "float16":
        dtype = torch.float16
    else:
        dtype = torch.float32
    
    print(f"\nğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=dtype,
        device_map=device,
        trust_remote_code=True,
        attn_implementation="eager",  # é¿å… flash-attention GLIBC é—®é¢˜
    )
    
    print(f"\nğŸ”§ åŠ è½½ LoRA adapter: {lora_adapter_path}")
    model = PeftModel.from_pretrained(
        base_model,
        lora_adapter_path,
        torch_dtype=dtype,
    )
    
    print("\nğŸ”„ åˆå¹¶æƒé‡ä¸­...")
    merged_model = model.merge_and_unload()
    
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    merged_model.save_pretrained(
        output_dir,
        safe_serialization=True,  # ä½¿ç”¨ safetensors æ ¼å¼
    )
    
    # ä¿å­˜ tokenizer
    print("ğŸ’¾ ä¿å­˜ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        lora_adapter_path,
        trust_remote_code=True,
    )
    tokenizer.save_pretrained(output_dir)
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    print("ğŸ’¾ ä¿å­˜åˆå¹¶é…ç½®...")
    merge_info = {
        "base_model": base_model_path,
        "lora_adapter": lora_adapter_path,
        "merged_at": __import__('datetime').datetime.now().isoformat(),
        "device": str(device),
        "dtype": str(dtype),
    }
    
    import json
    with open(os.path.join(output_dir, "merge_info.json"), "w", encoding="utf-8") as f:
        json.dump(merge_info, f, ensure_ascii=False, indent=2)
    
    print("\n" + "="*60)
    print("âœ… åˆå¹¶å®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")
    print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print("```python")
    print("from transformers import AutoModelForCausalLM, AutoTokenizer")
    print(f"model = AutoModelForCausalLM.from_pretrained('{output_dir}')")
    print(f"tokenizer = AutoTokenizer.from_pretrained('{output_dir}')")
    print("```")
    print("\n" + "="*60)


def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹")
    parser.add_argument(
        "--base_model",
        "-b",
        type=str,
        required=True,
        help="åŸºç¡€æ¨¡å‹è·¯å¾„",
    )
    parser.add_argument(
        "--lora_adapter",
        "-l",
        type=str,
        required=True,
        help="LoRA adapter è·¯å¾„",
    )
    parser.add_argument(
        "--output_dir",
        "-o",
        type=str,
        required=True,
        help="è¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="è®¾å¤‡: auto, cuda:0, cpu ç­‰ï¼ˆé»˜è®¤: autoï¼‰",
    )
    parser.add_argument(
        "--dtype",
        type=str,
        default="auto",
        choices=["auto", "bfloat16", "float16", "float32"],
        help="æ•°æ®ç±»å‹ï¼ˆé»˜è®¤: autoï¼‰",
    )
    
    args = parser.parse_args()
    
    # éªŒè¯è·¯å¾„
    if not Path(args.base_model).exists():
        print(f"âŒ é”™è¯¯: åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.base_model}")
        return 1
    
    if not Path(args.lora_adapter).exists():
        print(f"âŒ é”™è¯¯: LoRA adapter è·¯å¾„ä¸å­˜åœ¨: {args.lora_adapter}")
        return 1
    
    # æ‰§è¡Œåˆå¹¶
    try:
        merge_lora_weights(
            base_model_path=args.base_model,
            lora_adapter_path=args.lora_adapter,
            output_dir=args.output_dir,
            device=args.device,
            torch_dtype=args.dtype,
        )
        return 0
    except Exception as e:
        print(f"\nâŒ åˆå¹¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())

