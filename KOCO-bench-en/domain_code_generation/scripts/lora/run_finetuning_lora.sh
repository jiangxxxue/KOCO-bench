#!/bin/bash
# LoRA å¾®è°ƒè„šæœ¬ï¼ˆé€‚é… finetuning_lora.pyï¼‰
# åŸºäºåŸç‰ˆ run_finetuning.shï¼Œè°ƒæ•´ä¸º LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒ

# ä¸¥æ ¼æ¨¡å¼ï¼šé‡é”™ç«‹å³é€€å‡º
set -euo pipefail

# ========= åŸºç¡€è·¯å¾„ =========
MODEL_PATH="/home/shixianjie/models/Qwen2.5-Coder-7B-Instruct"
FRAMEWORK="smolagents"
DATA_PATH="../data/${FRAMEWORK}/${FRAMEWORK}_training_dataset.jsonl"
OUTPUT_DIR="../models/qwen2.5-coder-7b-${FRAMEWORK}-lora"

# ========= LoRA å‚æ•° =========
LORA_R=16                                        # LoRA rankï¼Œå»ºè®® 8-64
LORA_ALPHA=32                                    # LoRA alphaï¼Œé€šå¸¸ä¸º 2*r
LORA_DROPOUT=0.05                                # LoRA dropout
TARGET_MODULES="q_proj,v_proj,k_proj,o_proj"    # åº”ç”¨ LoRA çš„æ¨¡å—
USE_RSLORA=false                                 # æ˜¯å¦ä½¿ç”¨ Rank-Stabilized LoRA
USE_DORA=false                                   # æ˜¯å¦ä½¿ç”¨ DoRA

# ========= è®­ç»ƒå‚æ•°ï¼ˆé’ˆå¯¹ LoRA ä¼˜åŒ–ï¼‰=========
MAX_SEQ_LENGTH=2048
BATCH_SIZE=4                                     # LoRA æ˜¾å­˜å ç”¨æ›´å°‘ï¼Œå¯é€‚å½“å¢å¤§
GRADIENT_ACCUMULATION=2                          # ç›¸åº”å‡å°‘æ¢¯åº¦ç´¯ç§¯
LEARNING_RATE=1e-4                               # LoRA é€šå¸¸ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ï¼ˆ1e-4 åˆ° 3e-4ï¼‰
NUM_EPOCHS=5                                     # LoRA æ”¶æ•›å¿«ï¼Œå¯é€‚å½“å¢åŠ è½®æ•°
WARMUP_RATIO=0.03
KEEP_FILE_TYPES="python,shell,yaml,markdown"
STRIDE_FRACTION=0.125
ADD_FILE_PATH_HEADER="false"

# ========= GPU / ç¯å¢ƒ =========
export CUDA_VISIBLE_DEVICES=4,5
export TOKENIZERS_PARALLELISM=false

# ç¦ç”¨ flash-attention è‡ªåŠ¨æ£€æµ‹ï¼ˆé¿å… GLIBC é—®é¢˜ï¼‰
export TRANSFORMERS_NO_ADVISORY_WARNINGS=1
export DISABLE_FLASH_ATTN=1

echo "========================================================"
echo "ğŸš€ å¼€å§‹ ${FRAMEWORK} ä»£ç åº“ LoRA å¾®è°ƒï¼ˆfinetuning_lora.pyï¼‰"
echo "========================================================"
echo "æ¨¡å‹: ${MODEL_PATH}"
echo "æ•°æ®: ${DATA_PATH}"
echo "è¾“å‡º: ${OUTPUT_DIR}"
echo "åºåˆ—é•¿åº¦: ${MAX_SEQ_LENGTH}"
echo "Batchå¤§å°: ${BATCH_SIZE} x ${GRADIENT_ACCUMULATION} = $((BATCH_SIZE * GRADIENT_ACCUMULATION))"
echo "å­¦ä¹ ç‡: ${LEARNING_RATE}"
echo "è®­ç»ƒè½®æ•°: ${NUM_EPOCHS}"
echo "========================================================"
echo "LoRA é…ç½®:"
echo "  - Rank (r): ${LORA_R}"
echo "  - Alpha: ${LORA_ALPHA}"
echo "  - Dropout: ${LORA_DROPOUT}"
echo "  - Target Modules: ${TARGET_MODULES}"
echo "  - Use RSLoRA: ${USE_RSLORA}"
echo "  - Use DoRA: ${USE_DORA}"
echo "========================================================"
echo ""

mkdir -p "${OUTPUT_DIR}"

# åˆ‡æ¢åˆ° lora ç›®å½•æ‰§è¡Œ
cd "$(dirname "$0")"

# --------- LoRA å¾®è°ƒï¼ˆå•æœºå¤šå¡æˆ–å•å¡ï¼‰---------
python finetuning_lora.py \
  --model_name_or_path "${MODEL_PATH}" \
  --dataset_path "${DATA_PATH}" \
  --output_dir "${OUTPUT_DIR}" \
  --max_seq_length "${MAX_SEQ_LENGTH}" \
  --val_split_ratio 0.1 \
  --per_device_train_batch_size "${BATCH_SIZE}" \
  --per_device_eval_batch_size "${BATCH_SIZE}" \
  --gradient_accumulation_steps "${GRADIENT_ACCUMULATION}" \
  --num_train_epochs "${NUM_EPOCHS}" \
  --learning_rate "${LEARNING_RATE}" \
  --lr_scheduler_type cosine \
  --warmup_ratio "${WARMUP_RATIO}" \
  --max_grad_norm 1.0 \
  --optim adamw_torch \
  --logging_steps 10 \
  --save_steps 200 \
  --eval_steps 200 \
  --save_strategy steps \
  --save_total_limit 3 \
  --use_wandb false \
  --fp16 false \
  --bf16 true \
  --tf32 true \
  --dataloader_num_workers 4 \
  --gradient_checkpointing true \
  --remove_unused_columns false \
  --logging_first_step true \
  --report_to none \
  --keep_file_types "${KEEP_FILE_TYPES}" \
  --stride_fraction "${STRIDE_FRACTION}" \
  --add_file_path_header "${ADD_FILE_PATH_HEADER}" \
  --lora_r "${LORA_R}" \
  --lora_alpha "${LORA_ALPHA}" \
  --lora_dropout "${LORA_DROPOUT}" \
  --target_modules "${TARGET_MODULES}" \
  --use_rslora "${USE_RSLORA}" \
  --use_dora "${USE_DORA}"

echo ""
echo "========================================================"
echo "ğŸ‰ LoRA å¾®è°ƒå®Œæˆï¼Adapter ä¿å­˜åœ¨: ${OUTPUT_DIR}"
echo "========================================================"
echo ""
echo "ğŸ’¡ ä½¿ç”¨æ–¹æ³•ï¼š"
echo "from peft import PeftModel"
echo "from transformers import AutoModelForCausalLM, AutoTokenizer"
echo ""
echo "base_model = AutoModelForCausalLM.from_pretrained('${MODEL_PATH}')"
echo "model = PeftModel.from_pretrained(base_model, '${OUTPUT_DIR}')"
echo "tokenizer = AutoTokenizer.from_pretrained('${OUTPUT_DIR}')"
echo "========================================================"

