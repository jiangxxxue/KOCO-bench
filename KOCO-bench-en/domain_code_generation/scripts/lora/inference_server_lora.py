#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
inference_server_lora.py â€” LoRA æ¨¡å‹æ¨ç†æœåŠ¡å™¨

åŸºäº FastAPI æä¾›ä»£ç ç”ŸæˆæœåŠ¡ï¼Œæ”¯æŒ LoRA adapter åŠ è½½
"""

import os
import sys
import argparse
import torch
import uvicorn
from typing import List, Optional, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from peft import PeftModel


# ========================================
# å…¨å±€å˜é‡
# ========================================
model = None
tokenizer = None
generation_config = None
base_model_path = None
lora_adapter_path = None


# ========================================
# è¯·æ±‚/å“åº”æ¨¡å‹
# ========================================

class GenerationRequest(BaseModel):
    """ç”Ÿæˆè¯·æ±‚"""
    prompts: List[Any] = Field(..., description="è¾“å…¥æç¤ºåˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å¯¹è¯åˆ—è¡¨ï¼‰")
    num_completions: int = Field(1, ge=1, le=10, description="æ¯ä¸ªæç¤ºç”Ÿæˆçš„è¡¥å…¨æ•°é‡")
    max_tokens: int = Field(512, ge=1, le=4096, description="ç”Ÿæˆçš„æœ€å¤§tokenæ•°")
    temperature: float = Field(0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    top_p: float = Field(0.95, ge=0.0, le=1.0, description="Top-pé‡‡æ ·")
    do_sample: bool = Field(True, description="æ˜¯å¦ä½¿ç”¨é‡‡æ ·")


class GenerationResponse(BaseModel):
    """ç”Ÿæˆå“åº”"""
    completions: List[List[str]] = Field(..., description="ç”Ÿæˆç»“æœï¼Œå¤–å±‚åˆ—è¡¨å¯¹åº”è¾“å…¥æç¤ºï¼Œå†…å±‚åˆ—è¡¨å¯¹åº”æ¯ä¸ªæç¤ºçš„å¤šä¸ªè¡¥å…¨")
    model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹")


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str = Field("healthy", description="æœåŠ¡çŠ¶æ€")
    model: str = Field(..., description="åŠ è½½çš„æ¨¡å‹")
    base_model: str = Field(..., description="åŸºç¡€æ¨¡å‹è·¯å¾„")
    lora_adapter: str = Field(..., description="LoRA adapterè·¯å¾„")
    device: str = Field(..., description="è®¾å¤‡ä¿¡æ¯")


# ========================================
# æ¨¡å‹åŠ è½½
# ========================================

def load_lora_model(
    base_model: str,
    lora_adapter: str,
    device: str = "auto",
    torch_dtype: str = "bfloat16",
    max_context_len: int = 4096,
):
    """
    åŠ è½½åŸºç¡€æ¨¡å‹ + LoRA adapter
    
    Args:
        base_model: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_adapter: LoRA adapter è·¯å¾„
        device: è®¾å¤‡ï¼Œ"auto" æˆ– "cuda:0" ç­‰
        torch_dtype: æ•°æ®ç±»å‹
        max_context_len: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    
    Returns:
        model, tokenizer, generation_config
    """
    print(f"ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model}")
    
    # å¤„ç† torch_dtype
    if torch_dtype == "auto":
        dtype = "auto"
    elif torch_dtype == "bfloat16":
        dtype = torch.bfloat16
    elif torch_dtype == "float16":
        dtype = torch.float16
    else:
        dtype = torch.float32
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=dtype,
        device_map=device,
        trust_remote_code=True,
        attn_implementation="eager",  # é¿å… flash-attention é—®é¢˜
    )
    
    print(f"ğŸ“¦ åŠ è½½ LoRA adapter: {lora_adapter}")
    
    # åŠ è½½ LoRA adapter
    model = PeftModel.from_pretrained(
        base,
        lora_adapter,
        torch_dtype=dtype,
    )
    
    # åˆå¹¶ LoRA æƒé‡ä»¥æé«˜æ¨ç†é€Ÿåº¦ï¼ˆå¯é€‰ï¼‰
    # model = model.merge_and_unload()
    
    print(f"ğŸ“¦ åŠ è½½ tokenizer: {base_model}")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model,
        trust_remote_code=True,
        padding_side="left",  # æ‰¹é‡ç”Ÿæˆæ—¶éœ€è¦å·¦å¡«å……
    )
    
    # è®¾ç½® pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # ç”Ÿæˆé…ç½®
    gen_config = GenerationConfig(
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        top_p=0.95,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"  - åŸºç¡€æ¨¡å‹: {base_model}")
    print(f"  - LoRA adapter: {lora_adapter}")
    print(f"  - Device: {device}")
    print(f"  - Dtype: {dtype}")
    
    return model, tokenizer, gen_config


# ========================================
# ç”Ÿæˆå‡½æ•°
# ========================================

def format_prompt(prompt_data: Any) -> str:
    """
    æ ¼å¼åŒ– prompt ä¸ºå­—ç¬¦ä¸²
    
    Args:
        prompt_data: å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å¯¹è¯åˆ—è¡¨
    
    Returns:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    global tokenizer
    
    if isinstance(prompt_data, str):
        return prompt_data
    elif isinstance(prompt_data, list):
        # å¯¹è¯åˆ—è¡¨æ ¼å¼ï¼š[{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]
        # ä½¿ç”¨ tokenizer çš„ apply_chat_template æ–¹æ³•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(tokenizer, 'apply_chat_template'):
            try:
                return tokenizer.apply_chat_template(
                    prompt_data,
                    tokenize=False,
                    add_generation_prompt=True
                )
            except Exception:
                # å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ ¼å¼åŒ–
                pass
        
        # ç®€å•æ ¼å¼åŒ–
        formatted_parts = []
        for message in prompt_data:
            role = message.get("role", "user")
            content = message.get("content", "")
            if role == "system":
                formatted_parts.append(f"System: {content}")
            elif role == "user":
                formatted_parts.append(f"User: {content}")
            elif role == "assistant":
                formatted_parts.append(f"Assistant: {content}")
        return "\n\n".join(formatted_parts) + "\n\nAssistant: "
    else:
        return str(prompt_data)


def generate_completions(
    prompts: List[Any],
    num_completions: int = 1,
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.95,
    do_sample: bool = True,
) -> List[List[str]]:
    """
    æ‰¹é‡ç”Ÿæˆä»£ç è¡¥å…¨
    
    Args:
        prompts: æç¤ºåˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å¯¹è¯åˆ—è¡¨ï¼‰
        num_completions: æ¯ä¸ªæç¤ºç”Ÿæˆçš„è¡¥å…¨æ•°é‡
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: æ¸©åº¦å‚æ•°
        top_p: Top-pé‡‡æ ·
        do_sample: æ˜¯å¦é‡‡æ ·
    
    Returns:
        è¡¥å…¨åˆ—è¡¨ï¼Œå¤–å±‚å¯¹åº”æ¯ä¸ªæç¤ºï¼Œå†…å±‚å¯¹åº”æ¯ä¸ªæç¤ºçš„å¤šä¸ªè¡¥å…¨
    """
    global model, tokenizer, generation_config
    
    if model is None or tokenizer is None:
        raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
    
    results = []
    
    for prompt_data in prompts:
        prompt_completions = []
        
        # æ ¼å¼åŒ– prompt
        prompt = format_prompt(prompt_data)
        
        # ä¸ºæ¯ä¸ªè¡¥å…¨å•ç‹¬ç”Ÿæˆ
        for _ in range(num_completions):
            # Tokenize
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
            ).to(model.device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=do_sample,
                    temperature=temperature,
                    top_p=top_p,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            # è§£ç 
            input_length = inputs["input_ids"].shape[1]
            generated_tokens = outputs[0][input_length:]
            completion = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            prompt_completions.append(completion)
        
        results.append(prompt_completions)
    
    return results


# ========================================
# FastAPI åº”ç”¨
# ========================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    global model, tokenizer, generation_config, base_model_path, lora_adapter_path
    
    print("=" * 60)
    print("ğŸš€ å¯åŠ¨ LoRA æ¨ç†æœåŠ¡å™¨")
    print("=" * 60)
    
    model, tokenizer, generation_config = load_lora_model(
        base_model=base_model_path,
        lora_adapter=lora_adapter_path,
        device=app.state.device,
        torch_dtype=app.state.torch_dtype,
        max_context_len=app.state.max_context_len,
    )
    
    print("=" * 60)
    print("âœ… æœåŠ¡å™¨å¯åŠ¨å®Œæˆ")
    print("=" * 60)
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    print("ğŸ›‘ å…³é—­æœåŠ¡å™¨...")


app = FastAPI(
    title="LoRA ä»£ç ç”Ÿæˆæ¨ç†æœåŠ¡å™¨",
    description="æ”¯æŒ LoRA adapter çš„ä»£ç è¡¥å…¨æ¨ç†æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan,
)


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    if model is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    device_name = str(next(model.parameters()).device)
    
    return HealthResponse(
        status="healthy",
        model=f"{base_model_path} + {lora_adapter_path}",
        base_model=base_model_path,
        lora_adapter=lora_adapter_path,
        device=device_name,
    )


@app.post("/generate", response_model=GenerationResponse)
async def generate(request: GenerationRequest):
    """ä»£ç ç”Ÿæˆç«¯ç‚¹"""
    try:
        completions = generate_completions(
            prompts=request.prompts,
            num_completions=request.num_completions,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            do_sample=request.do_sample,
        )
        
        return GenerationResponse(
            completions=completions,
            model=f"{base_model_path} + {lora_adapter_path}",
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")


# ========================================
# ä¸»å‡½æ•°
# ========================================

def main():
    parser = argparse.ArgumentParser(
        description="LoRA ä»£ç è¡¥å…¨æ¨ç†æœåŠ¡å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¯åŠ¨æœåŠ¡å™¨ï¼ˆé»˜è®¤ç«¯å£ 8000ï¼‰
  python inference_server_lora.py \\
    --base_model /path/to/base/model \\
    --lora_adapter ../models/qwen2.5-coder-7b-verl-lora

  # æŒ‡å®šç«¯å£
  python inference_server_lora.py \\
    --base_model /path/to/base/model \\
    --lora_adapter ../models/qwen2.5-coder-7b-verl-lora \\
    --port 8001

  # æµ‹è¯•å¥åº·æ£€æŸ¥
  curl http://localhost:8000/health

  # æµ‹è¯•ç”Ÿæˆ
  curl -X POST http://localhost:8000/generate \\
    -H "Content-Type: application/json" \\
    -d '{"prompts": ["def hello():\\n    "], "num_completions": 1}'
"""
    )
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--base_model",
        type=str,
        required=True,
        help="åŸºç¡€æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--lora_adapter",
        type=str,
        required=True,
        help="LoRA adapter è·¯å¾„"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="è®¾å¤‡ (é»˜è®¤: auto)"
    )
    parser.add_argument(
        "--torch_dtype",
        type=str,
        default="bfloat16",
        choices=["auto", "bfloat16", "float16", "float32"],
        help="æ¨¡å‹æ•°æ®ç±»å‹ (é»˜è®¤: bfloat16)"
    )
    parser.add_argument(
        "--max_context_len",
        type=int,
        default=4096,
        help="æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ (é»˜è®¤: 4096)"
    )
    
    # æœåŠ¡å™¨å‚æ•°
    parser.add_argument(
        "--host",
        type=str,
        default="0.0.0.0",
        help="æœåŠ¡å™¨åœ°å€ (é»˜è®¤: 0.0.0.0)"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 8000)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®å…¨å±€å˜é‡
    global base_model_path, lora_adapter_path
    base_model_path = args.base_model
    lora_adapter_path = args.lora_adapter
    
    # ä¿å­˜å‚æ•°åˆ° app.state
    app.state.device = args.device
    app.state.torch_dtype = args.torch_dtype
    app.state.max_context_len = args.max_context_len
    
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info",
    )


if __name__ == "__main__":
    main()

