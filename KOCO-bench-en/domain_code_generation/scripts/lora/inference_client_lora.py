#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
inference_client_lora.py â€” LoRA æ¨ç†å®¢æˆ·ç«¯

é€šè¿‡ HTTP è¯·æ±‚è°ƒç”¨ LoRA æ¨ç†æœåŠ¡å™¨è¿›è¡Œä»£ç ç”Ÿæˆ
ä¸ inference_client.py ç±»ä¼¼ï¼Œä½†ä¸“é—¨ç”¨äº LoRA æœåŠ¡å™¨
"""

import json
import argparse
import requests
import time
from typing import List, Dict, Any
from pathlib import Path


class LoRAInferenceClient:
    """LoRA æ¨ç†å®¢æˆ·ç«¯"""
    
    def __init__(self, server_url: str):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            server_url: æœåŠ¡å™¨åœ°å€ï¼Œä¾‹å¦‚ http://localhost:8000
        """
        self.server_url = server_url.rstrip('/')
        self.health_url = f"{self.server_url}/health"
        self.generate_url = f"{self.server_url}/generate"
    
    def check_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            response = requests.get(self.health_url, timeout=5)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            raise RuntimeError(f"æœåŠ¡å™¨å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
    
    def generate(
        self,
        prompts: List[str],
        num_completions: int = 1,
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.95,
        do_sample: bool = True,
    ) -> List[List[str]]:
        """
        ç”Ÿæˆä»£ç è¡¥å…¨
        
        Args:
            prompts: æç¤ºåˆ—è¡¨
            num_completions: æ¯ä¸ªæç¤ºç”Ÿæˆçš„è¡¥å…¨æ•°é‡
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-pé‡‡æ ·
            do_sample: æ˜¯å¦é‡‡æ ·
        
        Returns:
            è¡¥å…¨åˆ—è¡¨ï¼Œå¤–å±‚å¯¹åº”æ¯ä¸ªæç¤ºï¼Œå†…å±‚å¯¹åº”æ¯ä¸ªæç¤ºçš„å¤šä¸ªè¡¥å…¨
        """
        payload = {
            "prompts": prompts,
            "num_completions": num_completions,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "do_sample": do_sample,
        }
        
        try:
            response = requests.post(
                self.generate_url,
                json=payload,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            response.raise_for_status()
            result = response.json()
            return result["completions"]
        
        except requests.exceptions.Timeout:
            raise RuntimeError("è¯·æ±‚è¶…æ—¶")
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"è¯·æ±‚å¤±è´¥: {e}")
        except Exception as e:
            raise RuntimeError(f"ç”Ÿæˆå¤±è´¥: {e}")


def load_jsonl_data(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½ JSONL æ•°æ®"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                data.append(json.loads(line))
    return data


def save_jsonl_data(data: List[Dict[str, Any]], file_path: str):
    """ä¿å­˜ JSONL æ•°æ®"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for record in data:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')


def format_prompt(prompt_data):
    """
    å°† prompt æ•°æ®æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    
    Args:
        prompt_data: å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å¯¹è¯åˆ—è¡¨
    
    Returns:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    if isinstance(prompt_data, str):
        return prompt_data
    elif isinstance(prompt_data, list):
        # å¯¹è¯åˆ—è¡¨æ ¼å¼ï¼š[{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]
        formatted_parts = []
        for message in prompt_data:
            role = message.get("role", "user")
            content = message.get("content", "")
            if role == "system":
                formatted_parts.append(f"System: {content}")
            elif role == "user":
                formatted_parts.append(f"User: {content}")
            elif role == "assistant":
                formatted_parts.append(f"Assistant: {content}")
        return "\n\n".join(formatted_parts)
    else:
        return str(prompt_data)


def process_dataset(
    client: LoRAInferenceClient,
    input_file: str,
    output_file: str,
    model_name: str,
    num_completions: int = 1,
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.95,
    batch_size: int = 1,
):
    """
    å¤„ç†æ•°æ®é›†
    
    Args:
        client: æ¨ç†å®¢æˆ·ç«¯
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºè¾“å‡ºç›®å½•ï¼‰
        num_completions: æ¯ä¸ªä»»åŠ¡ç”Ÿæˆçš„è¡¥å…¨æ•°é‡
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: æ¸©åº¦å‚æ•°
        top_p: Top-pé‡‡æ ·
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    # åŠ è½½æ•°æ®
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_file}")
    data = load_jsonl_data(input_file)
    print(f"  æ‰¾åˆ° {len(data)} ä¸ªä»»åŠ¡")
    
    # æ„å»ºè¾“å‡ºè·¯å¾„
    output_path = Path(output_file)
    if not output_path.is_absolute():
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºè¾“å…¥æ–‡ä»¶çš„ç›®å½•
        input_path = Path(input_file)
        base_dir = input_path.parent
        
        # åˆ›å»ºæ¨¡å‹è¾“å‡ºç›®å½•
        model_dir = base_dir / model_name
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_filename = input_path.stem + "_output" + input_path.suffix
        output_path = model_dir / output_filename
    
    print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print()
    
    # æ‰¹é‡å¤„ç†
    total = len(data)
    processed = 0
    
    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]
        # ç›´æ¥ä½¿ç”¨ promptï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å¯¹è¯åˆ—è¡¨ï¼ŒæœåŠ¡å™¨ç«¯ä¼šå¤„ç†æ ¼å¼åŒ–ï¼‰
        batch_prompts = [item["prompt"] for item in batch]
        
        print(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1} / {(total + batch_size - 1) // batch_size}")
        print(f"   ä»»åŠ¡ {i + 1}-{min(i + len(batch), total)} / {total}")
        
        try:
            # ç”Ÿæˆ
            start_time = time.time()
            completions = client.generate(
                prompts=batch_prompts,
                num_completions=num_completions,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
            )
            elapsed = time.time() - start_time
            
            # ä¿å­˜ç»“æœ
            for j, item in enumerate(batch):
                item["completions"] = completions[j]
            
            processed += len(batch)
            print(f"   âœ… å®Œæˆ ({elapsed:.2f}s) - è¿›åº¦: {processed}/{total}")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            # ä¿å­˜ç©ºç»“æœ
            for item in batch:
                item["completions"] = [""] * num_completions
        
        print()
    
    # ä¿å­˜è¾“å‡º
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
    save_jsonl_data(data, str(output_path))
    print()
    print("âœ… å¤„ç†å®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(
        description="LoRA ä»£ç è¡¥å…¨æ¨ç†å®¢æˆ·ç«¯ - é€šè¿‡ HTTP è¯·æ±‚è°ƒç”¨æ¨ç†æœåŠ¡å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ç”¨æ³•
  python inference_client_lora.py \\
    --server_url http://localhost:8000 \\
    --input_file ../data/algorithm_methods_data.jsonl \\
    --output_file ../data/algorithm_methods_output.jsonl

  # ç”Ÿæˆå¤šä¸ªè¡¥å…¨
  python inference_client_lora.py \\
    --server_url http://localhost:8000 \\
    --input_file ../data/algorithm_methods_data.jsonl \\
    --output_file ../data/algorithm_methods_output.jsonl \\
    --num_completions 4 \\
    --temperature 0.2

  # ä½¿ç”¨æ‰¹å¤„ç†
  python inference_client_lora.py \\
    --server_url http://localhost:8000 \\
    --input_file ../data/algorithm_methods_data.jsonl \\
    --output_file ../data/algorithm_methods_output.jsonl \\
    --batch_size 4
        """
    )
    
    # æœåŠ¡å™¨é…ç½®
    parser.add_argument(
        "--server_url",
        type=str,
        required=True,
        help="æ¨ç†æœåŠ¡å™¨åœ°å€ï¼Œä¾‹å¦‚ http://localhost:8000"
    )
    
    # æ–‡ä»¶è·¯å¾„
    parser.add_argument(
        "--input_file",
        type=str,
        required=True,
        help="è¾“å…¥ JSONL æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output_file",
        type=str,
        default=None,
        help="è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: è‡ªåŠ¨ç”Ÿæˆï¼‰"
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="lora-model",
        help="æ¨¡å‹åç§°ï¼ˆç”¨äºè¾“å‡ºç›®å½•ï¼‰"
    )
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument(
        "--num_completions",
        type=int,
        default=1,
        help="æ¯ä¸ªä»»åŠ¡ç”Ÿæˆçš„è¡¥å…¨æ•°é‡ (é»˜è®¤: 1)"
    )
    parser.add_argument(
        "--max_tokens",
        type=int,
        default=512,
        help="æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: 512)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="æ¸©åº¦å‚æ•° (é»˜è®¤: 0.7)"
    )
    parser.add_argument(
        "--top_p",
        type=float,
        default=0.95,
        help="Top-pé‡‡æ · (é»˜è®¤: 0.95)"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 1)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®è¾“å‡ºæ–‡ä»¶
    if args.output_file is None:
        input_path = Path(args.input_file)
        args.output_file = str(input_path.parent / f"{input_path.stem}_output{input_path.suffix}")
    
    print("=" * 60)
    print("ğŸš€ LoRA æ¨ç†å®¢æˆ·ç«¯")
    print("=" * 60)
    print(f"æœåŠ¡å™¨: {args.server_url}")
    print(f"è¾“å…¥æ–‡ä»¶: {args.input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output_file}")
    print(f"æ¨¡å‹åç§°: {args.model_name}")
    print(f"è¡¥å…¨æ•°é‡: {args.num_completions}")
    print(f"æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print("=" * 60)
    print()
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = LoRAInferenceClient(args.server_url)
    
    # å¥åº·æ£€æŸ¥
    print("ğŸ” æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€...")
    try:
        health = client.check_health()
        print(f"âœ… æœåŠ¡å™¨æ­£å¸¸")
        print(f"  - çŠ¶æ€: {health['status']}")
        print(f"  - åŸºç¡€æ¨¡å‹: {health['base_model']}")
        print(f"  - LoRA adapter: {health['lora_adapter']}")
        print(f"  - è®¾å¤‡: {health['device']}")
        print()
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨ä¸å¯ç”¨: {e}")
        return 1
    
    # å¤„ç†æ•°æ®é›†
    try:
        process_dataset(
            client=client,
            input_file=args.input_file,
            output_file=args.output_file,
            model_name=args.model_name,
            num_completions=args.num_completions,
            max_tokens=args.max_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
            batch_size=args.batch_size,
        )
        return 0
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())

