#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
inference_lora.py â€” LoRA æ¨¡å‹æ¨ç†ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•åŠ è½½å¹¶ä½¿ç”¨è®­ç»ƒå¥½çš„ LoRA adapter è¿›è¡Œä»£ç ç”Ÿæˆ
"""

import os
import sys
import torch
import argparse
from pathlib import Path
from typing import Optional

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


def load_lora_model(
    base_model_path: str,
    lora_adapter_path: str,
    device: str = "auto",
    torch_dtype: str = "auto",
):
    """
    åŠ è½½åŸºç¡€æ¨¡å‹ + LoRA adapter
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_adapter_path: LoRA adapter è·¯å¾„
        device: è®¾å¤‡ï¼Œ"auto" æˆ– "cuda:0" ç­‰
        torch_dtype: æ•°æ®ç±»å‹ï¼Œ"auto", "bfloat16", "float16" ç­‰
    
    Returns:
        model, tokenizer
    """
    print(f"ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    
    # å¤„ç† torch_dtype
    if torch_dtype == "auto":
        dtype = "auto"
    elif torch_dtype == "bfloat16":
        dtype = torch.bfloat16
    elif torch_dtype == "float16":
        dtype = torch.float16
    else:
        dtype = torch.float32
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=dtype,
        device_map=device,
        trust_remote_code=True,
        attn_implementation="eager",  # é¿å… flash-attention GLIBC é—®é¢˜
    )
    
    print(f"ğŸ”§ åŠ è½½ LoRA adapter: {lora_adapter_path}")
    
    # åŠ è½½ LoRA adapter
    model = PeftModel.from_pretrained(
        base_model,
        lora_adapter_path,
        torch_dtype=dtype,
    )
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        lora_adapter_path,
        trust_remote_code=True,
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print(f"   - è®¾å¤‡: {model.device}")
    print(f"   - æ•°æ®ç±»å‹: {model.dtype}")
    
    return model, tokenizer


def generate_code(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens: int = 256,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50,
    repetition_penalty: float = 1.1,
    do_sample: bool = True,
):
    """
    ç”Ÿæˆä»£ç 
    
    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        prompt: è¾“å…¥æç¤º
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        temperature: æ¸©åº¦å‚æ•°
        top_p: nucleus sampling
        top_k: top-k sampling
        repetition_penalty: é‡å¤æƒ©ç½š
        do_sample: æ˜¯å¦é‡‡æ ·
    
    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬
    """
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,
            do_sample=do_sample,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç è¾“å‡º
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text


def interactive_mode(model, tokenizer):
    """äº¤äº’å¼æ¨¡å¼"""
    print("\n" + "="*60)
    print("ğŸ¤– è¿›å…¥äº¤äº’å¼ä»£ç ç”Ÿæˆæ¨¡å¼")
    print("="*60)
    print("è¾“å…¥ä»£ç æç¤ºï¼Œæ¨¡å‹å°†è‡ªåŠ¨è¡¥å…¨")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("è¾“å…¥ 'clear' æ¸…ç©ºå†å²")
    print("="*60 + "\n")
    
    while True:
        try:
            prompt = input("\nğŸ“ è¯·è¾“å…¥ä»£ç æç¤º: ").strip()
            
            if not prompt:
                continue
                
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ é€€å‡ºäº¤äº’æ¨¡å¼")
                break
                
            if prompt.lower() == 'clear':
                print("\033c", end="")  # æ¸…å±
                continue
            
            print("\nğŸ”„ ç”Ÿæˆä¸­...")
            generated = generate_code(model, tokenizer, prompt)
            
            print("\n" + "â”€"*60)
            print("ğŸ“„ ç”Ÿæˆç»“æœ:")
            print("â”€"*60)
            print(generated)
            print("â”€"*60)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ é€€å‡ºäº¤äº’æ¨¡å¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


def batch_inference(model, tokenizer, prompts_file: str, output_file: str):
    """æ‰¹é‡æ¨ç†"""
    print(f"\nğŸ“‚ ä»æ–‡ä»¶è¯»å–æç¤º: {prompts_file}")
    
    with open(prompts_file, 'r', encoding='utf-8') as f:
        prompts = [line.strip() for line in f if line.strip()]
    
    print(f"ğŸ“Š å…± {len(prompts)} ä¸ªæç¤º")
    
    results = []
    for i, prompt in enumerate(prompts, 1):
        print(f"\nğŸ”„ [{i}/{len(prompts)}] ç”Ÿæˆä¸­...")
        print(f"æç¤º: {prompt[:50]}...")
        
        generated = generate_code(model, tokenizer, prompt)
        results.append({
            "prompt": prompt,
            "generated": generated,
        })
    
    # ä¿å­˜ç»“æœ
    import json
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æ‰¹é‡æ¨ç†å®Œæˆï¼ç»“æœä¿å­˜è‡³: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="LoRA æ¨¡å‹æ¨ç†")
    parser.add_argument(
        "--base_model",
        type=str,
        required=True,
        help="åŸºç¡€æ¨¡å‹è·¯å¾„",
    )
    parser.add_argument(
        "--lora_adapter",
        type=str,
        required=True,
        help="LoRA adapter è·¯å¾„",
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="interactive",
        choices=["interactive", "single", "batch"],
        help="æ¨ç†æ¨¡å¼: interactiveï¼ˆäº¤äº’ï¼‰, singleï¼ˆå•æ¬¡ï¼‰, batchï¼ˆæ‰¹é‡ï¼‰",
    )
    parser.add_argument(
        "--prompt",
        type=str,
        help="å•æ¬¡æ¨ç†çš„æç¤ºï¼ˆmode=single æ—¶ä½¿ç”¨ï¼‰",
    )
    parser.add_argument(
        "--prompts_file",
        type=str,
        help="æ‰¹é‡æ¨ç†çš„æç¤ºæ–‡ä»¶ï¼ˆmode=batch æ—¶ä½¿ç”¨ï¼‰",
    )
    parser.add_argument(
        "--output_file",
        type=str,
        default="./inference_results.json",
        help="æ‰¹é‡æ¨ç†çš„è¾“å‡ºæ–‡ä»¶",
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=256,
        help="æœ€å¤§ç”Ÿæˆ token æ•°",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="æ¸©åº¦å‚æ•°",
    )
    parser.add_argument(
        "--top_p",
        type=float,
        default=0.9,
        help="nucleus sampling",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="è®¾å¤‡: auto, cuda:0, cpu ç­‰",
    )
    parser.add_argument(
        "--dtype",
        type=str,
        default="auto",
        choices=["auto", "bfloat16", "float16", "float32"],
        help="æ•°æ®ç±»å‹",
    )
    
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_lora_model(
        base_model_path=args.base_model,
        lora_adapter_path=args.lora_adapter,
        device=args.device,
        torch_dtype=args.dtype,
    )
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œ
    if args.mode == "interactive":
        interactive_mode(model, tokenizer)
    
    elif args.mode == "single":
        if not args.prompt:
            print("âŒ single æ¨¡å¼éœ€è¦æä¾› --prompt å‚æ•°")
            return 1
        
        print(f"\nğŸ“ æç¤º: {args.prompt}")
        print("\nğŸ”„ ç”Ÿæˆä¸­...")
        
        generated = generate_code(
            model, tokenizer, args.prompt,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
        )
        
        print("\n" + "â”€"*60)
        print("ğŸ“„ ç”Ÿæˆç»“æœ:")
        print("â”€"*60)
        print(generated)
        print("â”€"*60)
    
    elif args.mode == "batch":
        if not args.prompts_file:
            print("âŒ batch æ¨¡å¼éœ€è¦æä¾› --prompts_file å‚æ•°")
            return 1
        
        batch_inference(model, tokenizer, args.prompts_file, args.output_file)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

