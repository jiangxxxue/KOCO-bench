{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc27cb50-ff49-4c94-a569-af53a5ee6bcf",
   "metadata": {},
   "source": [
    "# Tutorial 4: DPO vs. GRPO vs. Supervised - Choosing Your Algorithm\n",
    "\n",
    "ToolBrain offers a flexible selection of learning algorithms to fine-tune your agent. The three primary methods are **GRPO**, **DPO**, and **Supervised** learning. Understanding the differences between them and knowing when to use each is key to effective training.\n",
    "\n",
    "This tutorial provides a conceptual overview of each algorithm and shows you how to switch between them.\n",
    "\n",
    "## 1. Supervised Fine-Tuning (SFT)\n",
    "\n",
    "**What it is:** Supervised learning is the most traditional form of fine-tuning. You provide the model with a dataset of high-quality prompt-and-response pairs, and the model learns to imitate those responses. \n",
    "\n",
    "**How it works in ToolBrain:** You provide a dataset where each example is a list of `ChatSegment` objects, representing a multi-turn conversation. The model learns to generate the `assistant`'s response given the preceding conversation history.\n",
    "\n",
    "```python\n",
    "# From examples/05_supervised_training.py\n",
    "\n",
    "# The dataset is a list of conversations\n",
    "training_datasets: List[List[ChatSegment]] = [\n",
    "    [\n",
    "         ChatSegment(\n",
    "                role=\"other\",\n",
    "                text=\"You are a Python assistant. Compute the sum of 1..10 and explain briefly.\",\n",
    "            ),\n",
    "            ChatSegment(\n",
    "                role=\"assistant\",\n",
    "                text=\"The sum of numbers from 1 to 10 is 55, using the formula n(n+1)/2.\"\n",
    "            ),\n",
    "    ],\n",
    "]\n",
    "\n",
    "# The Brain is configured for Supervised learning\n",
    "brain = Brain(\n",
    "    agent=my_agent,\n",
    "    algorithm=\"Supervised\"\n",
    ")\n",
    "\n",
    "brain.train(training_datasets, num_iterations=5)\n",
    "```\n",
    "\n",
    "**When to use it:**\n",
    "- To teach the model a specific style, persona, or format.\n",
    "- To instill factual knowledge or canned responses.\n",
    "- As a pre-training step before RL to get the model into a reasonable starting state.\n",
    "\n",
    "**Limitations:** It doesn't actively explore or learn from its own mistakes; it only mimics the data it's given.\n",
    "\n",
    "## 2. GRPO (Group Relative Reward Policy Optimization)\n",
    "\n",
    "**What it is:** GRPO is ToolBrain's default and recommended Reinforcement Learning (RL) algorithm. Instead of just showing the model the single \"correct\" answer, GRPO lets the agent explore. It tries to solve a problem multiple times, generating a \"group\" of different traces.\n",
    "\n",
    "**How it works:**\n",
    "1.  For a single query, the agent generates `num_group_members` (e.g., 2 or 4) different execution traces.\n",
    "2.  The reward function scores each of these traces independently.\n",
    "3.  The GRPO algorithm then updates the model, teaching it to increase the probability of generating traces that received higher rewards and decrease the probability of those with lower rewards.\n",
    "\n",
    "```python\n",
    "# From examples/01_run_hello_world.py\n",
    "\n",
    "brain = Brain(\n",
    "    agent,\n",
    "    algorithm=\"GRPO\",                # Explicitly choosing GRPO\n",
    "    reward_func=reward_exact_match,\n",
    "    num_group_members=2              # Collect 2 traces per step\n",
    ")\n",
    "\n",
    "brain.train(training_dataset, num_iterations=10)\n",
    "```\n",
    "\n",
    "**When to use it:**\n",
    "- As your primary algorithm for teaching complex, multi-step tool use.\n",
    "- When the goal is to explore a solution space to find the best approach (e.g., hyperparameter optimization).\n",
    "- When you have a clear, quantitative reward signal (e.g., accuracy, F1 score, or a custom metric).\n",
    "\n",
    "## 3. DPO (Direct Preference Optimization)\n",
    "\n",
    "**What it is:** DPO is another powerful RL algorithm that learns from *preferences*. Instead of scoring each trace on an absolute scale, DPO learns from pairs of traces: one that is \"chosen\" (preferred) and one that is \"rejected\" (not preferred).\n",
    "\n",
    "**How it works:**\n",
    "1.  Like GRPO, the agent generates multiple traces for a query.\n",
    "2.  The reward function scores each trace.\n",
    "3.  ToolBrain automatically creates pairs of `(chosen, rejected)` traces, where the chosen trace has a higher reward than the rejected one.\n",
    "4.  The DPO algorithm updates the model to increase the likelihood of generating the chosen trace relative to the rejected one.\n",
    "\n",
    "```python\n",
    "# From examples/04_lightgbm_hpo_training_with_dpo/run_hpo_training.py\n",
    "\n",
    "brain = Brain(\n",
    "    agent=my_agent,\n",
    "    reward_func=reward_accuracy,\n",
    "    algorithm=\"DPO\",\n",
    "    num_group_members=4 # DPO needs at least 2, more is often better\n",
    ")\n",
    "\n",
    "brain.train(training_dataset, num_iterations=10)\n",
    "```\n",
    "\n",
    "**When to use it:**\n",
    "- When it's easier to say \"this is better than that\" than it is to assign a precise numerical score.\n",
    "- When your reward signal is noisy or relative.\n",
    "- It is often very effective for improving response quality based on human or LLM-as-a-Judge feedback.\n",
    "\n",
    "## How to Switch Algorithms\n",
    "\n",
    "Switching between algorithms is as simple as changing one parameter in the `Brain` constructor:\n",
    "\n",
    "```python\n",
    "# To use GRPO\n",
    "brain = Brain(agent, algorithm=\"GRPO\", ...)\n",
    "\n",
    "# To use DPO\n",
    "brain = Brain(agent, algorithm=\"DPO\", ...)\n",
    "\n",
    "# To use Supervised Fine-Tuning\n",
    "brain = Brain(agent, algorithm=\"Supervised\", ...)\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Algorithm  | Learning Style          | Best For                                                                 |\n",
    "|------------|-------------------------|--------------------------------------------------------------------------|\n",
    "| **Supervised** | Imitation               | Learning style, facts, and pre-training.                                 |\n",
    "| **GRPO**     | Exploration & Scoring   | Complex tool use, exploration tasks, and optimizing quantitative metrics. |\n",
    "| **DPO**      | Preference & Comparison | Tasks where relative quality is more important than an absolute score.     |\n",
    "\n",
    "Start with **GRPO** for most tool-learning tasks. If your reward signal is more about preference, or if GRPO isn't yielding the desired results, try **DPO**. Use **Supervised** learning for foundational training or when you have a dataset of perfect examples to imitate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b0d94-415c-435c-92c0-36c5ae2e10e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
