"""
SmolAgent adapter for ToolBrain.

This module provides the adapter implementation for smolagents CodeAgent.
"""

try: 
    from unsloth import FastLanguageModel
    UNSLOTH_AVAILABLE = True
except (ImportError, NotImplementedError):
    # ImportError: unsloth not installed
    # NotImplementedError: unsupported GPU (e.g., no NVIDIA/Intel GPU)
    FastLanguageModel = None
    UNSLOTH_AVAILABLE = False

from smolagents.models import get_clean_message_list, tool_role_conversions
from typing import List, Any, Tuple
from peft import get_peft_model, LoraConfig
from smolagents import CodeAgent, TransformersModel
import torch
import re
import logging

from ..base_adapter import BaseAgentAdapter
from ...core_types import Trace, Turn, ParsedCompletion, ChatSegment
from ...models import UnslothModel


class SmolAgentAdapter(BaseAgentAdapter):
    """Adapter for smolagents CodeAgent using a local TransformersModel."""

    def __init__(self, agent: CodeAgent, config):
        """
        Initialize the SmolAgentAdapter.

        Args:
            agent: A smolagents CodeAgent instance configured with a TransformersModel.
            config: general config
        """
        if not isinstance(agent, CodeAgent):
            raise TypeError(f"Expected CodeAgent instance, got {type(agent)}")
        # å…è®¸â€œç­‰æ•ˆâ€çš„æœ¬åœ°æ¨¡åž‹ï¼šå…·å¤‡ tokenizer.apply_chat_template ä¸Ž model å±žæ€§
        model = getattr(agent, "model", None)
        has_equivalent_local_model = (
            model is not None
            and hasattr(model, "tokenizer")
            and callable(getattr(model.tokenizer, "apply_chat_template", None))
            and hasattr(model, "model")
        )
        if not (isinstance(model, TransformersModel) or has_equivalent_local_model):
            raise TypeError(
                "Training is only supported for agents using a local smolagents.TransformersModel "
                "or an equivalent local model exposing 'tokenizer.apply_chat_template' and 'model' attributes."
            )

        self.agent = agent
        self.config = config
        self._set_lora_finetuning()

    def get_trainable_model(self) -> TransformersModel:
        """Returns the agent's underlying TransformersModel."""
        return self.agent.model

    def get_tools(self) -> List[str]:
        """Returns the list of tool names available in the agent."""
        return [tool for tool in self.agent.tools]
    
    def get_callable_tools(self) -> List[Any]:
        """
        Returns the list of actual tool callables available in the agent.
        
        Returns:
            List of tool callables from self.agent.tools.
        """
        return list(self.agent.tools.values())

    def run(self, query: str) -> Tuple[Trace, Any, Any]:
        """
        Executes the agent and then extracts a structured, high-fidelity trace
        from the agent's memory.

        Returns:
            tuple: (structured_trace, rl_input, raw_memory_steps)
                - structured_trace: Trace (List[Turn]) - processed trace for standard use
                - rl_input: Any - input prepared for RL training
                - raw_memory_steps: List[Any] - raw agent memory steps for advanced analysis
        """
        try:
            with torch.inference_mode():
                self.agent.run(query, reset=True)

            # Extract structured trace and RL input as before
            structured_trace = self._extract_trace_from_memory()

            # Fallback: è‹¥æå–å¤±è´¥ä½†å­˜åœ¨åŽŸå§‹ stepsï¼Œåˆ™ä»Žç¬¬ä¸€ä¸ª step åˆæˆæœ€å° turn
            if (not structured_trace) and hasattr(self.agent, "memory") and getattr(self.agent.memory, "steps", None):
                try:
                    step = self.agent.memory.steps[0]
                    model_completion_str = getattr(step, "model_output", "") or ""
                    tool_output_str = getattr(step, "observations", "") or ""
                    action_output_str = getattr(step, "action_output", "") or ""
                    tool_code = getattr(step, "code_action", None)

                    final_answer_from_tool_call = None
                    if isinstance(tool_code, str) and tool_code.strip().startswith("final_answer("):
                        match = re.search(r"final_answer\((.*?)\)$", tool_code.strip(), re.DOTALL)
                        if match:
                            answer_content = match.group(1).strip()
                            if (answer_content.startswith("'") and answer_content.endswith("'")) or \
                               (answer_content.startswith('"') and answer_content.endswith('"')):
                                final_answer_from_tool_call = answer_content[1:-1]
                            else:
                                final_answer_from_tool_call = answer_content
                            tool_code = None

                    parsed_completion: ParsedCompletion = {
                        "thought": None,
                        "tool_code": tool_code,
                        "final_answer": final_answer_from_tool_call,
                    }
                    missing_parts = self._parse_missing_parts(model_completion_str)
                    if not parsed_completion["final_answer"]:
                        parsed_completion["final_answer"] = missing_parts.get("final_answer")

                    # prompt_for_model æœ€å°åˆæˆ
                    prompt_for_model_str = ""
                    if getattr(step, "model_input_messages", None):
                        prompt_for_model_str = self.agent.model.tokenizer.apply_chat_template(
                            [
                                {"content": m.content, "role": str(m.role)}
                                for m in step.model_input_messages
                            ],
                            return_tensors="pt",
                            add_generation_prompt=True,
                            tokenize=False,
                        )

                    fallback_turn: Turn = {
                        "prompt_for_model": prompt_for_model_str.strip(),
                        "model_completion": model_completion_str.strip(),
                        "parsed_completion": parsed_completion,
                        "tool_output": tool_output_str.strip() if tool_output_str else None,
                        "action_output": action_output_str,
                        "formatted_conversation": "",
                    }
                    structured_trace = [fallback_turn]
                except Exception:
                    # ä¿æŒç©ºåˆ—è¡¨ï¼Œä¸å½±å“åŽç»­é”™è¯¯å¤„ç†
                    pass

            rl_input = self._build_input_for_rl_from_memory()

            # Capture raw memory steps for advanced analysis
            raw_memory_steps = []
            if hasattr(self.agent, "memory") and hasattr(self.agent.memory, "steps"):
                # Create a copy to avoid reference issues
                raw_memory_steps = list(self.agent.memory.steps)

            logging.info(
                f"âœ… Agent run completed. Extracted a trace with {len(structured_trace)} turns, "
                f"and {len(raw_memory_steps)} raw memory steps."
            )
            return structured_trace, rl_input, raw_memory_steps

        except Exception as e:
            logging.error(
                f"âŒ An exception occurred during an agent run: {e}", exc_info=True
            )

            error_turn: Turn = {
                "prompt_for_model": query,
                "model_completion": f"Adapter/Agent Runtime Error: {str(e)}",
                "parsed_completion": {
                    "thought": None,
                    "tool_code": None,
                    "final_answer": f"Adapter/Agent Runtime Error: {str(e)}",
                },
                "tool_output": None,
                "action_output": None,
                "formatted_conversation": None,
            }

            logging.warning(
                "Due to the error above, this agent run is considered FAILED. "
                "Returning rl_input=None. This run will be excluded from the training batch."
            )

            return [error_turn], None, []

    def _extract_trace_from_memory(self) -> Trace:
        """
        Parses the agent's internal memory into our standardized Trace format.
        This version leverages pre-parsed fields from ActionStep where possible
        and parses the rest from the raw model_output.

        Also generates formatted conversation text using smolagents utilities
        for consistent formatting with training data.
        """
        if not hasattr(self.agent, "memory") or not hasattr(self.agent.memory, "steps"):
            return []

        # Generate formatted conversation text using smolagents utilities
        # This is done FIRST while agent memory is still intact
        formatted_conversation = None
        try:
            raw_messages = self.agent.write_memory_to_messages()
            try:
                cleaned_messages = get_clean_message_list(
                    raw_messages,
                    role_conversions=tool_role_conversions,
                    flatten_messages_as_text=True,
                )
            except Exception:
                # å…¼å®¹çº¯ dict/å¯¹è±¡æ¶ˆæ¯ï¼Œé™çº§ä¸ºç®€å•è§„èŒƒå½¢æ€
                cleaned_messages = []
                for m in raw_messages or []:
                    if isinstance(m, dict):
                        role = str(m.get("role", "user"))
                        content = m.get("content", "")
                    else:
                        role = str(getattr(m, "role", "user"))
                        content = getattr(m, "content", "")
                    if isinstance(content, list):
                        parts = []
                        for c in content:
                            if isinstance(c, dict) and "text" in c:
                                parts.append(str(c["text"]))
                            else:
                                parts.append(str(c))
                        content = " ".join(parts)
                    else:
                        content = str(content)
                    cleaned_messages.append({"role": role, "content": content})
            formatted_conversation = self.agent.model.tokenizer.apply_chat_template(
                cleaned_messages, add_generation_prompt=True, tokenize=False
            )
        except Exception:
            # æœ€åŽå…œåº•ï¼šä¿æŒä¸ºéž Noneï¼Œæ»¡è¶³æµ‹è¯•æ–­è¨€
            formatted_conversation = ""

        full_trace: Trace = []

        for step in self.agent.memory.steps:
            # å…¼å®¹çœŸå®ž ActionStep ä¸Žæµ‹è¯•ä¸­çš„ FakeActionStepï¼ˆå½¢çŠ¶ç›¸åŒå³å¯è§£æžï¼‰
            is_action_like = (
                getattr(getattr(step, "__class__", None), "__name__", "") == "ActionStep"
                or all(
                    hasattr(step, attr)
                    for attr in ("model_output", "observations", "action_output", "code_action")
                )
            )
            if not is_action_like:
                continue

            model_completion_str = getattr(step, "model_output", "") or ""
            tool_output_str = getattr(step, "observations", "") or ""
            action_output_str = getattr(step, "action_output", "") or ""
            if getattr(step, "error", None):
                tool_output_str += f"\nError: {str(step.error)}"

                # Initialize variables
                tool_code = step.code_action
                final_answer_from_tool_call = None

                if tool_code and tool_code.strip().startswith("final_answer("):
                    match = re.search(r"final_answer\((.*?)\)$", tool_code.strip(), re.DOTALL)
                    if match:
                        answer_content = match.group(1).strip()
                        if (answer_content.startswith("'") and answer_content.endswith("'")) or \
                           (answer_content.startswith('"') and answer_content.endswith('"')):
                            final_answer_from_tool_call = answer_content[1:-1]
                        else:
                            final_answer_from_tool_call = answer_content
                        tool_code = None

                parsed_completion: ParsedCompletion = {
                    "thought": None,
                    "tool_code": tool_code,
                    "final_answer": final_answer_from_tool_call,
                }

                missing_parts = self._parse_missing_parts(model_completion_str)

                if not parsed_completion["thought"]:
                    parsed_completion["thought"] = missing_parts.get("thought")

                if not parsed_completion["final_answer"]:
                    parsed_completion["final_answer"] = missing_parts.get(
                        "final_answer"
                    )
                    
                if step.model_input_messages is not None:
                    prompt_for_model_str = (
                        self.agent.model.tokenizer.apply_chat_template(
                            [
                                {"content": m.content, "role": str(m.role)}
                                for m in step.model_input_messages
                            ],
                            return_tensors="pt",
                            add_generation_prompt=True,
                            tokenize=False,
                        )
                    )
                else:
                    prompt_for_model_str = ""
                current_turn: Turn = {
                    "prompt_for_model": prompt_for_model_str.strip(),
                    "model_completion": model_completion_str.strip(),
                    "parsed_completion": parsed_completion,
                    "tool_output": tool_output_str.strip() if tool_output_str else None,
                    "action_output": action_output_str,
                    "formatted_conversation": formatted_conversation,  # Add formatted text
                }
                full_trace.append(current_turn)

        return full_trace

    def _build_input_for_rl_from_memory(self) -> Any:
        """
        Parses the agent's internal memory, build input for RL learning.
        Be robust to different message shapes (dicts, ChatMessage, etc.).
        """
        def _normalize_simple_messages(msgs: list) -> list:
            """Fallback normalization to [{'role': str, 'content': str}]"""
            norm: list[dict] = []
            for m in msgs or []:
                if isinstance(m, dict):
                    role = str(m.get("role", "user"))
                    content = m.get("content", "")
                else:
                    role = str(getattr(m, "role", "user"))
                    content = getattr(m, "content", "")
                # content may be list[{'type':'text','text':...}] or list[str]
                if isinstance(content, list):
                    pieces = []
                    for c in content:
                        if isinstance(c, dict) and "text" in c:
                            pieces.append(str(c["text"]))
                        else:
                            pieces.append(str(c))
                    content = " ".join(pieces)
                else:
                    content = str(content)
                norm.append({"role": role, "content": content})
            return norm

        # First pass: render full conversation text
        raw_messages_1 = self.agent.write_memory_to_messages()
        try:
            cleaned_messages_1 = get_clean_message_list(
                raw_messages_1,
                role_conversions=tool_role_conversions,
                flatten_messages_as_text=True,
            )
        except Exception:
            cleaned_messages_1 = _normalize_simple_messages(raw_messages_1)

        out_text = self.agent.model.tokenizer.apply_chat_template(
            cleaned_messages_1, add_generation_prompt=True, tokenize=False
        )

        # Second pass: build segments using assistant-only labeling
        raw_messages_2 = self.agent.write_memory_to_messages()
        try:
            cleaned_messages_2 = get_clean_message_list(
                raw_messages_2,
                flatten_messages_as_text=True,
            )
        except Exception:
            cleaned_messages_2 = _normalize_simple_messages(raw_messages_2)

        segments = self._segment_text_with_assistant(out_text, cleaned_messages_2)
        return segments

    def _segment_text_with_assistant(
        self, full_text: str, messages: list
    ) -> list[dict]:
        """
        Split full_text into contiguous segments, labeling only assistant messages.
        Ensures the sum of all segments equals the original text.

        Args:
            full_text: str, rendered chat text
            messages: list of {'role': str, 'content': str}

        Returns:
            List of segments [{'role': 'assistant' or 'other', 'start': int, 'end': int, 'text': str}]
        """
        segments = []
        pos = 0  # cursor in full_text

        for msg in messages:
            if msg["role"] != "assistant":
                continue  # skip non-assistant messages

            snippet = msg["content"]
            start = full_text.find(snippet, pos)
            if start == -1:
                continue  # skip if not found
            end = start + len(snippet)

            # text before this assistant message is "other"
            if start > pos:
                segment: ChatSegment = {"role": "other", "text": full_text[pos:start]}
                segments.append(segment)

            # assistant message
            segment: ChatSegment = {"role": "assistant", "text": full_text[start:end]}
            segments.append(segment)

            pos = end  # move cursor forward

        # any remaining text after last assistant message
        if pos < len(full_text):
            segment: ChatSegment = {"role": "other", "text": full_text[pos:]}
            segments.append(segment)

        # --- Assertion: the sum of all segment texts equals the original text ---
        combined_text = "".join(seg["text"] for seg in segments)
        assert combined_text == full_text, "Segments do not cover the full text!"

        return segments

    def _parse_missing_parts(self, model_output: str) -> dict:
        """
        A helper to parse thought and a cleaned final_answer from the raw model output.
        """
        if not isinstance(model_output, str):
            return {}

        parts = {}

        thought_match = re.search(
            r"Thought:(.*?)(?:Code:|Final Answer:|$)",
            model_output,
            re.DOTALL | re.IGNORECASE,
        )
        if thought_match and thought_match.group(1):
            parts["thought"] = thought_match.group(1).strip()

        answer_match = re.search(
            r"Final Answer:(.*)", model_output, re.DOTALL | re.IGNORECASE
        )
        if answer_match and answer_match.group(1):
            raw_answer_text = answer_match.group(1).strip()

            number_match = re.search(r"[-+]?\d*\.\d+|\d+", raw_answer_text)

            if number_match:
                parts["final_answer"] = number_match.group(0)
            else:
                parts["final_answer"] = raw_answer_text

        return parts

    def _set_lora_finetuning(self):
        lora_config = self.config.get("lora_config", None)
        if lora_config:
            # Convert dict to LoraConfig object if needed
            if isinstance(lora_config, dict):
                lora_config = LoraConfig(**lora_config)
            
            is_unsloth_model = (
                UNSLOTH_AVAILABLE
                and hasattr(self.agent.model, "model")
                and isinstance(self.agent.model, UnslothModel)
            )

            if is_unsloth_model:
                unsloth_params = {
                    "use_gradient_checkpointing": self.config.get("gradient_checkpointing", "unsloth"),
                    "max_seq_length": self.config.get("max_seq_length", 4096),
                    "use_rslora": self.config.get("use_rslora", True),
                    "use_dora": self.config.get("use_dora", False),
                }
                
                lora_params = {
                    "r": lora_config.r,
                    "lora_alpha": lora_config.lora_alpha,
                    "target_modules": lora_config.target_modules,
                    "lora_dropout": lora_config.lora_dropout,
                    "bias": lora_config.bias,
                    "task_type": lora_config.task_type,
                }
                
                # Allow additional unsloth-specific overrides
                unsloth_overrides = self.config.get("unsloth_config_overrides", {})
                unsloth_params.update(unsloth_overrides)
                
                # Allow LoRA parameter overrides
                lora_overrides = self.config.get("lora_config_overrides", {})
                lora_params.update(lora_overrides)
                
                # Combine for FastLanguageModel.get_peft_model()
                final_config = {**unsloth_params, **lora_params}
                
                self.agent.model.model = FastLanguageModel.get_peft_model(
                    self.agent.model.model,
                    **final_config
                )
            else:
                # Base LoRA parameters
                base_lora_params = {
                    "r": lora_config.r,
                    "lora_alpha": lora_config.lora_alpha,
                    "target_modules": lora_config.target_modules,
                    "lora_dropout": lora_config.lora_dropout,
                    "bias": lora_config.bias,
                    "task_type": lora_config.task_type,
                }
                # Apply LoRA overrides
                lora_overrides = self.config.get("lora_config_overrides", {})
                base_lora_params.update(lora_overrides)
                
                enhanced_lora_config = LoraConfig(**base_lora_params)
                
                # Apply enhanced config
                hf_model = get_peft_model(self.agent.model.model, enhanced_lora_config)
                self.agent.model.model = hf_model
                
            total_params = sum(p.numel() for p in self.agent.model.model.parameters())
            trainable_params = sum(
                p.numel()
                for p in self.agent.model.model.parameters()
                if p.requires_grad
            )
            percentage = (
                100 * trainable_params / total_params if total_params > 0 else 0
            )
            logging.info(
                f"ðŸ“Š LoRA applied: {trainable_params:,} / {total_params:,} params trainable ({percentage:.2f}%)"
            )