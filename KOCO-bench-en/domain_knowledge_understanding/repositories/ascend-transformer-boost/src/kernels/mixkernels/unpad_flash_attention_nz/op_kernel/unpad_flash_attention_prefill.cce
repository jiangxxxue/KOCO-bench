/*
* Copyright (c) 2024 Huawei Technologies Co., Ltd.
* This file is a part of the CANN Open Software.
* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/
#include "kernels/utils/kernel/common_func.h"
#include "kernels/utils/kernel/simd.h"
#include "kernels/utils/kernel/iterator.h"
#include "kernels/utils/kernel/mma.h"
#include "kernels/utils/kernel/utils.h"
#include "kernel_operator.h"
#include "unpad_flash_attention_common.h"

template <typename T, typename SType, PrecType prec_type1, PrecType prec_type2>
__aicore__ inline void UnpadFlashAttentionCommon<T, SType, prec_type1, prec_type2>::RowSum(
   const int32_t __n0, const int32_t fm, int32_t Pingflag)
{
    if (__n0 / BLOCK_SIZE > 1) {
        add_v<ArchType::ASCEND_V200, T>(tvUbuf_tensor.ReinterpretCast<T>(),
                                            ls32Ubuf_tensor,
                                            ls32Ubuf_tensor[fm * BLOCK_SIZE],
                                            fm * BLOCK_SIZE * sizeof(T) / 256, // repeat
                                            1,                                   // dstBlockStride
                                            1,                                   // src0BlockStride
                                            1,                                   // src1BlockStride
                                            8,                                   // dstRepeatStride
                                            8,                                   // src0RepeatStride
                                            8                                    // src1RepeatStride
        );
        PIPE_BARRIER(V);
    } else {
        ub_to_ub<ArchType::ASCEND_V200, T>(tvUbuf_tensor.ReinterpretCast<T>(),
                                            ls32Ubuf_tensor,
                                            0,                   // sid
                                            1,                   // nBurst
                                            fm * BLOCK_SIZE * sizeof(T) / 32, // lenBurst
                                            0,                   // srcGap
                                            0                    // dstGap
        );
        PIPE_BARRIER(V);
    }
    for (int32_t rowsumIdx = 2; rowsumIdx < (__n0 / BLOCK_SIZE); ++rowsumIdx) {
        add_v<ArchType::ASCEND_V200, T>(tvUbuf_tensor.ReinterpretCast<T>(),
                                        tvUbuf_tensor.ReinterpretCast<T>(),
                                        ls32Ubuf_tensor[rowsumIdx * fm * BLOCK_SIZE],
                                        fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                        1,                                   // dstBlockStride
                                        1,                                   // src0BlockStride
                                        1,                                   // src1BlockStride
                                        8,                                   // dstRepeatStride
                                        8,                                   // src0RepeatStride
                                        8                                    // src1RepeatStride
        );
        PIPE_BARRIER(V);
    }
    SetMasknorm();
    if (__n0 % BLOCK_SIZE > 0) {
        __set_mask(__n0 % BLOCK_SIZE);
        if (__n0 / BLOCK_SIZE > 0) {
            add_v<ArchType::ASCEND_V200, T>(tvUbuf_tensor.ReinterpretCast<T>(),
                                            tvUbuf_tensor.ReinterpretCast<T>(),
                                            ls32Ubuf_tensor[__n0 / BLOCK_SIZE * fm * BLOCK_SIZE],
                                            fm, // repeat
                                            1,  // dstBlockStride
                                            1,  // src0BlockStride
                                            1,  // src1BlockStride
                                            sizeof(T) / 2,  // dstRepeatStride
                                            sizeof(T) / 2,  // src0RepeatStride
                                            sizeof(T) / 2   // src1RepeatStride
            );
            PIPE_BARRIER(V);
            SetVectorMask<int8_t>(0x0, 0xffff);
        }
    } else {
        SetVectorMask<int8_t>(0x0, 0xffff);
    }

    cadd_v<ArchType::ASCEND_V200, T>(llUbuf_tensor[Pingflag * UB_FLOAT_LINE_SIZE],
                                    tvUbuf_tensor.ReinterpretCast<T>(), fm,
                                    1,  // dstRepeatStride
                                    1,  // srcBlockStride
                                    sizeof(T) / 2); // srcRepeatStride, fp32 2 block
    PIPE_BARRIER(V);
    // Must Sync, otherwise error
    SET_FLAG(V, MTE1, EVENT_ID0);
    SetMasknorm();
    SetVectorMask<int8_t>(0xffffffffffffffff, 0xffffffffffffffff);
}

template <typename T, typename SType, PrecType prec_type1, PrecType prec_type2>
__aicore__ inline void UnpadFlashAttentionCommon<T, SType, prec_type1, prec_type2>::SoftmaxUpdate(
    int32_t fm, int32_t fk, int32_t oSize, int32_t Pingflag, int32_t initGgO, int32_t mD64)
{
    if (cubeUpdateO == 0 && initGgO == 0) { // 需要更新O
        if constexpr (prec_type2 == PrecType::BMM1_FP16_EXP_FP32) {
            conv_v<ArchType::ASCEND_V200, half, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                                    dmUbuf_tensor[Pingflag * UB_HALF_LINE_SIZE], mD64, 1, 1, uint16_t(8),
                                                    uint16_t(4));
        } else if constexpr (prec_type2 == PrecType::BMM1_FP32_EXP_FP32) {
            ub_to_ub<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                                dmUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_LINE_SIZE], // 256
                                                0, 1, fm / 8, 0, 0);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, half>(tvUbuf_tensor,
                                                dmUbuf_tensor[Pingflag * UB_HALF_LINE_SIZE],
                                                0, 1, fm / 16, 0, 0);
        }
        PIPE_BARRIER(V);
        UpdateExp<T, prec_type2>(tvUbuf_tensor.ReinterpretCast<float>(), mD64);
        PIPE_BARRIER(V);
        mul_v<ArchType::ASCEND_V200, T>(glUbuf_tensor, tvUbuf_tensor.ReinterpretCast<T>(), glUbuf_tensor,
                                            mD64, // repeat
                                            1,    // dstBlockStride
                                            1,    // src0BlockStride
                                            1,    // src1BlockStride
                                            8,    // dstRepeatStride
                                            8,    // src0RepeatStride
                                            8     // src1RepeatStride
        );
        PIPE_BARRIER(V);
        add_v<ArchType::ASCEND_V200, T>(glUbuf_tensor, glUbuf_tensor, llUbuf_tensor[Pingflag * UB_FLOAT_LINE_SIZE],
                                            mD64, // repeat
                                            1,    // dstBlockStride
                                            1,    // src0BlockStride
                                            1,    // src1BlockStride
                                            8,    // dstRepeatStride
                                            8,    // src0RepeatStride
                                            8     // src1RepeatStride
        );

        PIPE_BARRIER(V);
        if constexpr (prec_type2 == PrecType::BMM1_FP32_EXP_FP32) {
            ExpandToBlockFloat(tvUbuf_tensor.ReinterpretCast<float>(),
                                dmUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_LINE_SIZE],
                                fm);
        } else {
            ExpandToBlockHalf(tvUbuf_tensor, // broadcast(m_j-1 - m_j)
                          dmUbuf_tensor[Pingflag * UB_HALF_LINE_SIZE], fm);
        }
        

        if constexpr (prec_type2 == PrecType::BMM1_FP16_EXP_FP32) {
            conv_v<ArchType::ASCEND_V200, half, float>(tvUbuf_tensor.ReinterpretCast<float>()[fm * BLOCK_SIZE / 2],
                                                   tvUbuf_tensor,
                                                   fm * BLOCK_SIZE / 64, // repeat
                                                   1, 1, uint16_t(8), uint16_t(4));
        } else if constexpr (prec_type2 == PrecType::BMM1_FP32_EXP_FP32) {
            ub_to_ub<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>()[fm * BLOCK_SIZE],
                                                tvUbuf_tensor.ReinterpretCast<float>(),
                                                0, 1, fm * BLOCK_SIZE / 8, 0, 0);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, half>(tvUbuf_tensor[fm * BLOCK_SIZE],
                                                  tvUbuf_tensor,
                                                  0, 1, fm * BLOCK_SIZE / 16, 0, 0);
        }

        PIPE_BARRIER(V);
        UpdateExp<T, prec_type2>(tvUbuf_tensor.ReinterpretCast<T>()[fm * BLOCK_SIZE / 4 * sizeof(SType)],
                                                fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE);
        PIPE_BARRIER(V);

        if (vmPingpongFlag == 1) {
            WAIT_FLAG(MTE3, V, EVENT_ID2);
            vmPingpongFlag = 0;
        }

        for (int32_t vmulIdx = 0; vmulIdx < (fk / BLOCK_SIZE); ++vmulIdx) { // e^broadcast(m_j-1 - m_j) * Oj_1
            mul_v<ArchType::ASCEND_V200, T>(goUbuf_tensor[vmulIdx * fm * BLOCK_SIZE],
                                            goUbuf_tensor[vmulIdx * fm * BLOCK_SIZE],
                                            tvUbuf_tensor.ReinterpretCast<T>()[fm * BLOCK_SIZE / 4 * sizeof(SType)],
                                            fm * BLOCK_SIZE * sizeof(T) / 256, // repeat
                                            1,                                   // dstBlockStride
                                            1,                                   // src0BlockStride
                                            1,                                   // src1BlockStride
                                            8,                                   // dstRepeatStride
                                            8,                                   // src0RepeatStride
                                            8                                    // src1RepeatStride
            );
        }
        PIPE_BARRIER(V);

        // 2 for double buffer
        for (int32_t vaddIdx = 0; vaddIdx < 2; ++vaddIdx) { // update Oj
            add_v<ArchType::ASCEND_V200, T>(goUbuf_tensor[vaddIdx * oSize / 2], goUbuf_tensor[vaddIdx * oSize / 2],
                                            loUbuf_tensor.ReinterpretCast<T>()[vaddIdx * oSize / 2],
                                            (oSize * sizeof(T)) / 8 / FLOAT_VECTOR_SIZE, // repeat
                                            1,                             // dstBlockStride
                                            1,                             // src0BlockStride
                                            1,                             // src1BlockStride
                                            8,                             // dstRepeatStride
                                            8,                             // src0RepeatStride
                                            8                              // src1RepeatStride
            );
        }
        PIPE_BARRIER(V);
    } else if (cubeUpdateO == 0) {
        ub_to_ub<ArchType::ASCEND_V200, T>(glUbuf_tensor, llUbuf_tensor[Pingflag * UB_FLOAT_LINE_SIZE],
                                               0,      // sid
                                               1,      // nBurst
                                               fm * sizeof(T) / 32, // lenBurst
                                               0,      // srcGap
                                               0       // dstGap
        );

        PIPE_BARRIER(V);
        if (vmPingpongFlag == 1) {
            WAIT_FLAG(MTE3, V, EVENT_ID2);
            vmPingpongFlag = 0;
        }
        ub_to_ub<ArchType::ASCEND_V200, T>(goUbuf_tensor,
                                           loUbuf_tensor.ReinterpretCast<T>(),
                                            0,         // sid
                                            1,         // nBurst
                                            oSize * sizeof(T) / 32, // lenBurst
                                            0,         // srcGap
                                            0          // dstGap
        );
        PIPE_BARRIER(V);
    }
    SET_FLAG(V, MTE1, EVENT_ID0);
}

template <typename T, typename SType, PrecType prec_type1, PrecType prec_type2>
__aicore__ inline void UnpadFlashAttentionCommon<T, SType, prec_type1, prec_type2>::UpdateOutput(
    int32_t fm, int32_t fk, int32_t oSize, int32_t mD64, int32_t __m0)
{
    if (wrapO == 1) {
        if constexpr(prec_type2 == PrecType::BMM1_FP16_EXP_FP32 || prec_type2 == PrecType::BMM1_FP32_EXP_FP32 
                    || prec_type2 == PrecType::BMM2_ONLINE_SOFTMAX_FP16) {
            conv_v<ArchType::ASCEND_V200, T, half>(glUbuf_tensor.template ReinterpretCast<half>(),
                                                       glUbuf_tensor,
                                                       mD64,        // repeat
                                                       1,           // dstBlockStride
                                                       1,           // srcBlockStride
                                                       uint16_t(4), // dstRepeatStride
                                                       uint16_t(8)  // srcRepeatStride
            );
            PIPE_BARRIER(V);
            for (int32_t vconvIdx = 0; vconvIdx < 2; ++vconvIdx) {
                conv_v<ArchType::ASCEND_V200, T, half>(goUbuf_tensor.template ReinterpretCast<half>()[vconvIdx * oSize / 2],
                                                        goUbuf_tensor[vconvIdx * oSize / 2],
                                                        oSize / 2 / FLOAT_VECTOR_SIZE, // repeat
                                                        1,                             // dstBlockStride
                                                        1,                             // srcBlockStride
                                                        uint16_t(4),                   // dstRepeatStride
                                                        uint16_t(8)                    // srcRepeatStride
                );
                PIPE_BARRIER(V);
            }
            ExpandToBlockHalf(tvUbuf_tensor, glUbuf_tensor.template ReinterpretCast<half>(), fm);
        }
        for (int32_t vdivIdx = 0; vdivIdx < (fk / BLOCK_SIZE); ++vdivIdx) { // Oi / li
            div_v<ArchType::ASCEND_V200, half>(goUbuf_tensor.template ReinterpretCast<half>()[vdivIdx * fm * BLOCK_SIZE],
                                               goUbuf_tensor.template ReinterpretCast<half>()[vdivIdx * fm * BLOCK_SIZE],
                                               tvUbuf_tensor,
                                               __m0 * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                               1,                               // dstBlockStride
                                               1,                               // src0BlockStride
                                               1,                               // src1BlockStride
                                               8,                               // dstRepeatStride
                                               8,                               // src0RepeatStride
                                               8                                // src1RepeatStride
            );

            PIPE_BARRIER(V);
        }
        int32_t blockV = VECTOR_SIZE / BLOCK_SIZE;
        if (__m0 % blockV != 0) {
            __set_mask(__m0 * BLOCK_SIZE % 128);
            div_v<ArchType::ASCEND_V200, half>(goUbuf_tensor.template ReinterpretCast<half>()[__m0 * BLOCK_SIZE / 128 * 128],
                                               goUbuf_tensor.template ReinterpretCast<half>()[__m0 * BLOCK_SIZE / 128 * 128],
                                               tvUbuf_tensor[__m0 / blockV * blockV * 16],
                                               fk / BLOCK_SIZE, // repeat
                                               1,               // dstBlockStride
                                               1,               // src0BlockStride
                                               1,               // src1BlockStride
                                               fm,              // dstRepeatStride
                                               fm,              // src0RepeatStride
                                               0                // src1RepeatStride
            );

            SetVectorMask<int8_t>(-1, -1);
        }
        PIPE_BARRIER(V);
        SET_FLAG(V, MTE3, EVENT_ID2);
        WAIT_FLAG(V, MTE3, EVENT_ID2);

        // move O to gm
        if (ntokensQ <= STRIDE_UPPER_BOUND + fm) {
            ub_to_gm<ArchType::ASCEND_V200, half>(gmDsto_tensor[(int64_t)dstoOffset],
                                                  goUbuf_tensor.template ReinterpretCast<half>(), 0,
                                                  fk / BLOCK_SIZE,  // nburst
                                                  __m0,             // burstlen
                                                  fm - __m0,        // strStride
                                                  ntokensQ - __m0); // dstStride
        } else {
            for (uint64_t gmBurstIdx = 0; gmBurstIdx < (fk / BLOCK_SIZE); ++gmBurstIdx) {
                ub_to_gm<ArchType::ASCEND_V200, half>(
                    gmDsto_tensor[(int64_t)dstoOffset + gmBurstIdx * ntokensQ * BLOCK_SIZE],
                    goUbuf_tensor.template ReinterpretCast<half>()[gmBurstIdx * fm * BLOCK_SIZE], 0, 1, __m0, 0, 0);
            }
        }
        if (vmPingpongFlag == 0) {
            SET_FLAG(MTE3, V, EVENT_ID2);
            vmPingpongFlag = 1;
        }
    }
}

template <>
__aicore__ inline void UnpadFlashAttentionCommon<float, float, PrecType::BMM1_FP32_EXP_FP32, PrecType::BMM1_FP32_EXP_FP32>::SoftMax(
                                                const int32_t fm, const int32_t fn, const int32_t fk, const int32_t bn,
                                                const int32_t __m0, const int32_t __n0, const int32_t __n1,
                                                const int32_t add_mask_n0, const int32_t add_mask_n1,
                                                const float alibi_coeff, const float delta0,
                                                const float delta1, const uint32_t scaleType,
                                                const uint32_t alibi_left_align, uint32_t initGgDm)
{
    int32_t Pingflag = 0;
    int32_t Pongflag = 1;
    int32_t pSize = fm * fn;
    int32_t pSize_b = fm * bn;
    int32_t mD64 = (fm + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE;
    // 3. ################ Softmax Ping Starts #######################
    WAIT_FLAG(M, V, Pingflag);
    WAIT_FLAG(MTE3, V, Pingflag);
    l0c_to_ub<ArchType::ASCEND_V200, float, float>(lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE],
                                                    l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 1,
                                                    pSize / CUBE_MATRIX_SIZE, 0, 0);
    
    SET_FLAG(V, M, Pingflag);
    PIPE_BARRIER(V);
 
    if (scaleType == 1) {
#if __CCE_AICORE__ == 100
        WAIT_FLAG(V, MTE2, EVENT_ID0);
#else
        WAIT_FLAG(V, MTE2, EVENT_ID6);
#endif
        AscendC::GlobalTensor<float> logn_gm_tensor;
        logn_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gmSrcLogn));
        gm_to_ub<ArchType::ASCEND_V200, float>(logn_ub_tensor.ReinterpretCast<float>(), logn_gm_tensor[lognOffset], 0, 1, fm / FLOAT_BLOCK_SIZE, 0, 0);

        SET_FLAG(MTE2, S, Pingflag + 2);
        WAIT_FLAG(MTE2, S, Pingflag + 2);
        // expand logn to block
        ExpandToBlockFloat(tvUbuf_tensor.ReinterpretCast<float>(),
                            logn_ub_tensor.ReinterpretCast<float>(),
                            fm);
        // loop for column, repeat for row
        for (uint32_t fn_block_idx = 0; fn_block_idx < (__n0 / BLOCK_SIZE); ++fn_block_idx) {
            mul_v<ArchType::ASCEND_V200, float>(
                lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE + fn_block_idx * fm * BLOCK_SIZE],
                lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE + fn_block_idx * fm * BLOCK_SIZE],
                tvUbuf_tensor.ReinterpretCast<float>(),
                fm * BLOCK_SIZE / 64,     // repeat
                1,                        // dstBlockStride
                1,                        // src0BlockStride
                0,                        // src1BlockStride
                8,  // dstRepeatStride
                8,  // src0RepeatStride
                1                         // src1RepeatStride
            );
            PIPE_BARRIER(V);
        }
        if (__n0 % BLOCK_SIZE > 0) {
            __set_mask(__n0 % BLOCK_SIZE);
            mul_v<ArchType::ASCEND_V200, float>(
                lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE + __n0 / BLOCK_SIZE * fm * BLOCK_SIZE],
                lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE + __n0 / BLOCK_SIZE * fm * BLOCK_SIZE],
                tvUbuf_tensor.ReinterpretCast<float>(),
                fm,                    // repeat
                1,                        // dstBlockStride
                1,                        // src0BlockStride
                0,                        // src1BlockStride
                2,  // dstRepeatStride
                2,  // src0RepeatStride
                1                         // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
#if __CCE_AICORE__ == 100
                SET_FLAG(V, MTE2, EVENT_ID0);
#else
                SET_FLAG(V, MTE2, EVENT_ID6);
#endif
    }
    // 3.1. mask(attention score * tor)
    muls_v<ArchType::ASCEND_V200, float>(lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE],
                                        lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE], tor,
                                        pSize / 64,             // repeat
                                        1,                       // dstBlockStride
                                        1,                       // srcBlockStride
                                        uint16_t(8), uint16_t(8) // dstRepeatStride
    );
    
    PIPE_BARRIER(V);
    // Must Sync, otherwise error
    WAIT_FLAG(V, MTE1, EVENT_ID0);
    // Mask Ping Load UB
    if ((gmSrcm != nullptr) && (add_mask_n0 == 1)) {
#if __CCE_AICORE__ != 100
        WAIT_FLAG(MTE2, MTE1, Pingflag + 2);
#endif
        l1_to_ub<ArchType::ASCEND_V200, half>(loUbuf_tensor.ReinterpretCast<half>(),
                                              l1maskBufAddr_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                              1,                    // nBurst, 次数
                                              fm * fn / BLOCK_SIZE, // lenBurst
                                              0,                    // srcStride
                                              0);
        SET_FLAG(MTE1, MTE2, Pingflag + 2);
        SET_FLAG(MTE1, V, Pingflag);
        WAIT_FLAG(MTE1, V, Pingflag);
        
        conv_v<ArchType::ASCEND_V200, half, float>(
            loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2],
            loUbuf_tensor.ReinterpretCast<half>(),
            pSize / FLOAT_VECTOR_SIZE, // repeat
            1,                             // dstBlockStride
            1,                             // srcBlockStride
            uint16_t(8),                   // dstRepeatStride
            uint16_t(4)                    // srcRepeatStride
        );
        PIPE_BARRIER(V);
        if (gmSrcAlibiCoeff != nullptr) {
            if (alibi_left_align == 1){
                adds_v<ArchType::ASCEND_V200, float>(
                    loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2],
                    loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2], (float)delta0,
                    fm * fn / FLOAT_VECTOR_SIZE, // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    8,                     // dstRepeatStride
                    8                      // src0RepeatStride
                );
                PIPE_BARRIER(V);
            }
            muls_v<ArchType::ASCEND_V200, float>(loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2],
                                                loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2],
                                                (float)alibi_coeff,
                                                fm * fn / FLOAT_VECTOR_SIZE, // repeat
                                                1,                     // dstBlockStride
                                                1,                     // src0BlockStride
                                                8,                     // dstRepeatStride
                                                8                      // src0RepeatStride
            );
            PIPE_BARRIER(V);
        }
        add_v<ArchType::ASCEND_V200, float>(lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE],
                                           lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE],
                                           loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2], // maskFP32
                                           fm * fn / FLOAT_VECTOR_SIZE, // repeat
                                           1,                     // dstBlockStride
                                           1,                     // src0BlockStride
                                           1,                     // src1BlockStride
                                           8,                     // dstRepeatStride
                                           8,                     // src0RepeatStride
                                           8                      // src1RepeatStride
        );
        PIPE_BARRIER(V);
    }
    PIPE_BARRIER(V);
    // 3. softmax part
    if (__n0 / BLOCK_SIZE > 1) { // 前两个(fm, 16)求最大值
        max_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE],
                                           lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE + fm * BLOCK_SIZE],
                                           fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                           1,                             // dstBlockStride
                                           1,                             // src0BlockStride
                                           1,                             // src1BlockStride
                                           8,                             // dstRepeatStride
                                           8,                             // src0RepeatStride
                                           8                              // src1RepeatStride
        );
        PIPE_BARRIER(V);
    } else {
        ub_to_ub<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), 
                                                lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE],
                                                0,  // sid
                                                1,  // nBurst
                                                fm * 16 / 8, // lenBurst
                                                0,  // srcGap
                                                0   // dstGap
        );

        PIPE_BARRIER(V);
    }
    for (int32_t rowmaxIdx = 2; rowmaxIdx < (__n0 / BLOCK_SIZE); ++rowmaxIdx) { // 循环比较(fm, 16)
        max_v<ArchType::ASCEND_V200, float>(
            tvUbuf_tensor.ReinterpretCast<float>(), 
            tvUbuf_tensor.ReinterpretCast<float>(),
            lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE + rowmaxIdx * fm * BLOCK_SIZE],
            fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
            1,                             // dstBlockStride
            1,                             // src0BlockStride
            1,                             // src1BlockStride
            8,                             // dstRepeatStride
            8,                             // src0RepeatStride
            8                              // src1RepeatStride
        );
        PIPE_BARRIER(V);
    }
    if (__n0 % BLOCK_SIZE > 0) {
        if (__n0 / BLOCK_SIZE > 0) { // 大于16的余块
            __set_mask(__n0 % BLOCK_SIZE);
            max_v<ArchType::ASCEND_V200, float>(
                tvUbuf_tensor.ReinterpretCast<float>(), tvUbuf_tensor.ReinterpretCast<float>(),
                lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE + __n0 / BLOCK_SIZE * fm * BLOCK_SIZE],
                fm, // repeat
                1,  // dstBlockStride
                1,  // src0BlockStride
                1,  // src1BlockStride
                1,  // dstRepeatStride
                1,  // src0RepeatStride
                1   // src1RepeatStride
            );
            PIPE_BARRIER(V);
            set_vector_mask((uint64_t)-1, (uint64_t)-1);
        }
    }
    PIPE_BARRIER(V);
    if (__n0  > FLOAT_BLOCK_SIZE) {
        if(__n0 < BLOCK_SIZE) {
            __set_mask(__n0 % 8);
        }
        max_v<ArchType::ASCEND_V200, float>(
            tvUbuf_tensor.ReinterpretCast<float>(), 
            tvUbuf_tensor.ReinterpretCast<float>(),
            tvUbuf_tensor.ReinterpretCast<float>()[8],
            fm * FLOAT_BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
            2,                             // dstBlockStride
            2,                             // src0BlockStride
            2,                             // src1BlockStride
            16,                             // dstRepeatStride
            16,                             // src0RepeatStride
            16                              // src1RepeatStride
        );
        PIPE_BARRIER(V);
        ub_to_ub<ArchType::ASCEND_V200, float>(
            loUbuf_tensor, 
            tvUbuf_tensor.ReinterpretCast<float>(),
            0,  // sid
            1,  // nBurst
            fm * 16 / 8, // lenBurst
            0,  // srcGap
            0   // dstGap
        );
    } else {
        ub_to_ub<ArchType::ASCEND_V200, float>(
            loUbuf_tensor, 
            tvUbuf_tensor.ReinterpretCast<float>(),
            0,  // sid
            1,  // nBurst
            fm * 16 / 8, // lenBurst
            0,  // srcGap
            0   // dstGap
        );
    }
    if (__n0 < BLOCK_SIZE) {
        __set_vcg_mask_fp32(__n0);
    }
    PIPE_BARRIER(V);
    #if __CCE_AICORE__ != 100
        cgmax_v<ArchType::ASCEND_V200, float>(
            lmUbuf_tensor.ReinterpretCast<float>(),
            loUbuf_tensor,
            fm * FLOAT_BLOCK_SIZE / FLOAT_VECTOR_SIZE, //repeat
            1, //dstRepStride
            2, //srcBlkStride 
            16 //srcRepStride
        );
    #endif
    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
    PIPE_BARRIER(V);
    if (initGgDm == 0) {
        max_v<ArchType::ASCEND_V200, float>(hmUbuf_tensor.ReinterpretCast<float>(), 
                                            lmUbuf_tensor.ReinterpretCast<float>(), 
                                            gmUbuf_tensor.ReinterpretCast<float>(),
                                            mD64, // repeat
                                            1,     // dstBlockStride
                                            1,     // src0BlockStride
                                            1,     // src1BlockStride
                                            8,     // dstRepeatStride
                                            8,     // src0RepeatStride
                                            8      // src1RepeatStride
        );
        PIPE_BARRIER(V);
        // dm = mj - minew
        sub_v<ArchType::ASCEND_V200, float>(dmUbuf_tensor.ReinterpretCast<float>()[Pingflag * 256], 
                                            gmUbuf_tensor.ReinterpretCast<float>(), 
                                            hmUbuf_tensor.ReinterpretCast<float>(),
                                            mD64, // repeat
                                            1,     // dstBlockStride
                                            1,     // src0BlockStride
                                            1,     // src1BlockStride
                                            8,     // dstRepeatStride
                                            8,     // src0RepeatStride
                                            8      // src1RepeatStride
        );

        PIPE_BARRIER(V);
    } else {
        ub_to_ub<ArchType::ASCEND_V200, float>(hmUbuf_tensor.ReinterpretCast<float>(), 
                                            lmUbuf_tensor.ReinterpretCast<float>(),
                                            0,               // sid
                                            1,               // nBurst
                                            fm / 8, // lenBurst
                                            0,               // srcGap
                                            0                // dstGap
        );
        PIPE_BARRIER(V);
    }
    ub_to_ub<ArchType::ASCEND_V200, float>(
        gmUbuf_tensor.ReinterpretCast<float>(), 
        hmUbuf_tensor.ReinterpretCast<float>(),
        0,               // sid
        1,               // nBurst
        fm / 8, // lenBurst
        0,               // srcGap
        0                // dstGap
    );
    initGgDm = 0;
    SET_FLAG(V, S, EVENT_ID0);
    WAIT_FLAG(V, S, EVENT_ID0);
    PIPE_BARRIER(V);
    // *** hm_block = expand_to_block(hm), (fm,) -> (fm, 16)
    ExpandToBlockFloat(tvUbuf_tensor.ReinterpretCast<float>(),
                            hmUbuf_tensor.ReinterpretCast<float>(),
                            fm);
    // *** ls = ls - hm_block
    for (int32_t vsubIdx = 0; vsubIdx < (fn / BLOCK_SIZE); ++vsubIdx) { // (fm, fn)
        sub_v<ArchType::ASCEND_V200, float>(
            lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
            lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
            tvUbuf_tensor.ReinterpretCast<float>(),
            fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
            1,                             // dstBlockStride
            1,                             // src0BlockStride
            1,                             // src1BlockStride
            8,                             // dstRepeatStride
            8,                             // src0RepeatStride
            8                              // src1RepeatStride
        );
    }
    PIPE_BARRIER(V);
    SoftmaxExp<float, float, PrecType::BMM1_FP32_EXP_FP32>(lpUbuf_tensor[Pingflag * UB_FLOAT_BUF_SIZE],
                                lsUbuf_tensor.ReinterpretCast<float>()[Pingflag * UB_FLOAT_BUF_SIZE],
                                ls32Ubuf_tensor,
                                pSize);
    PIPE_BARRIER(V);
    SET_FLAG(V, MTE3, Pingflag);
    RowSum(__n0, fm, Pingflag);
    // 3. ################ Softmax Ping Ends #######################
    // 4. ################ Softmax Pong Starts #######################
    if (__n1 != -1) {
        WAIT_FLAG(M, V, Pongflag);
        WAIT_FLAG(MTE3, V, Pongflag);
        l0c_to_ub<ArchType::ASCEND_V200, float, float>(
            lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE],
            l0cBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 1,
            pSize_b / CUBE_MATRIX_SIZE, 0, 0);
        SET_FLAG(V, M, Pongflag);
        SET_FLAG(V, S, Pingflag + 2);
        WAIT_FLAG(V, S, Pingflag + 2);
        if (scaleType == 1) {
            // expand logn to block
            ExpandToBlockFloat(tvUbuf_tensor.ReinterpretCast<float>(),
                            logn_ub_tensor.ReinterpretCast<float>(),
                            fm);
            // loop for column, repeat for row
            for (uint32_t fn_block_idx = 0; fn_block_idx < (__n1 / BLOCK_SIZE); ++fn_block_idx) {
                mul_v<ArchType::ASCEND_V200, float>(
                    lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE + fn_block_idx * fm * BLOCK_SIZE],
                    lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE + fn_block_idx * fm * BLOCK_SIZE],
                    tvUbuf_tensor.ReinterpretCast<float>(),
                    fm *16/64,                    // repeat
                    1,                        // dstBlockStride
                    1,                        // src0BlockStride
                    0,                        // src1BlockStride
                    8,  // dstRepeatStride
                    8,  // src0RepeatStride
                    1                         // src1RepeatStride
                );
                PIPE_BARRIER(V);
            }
            if (__n1 % BLOCK_SIZE > 0) {
                __set_mask(__n1 % BLOCK_SIZE);
                mul_v<ArchType::ASCEND_V200, float>(
                    lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE + __n1 / BLOCK_SIZE * fm * BLOCK_SIZE],
                    lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE + __n1 / BLOCK_SIZE * fm * BLOCK_SIZE],
                    tvUbuf_tensor.ReinterpretCast<float>(),
                    fm,                    // repeat
                    1,                        // dstBlockStride
                    1,                        // src0BlockStride
                    0,                        // src1BlockStride
                    2,  // dstRepeatStride
                    2,  // src0RepeatStride
                    1                         // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
            PIPE_BARRIER(V);
        }
        // 3.1. mask(attention score * tor)
        muls_v<ArchType::ASCEND_V200, float>(
            lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE],
            lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE], tor,
            pSize_b / 64, // repeat
            1,             // dstBlockStride
            1,             // srcBlockStride
            uint16_t(8),   // dstRepeatStride
            uint16_t(8)    // srcRepeatStride
        );
        PIPE_BARRIER(V);
        WAIT_FLAG(V, MTE1, EVENT_ID0);
        
        if ((gmSrcm != nullptr) && (add_mask_n1 == 1)) {
            // Mask Pong Load UB
#if __CCE_AICORE__ != 100
            WAIT_FLAG(MTE2, MTE1, Pongflag + 2);
#endif
            l1_to_ub<ArchType::ASCEND_V200, half>(loUbuf_tensor.ReinterpretCast<half>(),
                                                  l1maskBufAddr_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                                  1,                    // nBurst, 次数
                                                  fm * bn / BLOCK_SIZE, // lenBurst
                                                  0,                    // srcStride，尾-头,32byte
                                                  0);
            SET_FLAG(MTE1, MTE2, Pongflag + 2);
            SET_FLAG(MTE1, V, Pongflag);
            WAIT_FLAG(MTE1, V, Pongflag);
            conv_v<ArchType::ASCEND_V200, half, float>(
                loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2],
                loUbuf_tensor.ReinterpretCast<half>(),
                pSize / FLOAT_VECTOR_SIZE, // repeat
                1,                             // dstBlockStride
                1,                             // srcBlockStride
                uint16_t(8),                   // dstRepeatStride
                uint16_t(4)                    // srcRepeatStride
            );
            PIPE_BARRIER(V);
            if (gmSrcAlibiCoeff != nullptr) {
                if(alibi_left_align == 1){
                    adds_v<ArchType::ASCEND_V200, float>(
                        loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2],
                        loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2], (float)delta1,
                        fm * fn / FLOAT_VECTOR_SIZE, // repeat
                        1,                     // dstBlockStride
                        1,                     // src0BlockStride
                        8,                     // dstRepeatStride
                        8                      // src0RepeatStride
                    );
                    PIPE_BARRIER(V);
                }
                muls_v<ArchType::ASCEND_V200, float>(loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2],
                                                    loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2],
                                                    (float)alibi_coeff,
                                                    fm * fn / FLOAT_VECTOR_SIZE, // repeat
                                                    1,                     // dstBlockStride
                                                    1,                     // src0BlockStride
                                                    8,                     // dstRepeatStride
                                                    8                      // src0RepeatStride
                );
                PIPE_BARRIER(V);
            }
            add_v<ArchType::ASCEND_V200, float>(lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE],
                                                lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE],
                                                loUbuf_tensor.ReinterpretCast<float>()[L0AB_HALF_BUF_SIZE / 2], // maskFP32
                                                fm * fn / FLOAT_VECTOR_SIZE, // repeat
                                                1,                     // dstBlockStride
                                                1,                     // src0BlockStride
                                                1,                     // src1BlockStride
                                                8,                     // dstRepeatStride
                                                8,                     // src0RepeatStride
                                                8                      // src1RepeatStride
            );
            PIPE_BARRIER(V);
        }
        // 3. softmax part
        if (__n1 / BLOCK_SIZE > 1) { // 前两个(fm, 16)求最大值
            max_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE],
                                               lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE + fm * BLOCK_SIZE],
                                               fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                               1,                             // dstBlockStride
                                               1,                             // src0BlockStride
                                               1,                             // src1BlockStride
                                               8,                             // dstRepeatStride
                                               8,                             // src0RepeatStride
                                               8                              // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), 
                                                    lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE],
                                                    0,  // sid
                                                    1,  // nBurst
                                                    fm * 16 / 8, // lenBurst
                                                    0,  // srcGap
                                                    0   // dstGap
            );

            PIPE_BARRIER(V);
        }
        for (int32_t rowmaxIdx = 2; rowmaxIdx < (__n1 / BLOCK_SIZE); ++rowmaxIdx) { // 循环比较(fm, 16)
            max_v<ArchType::ASCEND_V200, float>(
                tvUbuf_tensor.ReinterpretCast<float>(), 
                tvUbuf_tensor.ReinterpretCast<float>(),
                lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE + rowmaxIdx * fm * BLOCK_SIZE],
                fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                1,                             // dstBlockStride
                1,                             // src0BlockStride
                1,                             // src1BlockStride
                8,                             // dstRepeatStride
                8,                             // src0RepeatStride
                8                              // src1RepeatStride
            );

            PIPE_BARRIER(V);
        }
        if (__n1 % BLOCK_SIZE > 0) {
            if (__n1 / BLOCK_SIZE > 0) {
                __set_mask(__n1 % BLOCK_SIZE);
                max_v<ArchType::ASCEND_V200, float>(
                    tvUbuf_tensor.ReinterpretCast<float>(), tvUbuf_tensor.ReinterpretCast<float>(),
                    lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE + __n1 / BLOCK_SIZE * fm * BLOCK_SIZE],
                    fm, // repeat
                    1,  // dstBlockStride
                    1,  // src0BlockStride
                    1,  // src1BlockStride
                    1,  // dstRepeatStride 
                    1,  // src0RepeatStride
                    1   // src1RepeatStride
                );
                PIPE_BARRIER(V);
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
        }
        
        PIPE_BARRIER(V);
        if (__n1  > 8) {
            if(__n1 < BLOCK_SIZE) {
            __set_mask(__n1 % 8);
            }
            max_v<ArchType::ASCEND_V200, float>(
                tvUbuf_tensor.ReinterpretCast<float>(),
                tvUbuf_tensor.ReinterpretCast<float>(),
                tvUbuf_tensor.ReinterpretCast<float>()[8],
                fm * FLOAT_BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                2,                             // dstBlockStride
                2,                             // src0BlockStride
                2,                             // src1BlockStride
                16,                             // dstRepeatStride
                16,                             // src0RepeatStride
                16                              // src1RepeatStride
            );
            PIPE_BARRIER(V);
            ub_to_ub<ArchType::ASCEND_V200, float>(
                loUbuf_tensor, 
                tvUbuf_tensor.ReinterpretCast<float>(),
                0,  // sid
                1,  // nBurst
                fm * 16 / 8, // lenBurst
                0,  // srcGap
                0   // dstGap
            );
        } else {
            ub_to_ub<ArchType::ASCEND_V200, float>(
                loUbuf_tensor, 
                tvUbuf_tensor.ReinterpretCast<float>(),
                0,  // sid
                1,  // nBurst
                fm * 16 / 8, // lenBurst
                0,  // srcGap
                0   // dstGap
            );
        }
        if (__n1 < BLOCK_SIZE) {
            __set_vcg_mask_fp32(__n1);
        }
        PIPE_BARRIER(V);
        #if __CCE_AICORE__ != 100
            cgmax_v<ArchType::ASCEND_V200, float>(
                lmUbuf_tensor.ReinterpretCast<float>(),
                loUbuf_tensor,
                fm * FLOAT_BLOCK_SIZE / FLOAT_VECTOR_SIZE, //repeat
                1, //dstRepStride
                2, //srcBlkStride 
                16 //srcRepStride
        );
        #endif
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        PIPE_BARRIER(V);
        if (initGgDm == 0) {
            max_v<ArchType::ASCEND_V200, float>(hmUbuf_tensor.ReinterpretCast<float>(), 
                                                lmUbuf_tensor.ReinterpretCast<float>(), 
                                                gmUbuf_tensor.ReinterpretCast<float>(),
                                                mD64, // repeat
                                                1,     // dstBlockStride
                                                1,     // src0BlockStride
                                                1,     // src1BlockStride
                                                8,     // dstRepeatStride
                                                8,     // src0RepeatStride
                                                8      // src1RepeatStride
            );

            PIPE_BARRIER(V);
            // dm = mj - minew
            sub_v<ArchType::ASCEND_V200, float>(dmUbuf_tensor.ReinterpretCast<float>()[Pongflag * 256], 
                                                gmUbuf_tensor.ReinterpretCast<float>(), 
                                                hmUbuf_tensor.ReinterpretCast<float>(),
                                                mD64, // repeat
                                                1,     // dstBlockStride
                                                1,     // src0BlockStride
                                                1,     // src1BlockStride
                                                8,     // dstRepeatStride
                                                8,     // src0RepeatStride
                                                8      // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, float>(hmUbuf_tensor.ReinterpretCast<float>(), 
                                                    lmUbuf_tensor.ReinterpretCast<float>(),
                                                    0,               // sid
                                                    1,               // nBurst
                                                    fm / FLOAT_BLOCK_SIZE, // lenBurst
                                                    0,               // srcGap
                                                    0                // dstGap
            );
            PIPE_BARRIER(V);
        }
        ub_to_ub<ArchType::ASCEND_V200, float>(
                gmUbuf_tensor.ReinterpretCast<float>(), 
                hmUbuf_tensor.ReinterpretCast<float>(),
                0,               // sid
                1,               // nBurst
                fm / FLOAT_BLOCK_SIZE, // lenBurst
                0,               // srcGap
                0                // dstGap
        );
        initGgDm = 0;
        SET_FLAG(V, S, EVENT_ID0);
        WAIT_FLAG(V, S, EVENT_ID0);
        PIPE_BARRIER(V);
        // (fm,) -> (fm, 16)
        ExpandToBlockFloat(tvUbuf_tensor.ReinterpretCast<float>(),
                            hmUbuf_tensor.ReinterpretCast<float>(),
                            fm);
        // *** ls = ls - hm_block
        for (int32_t vsubIdx = 0; vsubIdx < (bn / BLOCK_SIZE); ++vsubIdx) { // (fm, bn)
            sub_v<ArchType::ASCEND_V200, float>(
                lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                tvUbuf_tensor.ReinterpretCast<float>(),
                fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                1,                             // dstBlockStride
                1,                             // src0BlockStride
                1,                             // src1BlockStride
                8,                             // dstRepeatStride
                8,                             // src0RepeatStride
                8                              // src1RepeatStride
            );
        }
        PIPE_BARRIER(V);
        SoftmaxExp<float, float, PrecType::BMM1_FP32_EXP_FP32>(lpUbuf_tensor[Pongflag * UB_FLOAT_BUF_SIZE],
                                lsUbuf_tensor.ReinterpretCast<float>()[Pongflag * UB_FLOAT_BUF_SIZE],
                                ls32Ubuf_tensor,
                                pSize_b);
        PIPE_BARRIER(V);
        SET_FLAG(V, MTE3, Pongflag);
        RowSum(__n1, fm, Pongflag);
    }
    // 4. ################ Softmax Pong Ends #######################
}
template <>
__aicore__ inline void UnpadFlashAttentionCommon<float, half, PrecType::BMM1_FP16_EXP_FP32 , PrecType::BMM1_FP16_EXP_FP32>::SoftMax(
                                                const int32_t fm, const int32_t fn, const int32_t fk, const int32_t bn,
                                                const int32_t __m0, const int32_t __n0, const int32_t __n1,
                                                const int32_t add_mask_n0, const int32_t add_mask_n1,
                                                const half alibi_coeff, const half delta0, const half delta1,
                                                const uint32_t scaleType, const uint32_t alibi_left_align, uint32_t initGgDm)
{
    int32_t Pingflag = 0;
    int32_t Pongflag = 1;
    int32_t pSize = fm * fn;
    int32_t pSize_b = fm * bn;
    int32_t mD128 = (fm + VECTOR_SIZE - 1) / VECTOR_SIZE;
    // 3. ################ Softmax Ping Starts #######################
    WAIT_FLAG(M, V, Pingflag);
    WAIT_FLAG(MTE3, V, Pingflag);
    l0c_to_ub<ArchType::ASCEND_V200, float, half>(lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                                  l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 1,
                                                  pSize / CUBE_MATRIX_SIZE, 0, 0);
    SET_FLAG(V, M, Pingflag);
    PIPE_BARRIER(V);

    if (scaleType == 1) {
#if __CCE_AICORE__ == 100
        WAIT_FLAG(V, MTE2, EVENT_ID0);
#else
        WAIT_FLAG(V, MTE2, EVENT_ID6);
#endif
        AscendC::GlobalTensor<half> logn_gm_tensor;
        logn_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(gmSrcLogn));
        gm_to_ub<ArchType::ASCEND_V200, half>(logn_ub_tensor, logn_gm_tensor[lognOffset], 0, 1, fm / BLOCK_SIZE, 0, 0);

        SET_FLAG(MTE2, V, Pingflag + 2);
        WAIT_FLAG(MTE2, V, Pingflag + 2);

        // expand logn to block
        ExpandToBlockHalf(tvUbuf_tensor, logn_ub_tensor, fm); // (fm,) -> (fm, 16)
        PIPE_BARRIER(V);
        // loop for column, repeat for row
        for (uint32_t fn_block_idx = 0; fn_block_idx < (__n0 / VECTOR_SIZE); ++fn_block_idx) {
            mul_v<ArchType::ASCEND_V200, half>(
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + fn_block_idx * fm * VECTOR_SIZE],
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + fn_block_idx * fm * VECTOR_SIZE],
                tvUbuf_tensor.ReinterpretCast<half>(),
                __m0,                    // repeat
                fm,                        // dstBlockStride
                fm,                        // src0BlockStride
                0,                        // src1BlockStride
                1,  // dstRepeatStride
                1,  // src0RepeatStride
                1                         // src1RepeatStride
            );
        }
        if (__n0 % VECTOR_SIZE > 0) {
            __set_mask(__n0 % VECTOR_SIZE);
            mul_v<ArchType::ASCEND_V200, half>(
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + __n0 / VECTOR_SIZE * fm * VECTOR_SIZE],
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + __n0 / VECTOR_SIZE * fm * VECTOR_SIZE],
                tvUbuf_tensor.ReinterpretCast<half>(),
                __m0,                    // repeat
                fm,                        // dstBlockStride
                fm,                        // src0BlockStride
                0,                        // src1BlockStride
                1,  // dstRepeatStride
                1,  // src0RepeatStride
                1                         // src1RepeatStride
            );
            __set_mask(VECTOR_SIZE);
        }
        PIPE_BARRIER(V);
#if __CCE_AICORE__ == 100
                SET_FLAG(V, MTE2, EVENT_ID0);
#else
                SET_FLAG(V, MTE2, EVENT_ID6);
#endif
    }


    // 3.1. mask(attention score * tor)
    muls_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                        lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], tor,
                                        pSize / 128,             // repeat
                                        1,                       // dstBlockStride
                                        1,                       // srcBlockStride
                                        uint16_t(8), uint16_t(8) // dstRepeatStride
    );
    PIPE_BARRIER(V);
    // Must Sync, otherwise error
    WAIT_FLAG(V, MTE1, EVENT_ID0);
    // Mask Ping Load UB
    if ((gmSrcm != nullptr) && (add_mask_n0 == 1)) {
#if __CCE_AICORE__ != 100
        WAIT_FLAG(MTE2, MTE1, Pingflag + 2);
#endif
        l1_to_ub<ArchType::ASCEND_V200, half>(loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                                              l1maskBufAddr_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                              1,                    // nBurst, 次数
                                              fm * fn / BLOCK_SIZE, // lenBurst
                                              0,                    // srcStride，尾-头,32byte
                                              0);
        SET_FLAG(MTE1, MTE2, Pingflag + 2);
        SET_FLAG(MTE1, V, Pingflag);
        WAIT_FLAG(MTE1, V, Pingflag);
        if (gmSrcAlibiCoeff != nullptr) {
            if (srcmOffset0) {
                if (isSqrt == 1) {
                    mul_v<ArchType::ASCEND_V200, half>(
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        fm * fn / VECTOR_SIZE, // repeat
                        1,                     // dstBlockStride
                        1,                     // src0BlockStride
                        1,                     // src1BlockStride
                        8,                     // dstRepeatStride
                        8,                     // src0RepeatStride
                        8                      // src1RepeatStride
                    );
                    PIPE_BARRIER(V);
                }
                adds_v<ArchType::ASCEND_V200, half>(
                    loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                    loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE], (half)delta0,
                    fm * fn / VECTOR_SIZE, // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    8,                     // dstRepeatStride
                    8                      // src0RepeatStride
                );
                PIPE_BARRIER(V);
                if (isSqrt == 1) {
                    sqrt_v<ArchType::ASCEND_V200, half>(
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        fm * fn / VECTOR_SIZE, // repeat
                        1,                     // dstBlockStride
                        1,                     // src0BlockStride
                        8,                     // dstRepeatStride
                        8                      // src0RepeatStride
                    );
                    PIPE_BARRIER(V);
                }
            }
            muls_v<ArchType::ASCEND_V200, half>(loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                                                loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                                                (half)alibi_coeff,
                                                fm * fn / VECTOR_SIZE, // repeat
                                                1,                     // dstBlockStride
                                                1,                     // src0BlockStride
                                                8,                     // dstRepeatStride
                                                8                      // src0RepeatStride
            );
            PIPE_BARRIER(V);
        }
        add_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                           lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                           loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                                           fm * fn / VECTOR_SIZE, // repeat
                                           1,                     // dstBlockStride
                                           1,                     // src0BlockStride
                                           1,                     // src1BlockStride
                                           8,                     // dstRepeatStride
                                           8,                     // src0RepeatStride
                                           8                      // src1RepeatStride
        );

        PIPE_BARRIER(V);
    }
    // 3. softmax part
    if (__n0 / BLOCK_SIZE > 1) { // 前两个(fm, 16)求最大值
        max_v<ArchType::ASCEND_V200, half>(tvUbuf_tensor, lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                           lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + fm * BLOCK_SIZE],
                                           fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                           1,                             // dstBlockStride
                                           1,                             // src0BlockStride
                                           1,                             // src1BlockStride
                                           8,                             // dstRepeatStride
                                           8,                             // src0RepeatStride
                                           8                              // src1RepeatStride
        );
        PIPE_BARRIER(V);
    } else {
        ub_to_ub<ArchType::ASCEND_V200, half>(tvUbuf_tensor, lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                              0,  // sid
                                              1,  // nBurst
                                              fm, // lenBurst
                                              0,  // srcGap
                                              0   // dstGap
        );

        PIPE_BARRIER(V);
    }
    for (int32_t rowmaxIdx = 2; rowmaxIdx < (__n0 / BLOCK_SIZE); ++rowmaxIdx) { // 循环比较(fm, 16)
        max_v<ArchType::ASCEND_V200, half>(tvUbuf_tensor, tvUbuf_tensor,
                                           lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + rowmaxIdx * fm * BLOCK_SIZE],
                                           fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                           1,                             // dstBlockStride
                                           1,                             // src0BlockStride
                                           1,                             // src1BlockStride
                                           8,                             // dstRepeatStride
                                           8,                             // src0RepeatStride
                                           8                              // src1RepeatStride
        );
        PIPE_BARRIER(V);
    }
    if (__n0 % BLOCK_SIZE > 0) {
        __set_mask(__n0 % BLOCK_SIZE);
        if (__n0 / BLOCK_SIZE > 0) {
            max_v<ArchType::ASCEND_V200, half>(
                tvUbuf_tensor, tvUbuf_tensor,
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + __n0 / BLOCK_SIZE * fm * BLOCK_SIZE],
                fm, // repeat
                1,  // dstBlockStride
                1,  // src0BlockStride
                1,  // src1BlockStride
                1,  // dstRepeatStride
                1,  // src0RepeatStride
                1   // src1RepeatStride
            );

            PIPE_BARRIER(V);
            set_vector_mask((uint64_t)-1, (uint64_t)-1);
        }
    }
    if (__n0 < BLOCK_SIZE) {
        __set_vcg_mask(__n0);
    }
    cgmax_v<ArchType::ASCEND_V200, half>(lmUbuf_tensor, tvUbuf_tensor, fm * BLOCK_SIZE / VECTOR_SIZE, 1, 1, 8);
    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
    PIPE_BARRIER(V);

    if (initGgDm == 0) { // 需要update m_j
        max_v<ArchType::ASCEND_V200, half>(hmUbuf_tensor, lmUbuf_tensor, gmUbuf_tensor,
                                           mD128, // repeat
                                           1,     // dstBlockStride
                                           1,     // src0BlockStride
                                           1,     // src1BlockStride
                                           8,     // dstRepeatStride
                                           8,     // src0RepeatStride
                                           8      // src1RepeatStride
        );
        PIPE_BARRIER(V);
        sub_v<ArchType::ASCEND_V200, half>(dmUbuf_tensor[Pingflag * UB_HALF_LINE_SIZE], gmUbuf_tensor, hmUbuf_tensor,
                                           mD128, // repeat
                                           1,     // dstBlockStride
                                           1,     // src0BlockStride
                                           1,     // src1BlockStride
                                           8,     // dstRepeatStride
                                           8,     // src0RepeatStride
                                           8      // src1RepeatStride
        );

        PIPE_BARRIER(V);
    } else {
        ub_to_ub<ArchType::ASCEND_V200, half>(hmUbuf_tensor, lmUbuf_tensor,
                                              0,               // sid
                                              1,               // nBurst
                                              fm / BLOCK_SIZE, // lenBurst
                                              0,               // srcGap
                                              0                // dstGap
        );
        PIPE_BARRIER(V);
    }

    ub_to_ub<ArchType::ASCEND_V200, half>(gmUbuf_tensor, hmUbuf_tensor,
                                          0,               // sid
                                          1,               // nBurst
                                          fm / BLOCK_SIZE, // lenBurst
                                          0,               // srcGap
                                          0                // dstGap
    );

    initGgDm = 0;
    PIPE_BARRIER(V);
    ExpandToBlockHalf(tvUbuf_tensor, hmUbuf_tensor, fm);                // (fm,) -> (fm, 16)
    for (int32_t vsubIdx = 0; vsubIdx < (fn / BLOCK_SIZE); ++vsubIdx) { // (fm, fn)
        sub_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                                           lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                                           tvUbuf_tensor,
                                           fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                           1,                             // dstBlockStride
                                           1,                             // src0BlockStride
                                           1,                             // src1BlockStride
                                           8,                             // dstRepeatStride
                                           8,                             // src0RepeatStride
                                           8                              // src1RepeatStride
        );
    }
    PIPE_BARRIER(V);
    // 2 for Repeatimes above 255
    for (int32_t vconvIdx = 0; vconvIdx < 2; ++vconvIdx) {
        conv_v<ArchType::ASCEND_V200, half, float>(ls32Ubuf_tensor[vconvIdx * pSize / 2],
                                                   lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + vconvIdx * pSize / 2],
                                                   pSize / 2 / FLOAT_VECTOR_SIZE, // repeat
                                                   1,                             // dstBlockStride
                                                   1,                             // srcBlockStride
                                                   uint16_t(8),                   // dstRepeatStride
                                                   uint16_t(4)                    // srcRepeatStride
        );
    }
    PIPE_BARRIER(V);
    // 2 for Repeatimes above 255
    for (int32_t vexpIdx = 0; vexpIdx < 2; ++vexpIdx) {
        exp_v<ArchType::ASCEND_V200, float>(ls32Ubuf_tensor[vexpIdx * pSize / 2], ls32Ubuf_tensor[vexpIdx * pSize / 2],
                                            pSize / 2 / FLOAT_VECTOR_SIZE, // repeat
                                            1,                             // dstBlockStride
                                            1,                             // srcBlockStride
                                            8,                             // dstRepeatStride
                                            8                              // srcRepeatStride
        );
    }
    PIPE_BARRIER(V);
    // 2 for Repeatimes above 255
    for (int32_t vconvIdx = 0; vconvIdx < 2; ++vconvIdx) {
        conv_v<ArchType::ASCEND_V200, float, half>(lpUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + vconvIdx * pSize / 2],
                                                   ls32Ubuf_tensor[vconvIdx * pSize / 2],
                                                   pSize / 2 / FLOAT_VECTOR_SIZE, // repeat
                                                   1,                             // dstBlockStride
                                                   1,                             // srcBlockStride
                                                   4,                             // dstRepeatStride
                                                   8                              // srcRepeatStride
        );
    }
    PIPE_BARRIER(V);
    SET_FLAG(V, MTE3, Pingflag);
    if (__n0 / BLOCK_SIZE > 1) {
        add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), ls32Ubuf_tensor,
                                            ls32Ubuf_tensor[fm * BLOCK_SIZE],
                                            fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                            1,                                   // dstBlockStride
                                            1,                                   // src0BlockStride
                                            1,                                   // src1BlockStride
                                            8,                                   // dstRepeatStride
                                            8,                                   // src0RepeatStride
                                            8                                    // src1RepeatStride
        );
        PIPE_BARRIER(V);
    } else {
        ub_to_ub<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), ls32Ubuf_tensor,
                                               0,                   // sid
                                               1,                   // nBurst
                                               fm * BLOCK_SIZE / 8, // lenBurst
                                               0,                   // srcGap
                                               0                    // dstGap
        );
        PIPE_BARRIER(V);
    }
    for (int32_t rowsumIdx = 2; rowsumIdx < (__n0 / BLOCK_SIZE); ++rowsumIdx) {
        add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                            tvUbuf_tensor.ReinterpretCast<float>(),
                                            ls32Ubuf_tensor[rowsumIdx * fm * BLOCK_SIZE],
                                            fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                            1,                                   // dstBlockStride
                                            1,                                   // src0BlockStride
                                            1,                                   // src1BlockStride
                                            8,                                   // dstRepeatStride
                                            8,                                   // src0RepeatStride
                                            8                                    // src1RepeatStride
        );
        PIPE_BARRIER(V);
    }
    SetMasknorm();
    if (__n0 % BLOCK_SIZE > 0) {
        __set_mask(__n0 % BLOCK_SIZE);
        if (__n0 / BLOCK_SIZE > 0) {
            add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                                tvUbuf_tensor.ReinterpretCast<float>(),
                                                ls32Ubuf_tensor[__n0 / BLOCK_SIZE * fm * BLOCK_SIZE],
                                                fm, // repeat
                                                1,  // dstBlockStride
                                                1,  // src0BlockStride
                                                1,  // src1BlockStride
                                                2,  // dstRepeatStride
                                                2,  // src0RepeatStride
                                                2   // src1RepeatStride
            );
            PIPE_BARRIER(V);
            SetVectorMask<int8_t>(0x0, 0xffff);
        }
    } else {
        SetVectorMask<int8_t>(0x0, 0xffff);
    }

    cadd_v<ArchType::ASCEND_V200, float>(llUbuf_tensor[Pingflag * UB_FLOAT_LINE_SIZE],
                                         tvUbuf_tensor.ReinterpretCast<float>(), fm,
                                         1,  // dstRepeatStride
                                         1,  // srcBlockStride
                                         2); // srcRepeatStride, fp32 2 block

    PIPE_BARRIER(V);
    // Must Sync, otherwise error
    SET_FLAG(V, MTE1, EVENT_ID0);
    SetMasknorm();
    SetVectorMask<int8_t>(0xffffffffffffffff, 0xffffffffffffffff);
    // 3. ################ Softmax Ping Ends #######################
    // 4. ################ Softmax Pong Starts #######################
    if (__n1 != -1) {
        WAIT_FLAG(M, V, Pongflag);
        WAIT_FLAG(MTE3, V, Pongflag);
        l0c_to_ub<ArchType::ASCEND_V200, float, half>(lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                                      l0cBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 1,
                                                      pSize_b / CUBE_MATRIX_SIZE, 0, 0);
        PIPE_BARRIER(V);
        SET_FLAG(V, M, Pongflag);

        if (scaleType == 1) {
            // expand logn to block
            ExpandToBlockHalf(tvUbuf_tensor, logn_ub_tensor, fm); // (fm,) -> (fm, 16)
            PIPE_BARRIER(V);
            // loop for column, repeat for row
            for (uint32_t fn_block_idx = 0; fn_block_idx < (__n1 / VECTOR_SIZE); ++fn_block_idx) {
                mul_v<ArchType::ASCEND_V200, half>(
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + fn_block_idx * fm * VECTOR_SIZE],
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + fn_block_idx * fm * VECTOR_SIZE],
                    tvUbuf_tensor.ReinterpretCast<half>(),
                    __m0,                    // repeat
                    fm,                        // dstBlockStride
                    fm,                        // src0BlockStride
                    0,                        // src1BlockStride
                    1,  // dstRepeatStride
                    1,  // src0RepeatStride
                    1                         // src1RepeatStride
                );
            }
            if (__n1 % VECTOR_SIZE > 0) {
                __set_mask(__n1 % VECTOR_SIZE);
                mul_v<ArchType::ASCEND_V200, half>(
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + __n1 / VECTOR_SIZE * fm * VECTOR_SIZE],
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + __n1 / VECTOR_SIZE * fm * VECTOR_SIZE],
                    tvUbuf_tensor.ReinterpretCast<half>(),
                    __m0,                    // repeat
                    fm,                        // dstBlockStride
                    fm,                        // src0BlockStride
                    0,                        // src1BlockStride
                    1,  // dstRepeatStride
                    1,  // src0RepeatStride
                    1                         // src1RepeatStride
                );
                __set_mask(VECTOR_SIZE);
            }
            PIPE_BARRIER(V);
        }

        // 3.1. mask(attention score * tor)
        muls_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                            lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], tor,
                                            pSize_b / 128, // repeat
                                            1,             // dstBlockStride
                                            1,             // srcBlockStride
                                            uint16_t(8),   // dstRepeatStride
                                            uint16_t(8)    // srcRepeatStride
        );
        PIPE_BARRIER(V);
        WAIT_FLAG(V, MTE1, EVENT_ID0);
        if ((gmSrcm != nullptr) && (add_mask_n1 == 1)) {
            // Mask Pong Load UB
#if __CCE_AICORE__ != 100
            WAIT_FLAG(MTE2, MTE1, Pongflag + 2);
#endif
            l1_to_ub<ArchType::ASCEND_V200, half>(loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                                                  l1maskBufAddr_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                                  1,                    // nBurst, 次数
                                                  fm * bn / BLOCK_SIZE, // lenBurst
                                                  0,                    // srcStride，尾-头,32byte
                                                  0);
            SET_FLAG(MTE1, MTE2, Pongflag + 2);
            SET_FLAG(MTE1, V, Pongflag);
            WAIT_FLAG(MTE1, V, Pongflag);
            if (gmSrcAlibiCoeff != nullptr) {
                if (srcmOffset1) {
                    if (isSqrt == 1) {
                        mul_v<ArchType::ASCEND_V200, half>(
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            fm * fn / VECTOR_SIZE, // repeat
                            1,                     // dstBlockStride
                            1,                     // src0BlockStride
                            1,                     // src1BlockStride
                            8,                     // dstRepeatStride
                            8,                     // src0RepeatStride
                            8                      // src1RepeatStride
                        );
                        PIPE_BARRIER(V);
                    }
                    adds_v<ArchType::ASCEND_V200, half>(
                        loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                        loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE], (half)delta1,
                        fm * fn / VECTOR_SIZE, // repeat
                        1,                     // dstBlockStride
                        1,                     // src0BlockStride
                        8,                     // dstRepeatStride
                        8                      // src0RepeatStride
                    );
                    PIPE_BARRIER(V);
                    if (isSqrt == 1) {
                        sqrt_v<ArchType::ASCEND_V200, half>(
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            fm * fn / VECTOR_SIZE, // repeat
                            1,                     // dstBlockStride
                            1,                     // src0BlockStride
                            8,                     // dstRepeatStride
                            8                      // src0RepeatStride
                        );
                        PIPE_BARRIER(V);
                    }
                }
                muls_v<ArchType::ASCEND_V200, half>(
                    loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                    loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE], (half)alibi_coeff,
                    fm * fn / VECTOR_SIZE, // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    8,                     // dstRepeatStride
                    8                      // src0RepeatStride
                );
                PIPE_BARRIER(V);
            }
            add_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                               lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                               loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                                               fm * bn / VECTOR_SIZE, // repeat
                                               1,                     // dstBlockStride
                                               1,                     // src0BlockStride
                                               1,                     // src1BlockStride
                                               8,                     // dstRepeatStride
                                               8,                     // src0RepeatStride
                                               8                      // src1RepeatStride
            );
            PIPE_BARRIER(V);
        }
        // 3. softmax part
        if (__n1 / BLOCK_SIZE > 1) { // 前两个(fm, 16)求最大值
            max_v<ArchType::ASCEND_V200, half>(tvUbuf_tensor, lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                               lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + fm * BLOCK_SIZE],
                                               fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                               1,                             // dstBlockStride
                                               1,                             // src0BlockStride
                                               1,                             // src1BlockStride
                                               8,                             // dstRepeatStride
                                               8,                             // src0RepeatStride
                                               8                              // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, half>(tvUbuf_tensor, lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                                  0,  // sid
                                                  1,  // nBurst
                                                  fm, // lenBurst
                                                  0,  // srcGap
                                                  0   // dstGap
            );

            PIPE_BARRIER(V);
        }
        for (int32_t rowmaxIdx = 2; rowmaxIdx < (__n1 / BLOCK_SIZE); ++rowmaxIdx) { // 循环比较(fm, 16)
            max_v<ArchType::ASCEND_V200, half>(
                tvUbuf_tensor, tvUbuf_tensor,
                lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + rowmaxIdx * fm * BLOCK_SIZE],
                fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                1,                             // dstBlockStride
                1,                             // src0BlockStride
                1,                             // src1BlockStride
                8,                             // dstRepeatStride
                8,                             // src0RepeatStride
                8                              // src1RepeatStride
            );

            PIPE_BARRIER(V);
        }
        if (__n1 % BLOCK_SIZE > 0) {
            __set_mask(__n1 % BLOCK_SIZE);
            if (__n1 / BLOCK_SIZE > 0) {
                max_v<ArchType::ASCEND_V200, half>(
                    tvUbuf_tensor, tvUbuf_tensor,
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + __n1 / BLOCK_SIZE * fm * BLOCK_SIZE],
                    fm, // repeat
                    1,  // dstBlockStride
                    1,  // src0BlockStride
                    1,  // src1BlockStride
                    1,  // dstRepeatStride
                    1,  // src0RepeatStride
                    1   // src1RepeatStride
                );
                max_v<ArchType::ASCEND_V200, half>(
                    tvUbuf_tensor, tvUbuf_tensor,
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + __n1 / BLOCK_SIZE * fm * BLOCK_SIZE], fm, 1, 1, 1, 1,
                    1, 1);
                PIPE_BARRIER(V);
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
        }
        if (__n1 < BLOCK_SIZE) {
            __set_vcg_mask(__n1);
        }
        cgmax_v<ArchType::ASCEND_V200, half>(lmUbuf_tensor, tvUbuf_tensor, fm * BLOCK_SIZE / VECTOR_SIZE, 1, 1, 8);
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        PIPE_BARRIER(V);

        if (initGgDm == 0) { // 需要update m_j
            max_v<ArchType::ASCEND_V200, half>(hmUbuf_tensor, lmUbuf_tensor, gmUbuf_tensor,
                                               mD128, // repeat
                                               1,     // dstBlockStride
                                               1,     // src0BlockStride
                                               1,     // src1BlockStride
                                               8,     // dstRepeatStride
                                               8,     // src0RepeatStride
                                               8      // src1RepeatStride
            );

            PIPE_BARRIER(V);
            sub_v<ArchType::ASCEND_V200, half>(dmUbuf_tensor[Pongflag * UB_HALF_LINE_SIZE], gmUbuf_tensor,
                                               hmUbuf_tensor,
                                               mD128, // repeat
                                               1,     // dstBlockStride
                                               1,     // src0BlockStride
                                               1,     // src1BlockStride
                                               8,     // dstRepeatStride
                                               8,     // src0RepeatStride
                                               8      // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, half>(hmUbuf_tensor, lmUbuf_tensor,
                                                  0,               // sid
                                                  1,               // nBurst
                                                  fm / BLOCK_SIZE, // lenBurst
                                                  0,               // srcGap
                                                  0                // dstGap
            );
            PIPE_BARRIER(V);
        }
        ub_to_ub<ArchType::ASCEND_V200, half>(gmUbuf_tensor, hmUbuf_tensor,
                                              0,               // sid
                                              1,               // nBurst
                                              fm / BLOCK_SIZE, // lenBurst
                                              0,               // srcGap
                                              0                // dstGap
        );
        ub_to_ub<ArchType::ASCEND_V200, half>(gmUbuf_tensor, hmUbuf_tensor, 0, 1, fm / BLOCK_SIZE, 0, 0);
        initGgDm = 0;
        PIPE_BARRIER(V);
        ExpandToBlockHalf(tvUbuf_tensor, hmUbuf_tensor, fm);                // (fm,) -> (fm, 16)
        for (int32_t vsubIdx = 0; vsubIdx < (bn / BLOCK_SIZE); ++vsubIdx) { // (fm, bn)
            sub_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                                               lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                                               tvUbuf_tensor,
                                               fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                               1,                             // dstBlockStride
                                               1,                             // src0BlockStride
                                               1,                             // src1BlockStride
                                               8,                             // dstRepeatStride
                                               8,                             // src0RepeatStride
                                               8                              // src1RepeatStride
            );
        }
        PIPE_BARRIER(V);
        // 2 for Repeatimes above 255
        for (int32_t vconvIdx = 0; vconvIdx < 2; ++vconvIdx) {
            conv_v<ArchType::ASCEND_V200, half, float>(
                ls32Ubuf_tensor[vconvIdx * pSize_b / 2],
                lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + vconvIdx * pSize_b / 2],
                pSize_b / 2 / FLOAT_VECTOR_SIZE, // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                uint16_t(8),                     // dstRepeatStride
                uint16_t(4)                      // srcRepeatStride
            );
        }
        PIPE_BARRIER(V);
        for (int32_t vexpIdx = 0; vexpIdx < 2; ++vexpIdx) {
            exp_v<ArchType::ASCEND_V200, float>(ls32Ubuf_tensor[vexpIdx * pSize_b / 2],
                                                ls32Ubuf_tensor[vexpIdx * pSize_b / 2],
                                                pSize_b / 2 / FLOAT_VECTOR_SIZE, // repeat
                                                1,                               // dstBlockStride
                                                1,                               // srcBlockStride
                                                8,                               // dstRepeatStride
                                                8                                // srcRepeatStride
            );
        }
        PIPE_BARRIER(V);

        // 2 for double buffer
        for (int32_t vconvIdx = 0; vconvIdx < 2; ++vconvIdx) {
            conv_v<ArchType::ASCEND_V200, float, half>(
                lpUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + vconvIdx * pSize_b / 2],
                ls32Ubuf_tensor[vconvIdx * pSize_b / 2],
                pSize_b / 2 / FLOAT_VECTOR_SIZE, // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                4,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
        }
        PIPE_BARRIER(V);
        SET_FLAG(V, MTE3, Pongflag);
        if (__n1 / BLOCK_SIZE > 1) {
            add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), ls32Ubuf_tensor,
                                                ls32Ubuf_tensor[fm * BLOCK_SIZE],
                                                fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                                1,                                   // dstBlockStride
                                                1,                                   // src0BlockStride
                                                1,                                   // src1BlockStride
                                                8,                                   // dstRepeatStride
                                                8,                                   // src0RepeatStride
                                                8                                    // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), ls32Ubuf_tensor,
                                                   0,                   // sid
                                                   1,                   // nBurst
                                                   fm * BLOCK_SIZE / 8, // lenBurst
                                                   0,                   // srcGap
                                                   0                    // dstGap
            );
            PIPE_BARRIER(V);
        }
        for (int32_t rowsumIdx = 2; rowsumIdx < (__n1 / BLOCK_SIZE); ++rowsumIdx) {
            add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                                tvUbuf_tensor.ReinterpretCast<float>(),
                                                ls32Ubuf_tensor[rowsumIdx * fm * BLOCK_SIZE],
                                                fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                                1,                                   // dstBlockStride
                                                1,                                   // src0BlockStride
                                                1,                                   // src1BlockStride
                                                8,                                   // dstRepeatStride
                                                8,                                   // src0RepeatStride
                                                8                                    // src1RepeatStride
            );
            PIPE_BARRIER(V);
        }
        SetMasknorm();
        if (__n1 % BLOCK_SIZE > 0) {
            __set_mask(__n1 % BLOCK_SIZE);
            if (__n1 / BLOCK_SIZE > 0) {
                add_v<ArchType::ASCEND_V200, float>(
                    tvUbuf_tensor.ReinterpretCast<float>(), tvUbuf_tensor.ReinterpretCast<float>(),
                    ls32Ubuf_tensor[__n1 / BLOCK_SIZE * fm * BLOCK_SIZE], fm, 1, 1, 1, 2, 2, 2);
                PIPE_BARRIER(V);
                SetVectorMask<int8_t>(0x0, 0xffff);
            }
        } else {
            SetVectorMask<int8_t>(0x0, 0xffff);
        }
        cadd_v<ArchType::ASCEND_V200, float>(llUbuf_tensor[Pongflag * UB_FLOAT_LINE_SIZE],
                                             tvUbuf_tensor.ReinterpretCast<float>(), fm,
                                             1,  // dstRepeatStride
                                             1,  // srcBlockStride
                                             2); // srcRepeatStride, fp32 2 block
        PIPE_BARRIER(V);
        SET_FLAG(V, MTE1, EVENT_ID0);
        SetMasknorm();
        SetVectorMask<int8_t>(0xffffffffffffffff, 0xffffffffffffffff);
    }
    // 4. ################ Softmax Pong Ends #######################
}

template <>
__aicore__ inline void UnpadFlashAttentionCommon<float, half, PrecType::BMM1_FP16_EXP_FP32 , PrecType::BMM2_ONLINE_SOFTMAX_FP16>::SoftMax(
                                                const int32_t fm, const int32_t fn, const int32_t fk, const int32_t bn,
                                                const int32_t __m0, const int32_t __n0, const int32_t __n1,
                                                const int32_t add_mask_n0, const int32_t add_mask_n1,
                                                const half alibi_coeff, const half delta0, const half delta1,
                                                const uint32_t scaleType, const uint32_t alibi_left_align, uint32_t initGgDm)
{
    int32_t Pingflag = 0;
    int32_t Pongflag = 1;
    int32_t pSize = fm * fn;
    int32_t pSize_b = fm * bn;
    int32_t oSize = fm * fk;
    int32_t mD128 = (fm + VECTOR_SIZE - 1) / VECTOR_SIZE;
    int32_t mD64 = (fm + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE;

    const uint32_t l1q_buf_addr_offset = 0;
    const uint32_t l1kpv_buf_addr_offset = 4 * L1_UINT8_BLOCK_SIZE;
    const uint32_t l1diag_buf_addr_offset = UB_UINT8_BLOCK_SIZE; // 32k

    AscendC::LocalTensor<half> l1qBuf_tensor =
        l1qBufAddr_tensor.ReinterpretCast<uint8_t>()[l1q_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1kPingBuf_tensor =
        l1kBufAddr_tensor.ReinterpretCast<uint8_t>()[Pingflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1kPongBuf_tensor =
        l1kBufAddr_tensor.ReinterpretCast<uint8_t>()[Pongflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1vPingBuf_tensor =
        l1vBufAddr_tensor.ReinterpretCast<uint8_t>()[Pingflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1vPongBuf_tensor =
        l1vBufAddr_tensor.ReinterpretCast<uint8_t>()[Pongflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1pPingBuf_tensor =
        l1pBufAddr_tensor.ReinterpretCast<uint8_t>()[Pingflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1pPongBuf_tensor =
        l1pBufAddr_tensor.ReinterpretCast<uint8_t>()[Pongflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1dmDiagPingBuf_tensor = 
        l1diagBufAddr_tensor.ReinterpretCast<uint8_t>()[Pingflag * l1diag_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1dmDiagPongBuf_tensor =
        l1diagBufAddr_tensor.ReinterpretCast<uint8_t>()[Pongflag * l1diag_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1oTempBuf_tensor = l1oBufAddr_tensor.ReinterpretCast<half>();
    // 3. ################ Softmax Ping Starts #######################
    WAIT_FLAG(M, V, Pingflag);
    WAIT_FLAG(MTE3, V, Pingflag);
    l0c_to_ub<ArchType::ASCEND_V200, float, half>(lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                                  l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 1,
                                                  pSize / CUBE_MATRIX_SIZE, 0, 0);
    SET_FLAG(V, M, Pingflag);
    PIPE_BARRIER(V);

    if (scaleType == 1) {
#if __CCE_AICORE__ == 100
        WAIT_FLAG(V, MTE2, EVENT_ID0);
#else
        WAIT_FLAG(V, MTE2, EVENT_ID6);
#endif
        AscendC::GlobalTensor<half> logn_gm_tensor;
        logn_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(gmSrcLogn));
        gm_to_ub<ArchType::ASCEND_V200, half>(logn_ub_tensor, logn_gm_tensor[lognOffset], 0, 1, fm / BLOCK_SIZE, 0, 0);

        SET_FLAG(MTE2, V, Pingflag + 2);
        WAIT_FLAG(MTE2, V, Pingflag + 2);

        // expand logn to block
        ExpandToBlockHalf(tvUbuf_tensor, logn_ub_tensor, fm); // (fm,) -> (fm, 16)
        PIPE_BARRIER(V);
        // loop for column, repeat for row
        for (uint32_t fn_block_idx = 0; fn_block_idx < (__n0 / VECTOR_SIZE); ++fn_block_idx) {
            mul_v<ArchType::ASCEND_V200, half>(
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + fn_block_idx * fm * VECTOR_SIZE],
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + fn_block_idx * fm * VECTOR_SIZE],
                tvUbuf_tensor.ReinterpretCast<half>(),
                __m0,                    // repeat
                fm,                        // dstBlockStride
                fm,                        // src0BlockStride
                0,                        // src1BlockStride
                1,  // dstRepeatStride
                1,  // src0RepeatStride
                1                         // src1RepeatStride
            );
        }
        if (__n0 % VECTOR_SIZE > 0) {
            __set_mask(__n0 % VECTOR_SIZE);
            mul_v<ArchType::ASCEND_V200, half>(
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + __n0 / VECTOR_SIZE * fm * VECTOR_SIZE],
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + __n0 / VECTOR_SIZE * fm * VECTOR_SIZE],
                tvUbuf_tensor.ReinterpretCast<half>(),
                __m0,                    // repeat
                fm,                        // dstBlockStride
                fm,                        // src0BlockStride
                0,                        // src1BlockStride
                1,  // dstRepeatStride
                1,  // src0RepeatStride
                1                         // src1RepeatStride
            );
            __set_mask(VECTOR_SIZE);
        }
        PIPE_BARRIER(V);
#if __CCE_AICORE__ == 100
                SET_FLAG(V, MTE2, EVENT_ID0);
#else
                SET_FLAG(V, MTE2, EVENT_ID6);
#endif
    }


    // 3.1. mask(attention score * tor)
    muls_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                        lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], tor,
                                        pSize / 128,             // repeat
                                        1,                       // dstBlockStride
                                        1,                       // srcBlockStride
                                        uint16_t(8), uint16_t(8) // dstRepeatStride
    );
    PIPE_BARRIER(V);
    // Must Sync, otherwise error
    WAIT_FLAG(V, MTE1, EVENT_ID0);
    // Mask Ping Load UB
    if ((gmSrcm != nullptr) && (add_mask_n0 == 1)) {
#if __CCE_AICORE__ != 100
        WAIT_FLAG(MTE2, MTE1, Pingflag + 2);
#endif
        l1_to_ub<ArchType::ASCEND_V200, half>(loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                                              l1maskBufAddr_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                              1,                    // nBurst, 次数
                                              fm * fn / BLOCK_SIZE, // lenBurst
                                              0,                    // srcStride，尾-头,32byte
                                              0);
        SET_FLAG(MTE1, MTE2, Pingflag + 2);
        SET_FLAG(MTE1, V, Pingflag);
        WAIT_FLAG(MTE1, V, Pingflag);
        if (gmSrcAlibiCoeff != nullptr) {
            if (srcmOffset0) {
                if (isSqrt == 1) {
                    mul_v<ArchType::ASCEND_V200, half>(
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        fm * fn / VECTOR_SIZE, // repeat
                        1,                     // dstBlockStride
                        1,                     // src0BlockStride
                        1,                     // src1BlockStride
                        8,                     // dstRepeatStride
                        8,                     // src0RepeatStride
                        8                      // src1RepeatStride
                    );
                    PIPE_BARRIER(V);
                }
                adds_v<ArchType::ASCEND_V200, half>(
                    loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                    loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE], (half)delta0,
                    fm * fn / VECTOR_SIZE, // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    8,                     // dstRepeatStride
                    8                      // src0RepeatStride
                );
                PIPE_BARRIER(V);
                if (isSqrt == 1) {
                    sqrt_v<ArchType::ASCEND_V200, half>(
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                        fm * fn / VECTOR_SIZE, // repeat
                        1,                     // dstBlockStride
                        1,                     // src0BlockStride
                        8,                     // dstRepeatStride
                        8                      // src0RepeatStride
                    );
                    PIPE_BARRIER(V);
                }
            }
            muls_v<ArchType::ASCEND_V200, half>(loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                                                loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                                                (half)alibi_coeff,
                                                fm * fn / VECTOR_SIZE, // repeat
                                                1,                     // dstBlockStride
                                                1,                     // src0BlockStride
                                                8,                     // dstRepeatStride
                                                8                      // src0RepeatStride
            );
            PIPE_BARRIER(V);
        }
        add_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                           lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                           loUbuf_tensor.ReinterpretCast<half>()[Pingflag * L0AB_HALF_BUF_SIZE],
                                           fm * fn / VECTOR_SIZE, // repeat
                                           1,                     // dstBlockStride
                                           1,                     // src0BlockStride
                                           1,                     // src1BlockStride
                                           8,                     // dstRepeatStride
                                           8,                     // src0RepeatStride
                                           8                      // src1RepeatStride
        );

        PIPE_BARRIER(V);
    }
    // 3. softmax part
    if (__n0 / BLOCK_SIZE > 1) { // 前两个(fm, 16)求最大值
        max_v<ArchType::ASCEND_V200, half>(tvUbuf_tensor, lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                           lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + fm * BLOCK_SIZE],
                                           fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                           1,                             // dstBlockStride
                                           1,                             // src0BlockStride
                                           1,                             // src1BlockStride
                                           8,                             // dstRepeatStride
                                           8,                             // src0RepeatStride
                                           8                              // src1RepeatStride
        );
        PIPE_BARRIER(V);
    } else {
        ub_to_ub<ArchType::ASCEND_V200, half>(tvUbuf_tensor, lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                              0,  // sid
                                              1,  // nBurst
                                              fm, // lenBurst
                                              0,  // srcGap
                                              0   // dstGap
        );

        PIPE_BARRIER(V);
    }
    for (int32_t rowmaxIdx = 2; rowmaxIdx < (__n0 / BLOCK_SIZE); ++rowmaxIdx) { // 循环比较(fm, 16)
        max_v<ArchType::ASCEND_V200, half>(tvUbuf_tensor, tvUbuf_tensor,
                                           lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + rowmaxIdx * fm * BLOCK_SIZE],
                                           fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                           1,                             // dstBlockStride
                                           1,                             // src0BlockStride
                                           1,                             // src1BlockStride
                                           8,                             // dstRepeatStride
                                           8,                             // src0RepeatStride
                                           8                              // src1RepeatStride
        );
        PIPE_BARRIER(V);
    }
    if (__n0 % BLOCK_SIZE > 0) {
        __set_mask(__n0 % BLOCK_SIZE);
        if (__n0 / BLOCK_SIZE > 0) {
            max_v<ArchType::ASCEND_V200, half>(
                tvUbuf_tensor, tvUbuf_tensor,
                lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + __n0 / BLOCK_SIZE * fm * BLOCK_SIZE],
                fm, // repeat
                1,  // dstBlockStride
                1,  // src0BlockStride
                1,  // src1BlockStride
                1,  // dstRepeatStride
                1,  // src0RepeatStride
                1   // src1RepeatStride
            );

            PIPE_BARRIER(V);
            set_vector_mask((uint64_t)-1, (uint64_t)-1);
        }
    }
    if (__n0 < BLOCK_SIZE) {
        __set_vcg_mask(__n0);
    }
    cgmax_v<ArchType::ASCEND_V200, half>(lmUbuf_tensor, tvUbuf_tensor, fm * BLOCK_SIZE / VECTOR_SIZE, 1, 1, 8);
    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
    PIPE_BARRIER(V);

    if (initGgDm == 0) { // 需要update m_j
        max_v<ArchType::ASCEND_V200, half>(hmUbuf_tensor, lmUbuf_tensor, gmUbuf_tensor,
                                           mD128, // repeat
                                           1,     // dstBlockStride
                                           1,     // src0BlockStride
                                           1,     // src1BlockStride
                                           8,     // dstRepeatStride
                                           8,     // src0RepeatStride
                                           8      // src1RepeatStride
        );
        PIPE_BARRIER(V);
        sub_v<ArchType::ASCEND_V200, half>(dmUbuf_tensor[Pingflag * UB_HALF_LINE_SIZE], gmUbuf_tensor, hmUbuf_tensor,
                                           mD128, // repeat
                                           1,     // dstBlockStride
                                           1,     // src0BlockStride
                                           1,     // src1BlockStride
                                           8,     // dstRepeatStride
                                           8,     // src0RepeatStride
                                           8      // src1RepeatStride
        );

        PIPE_BARRIER(V);
    } else {
        ub_to_ub<ArchType::ASCEND_V200, half>(hmUbuf_tensor, lmUbuf_tensor,
                                              0,               // sid
                                              1,               // nBurst
                                              fm / BLOCK_SIZE, // lenBurst
                                              0,               // srcGap
                                              0                // dstGap
        );
        PIPE_BARRIER(V);
    }

    ub_to_ub<ArchType::ASCEND_V200, half>(gmUbuf_tensor, hmUbuf_tensor,
                                          0,               // sid
                                          1,               // nBurst
                                          fm / BLOCK_SIZE, // lenBurst
                                          0,               // srcGap
                                          0                // dstGap
    );

    PIPE_BARRIER(V);
    ExpandToBlockHalf(tvUbuf_tensor, hmUbuf_tensor, fm);                // (fm,) -> (fm, 16)
    for (int32_t vsubIdx = 0; vsubIdx < (fn / BLOCK_SIZE); ++vsubIdx) { // (fm, fn)
        sub_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                                           lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                                           tvUbuf_tensor,
                                           fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                           1,                             // dstBlockStride
                                           1,                             // src0BlockStride
                                           1,                             // src1BlockStride
                                           8,                             // dstRepeatStride
                                           8,                             // src0RepeatStride
                                           8                              // src1RepeatStride
        );
    }

    PIPE_BARRIER(V);
    
    if (initGgDm == 0) {
        conv_v<ArchType::ASCEND_V200, half, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                                   dmUbuf_tensor[Pingflag * UB_HALF_LINE_SIZE], mD64, 1, 1, uint16_t(8),
                                                   uint16_t(4));
    }
    // 2 for Repeatimes above 255
    for (int32_t vconvIdx = 0; vconvIdx < 2; ++vconvIdx) {
        conv_v<ArchType::ASCEND_V200, half, float>(ls32Ubuf_tensor[vconvIdx * pSize / 2],
                                                   lsUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + vconvIdx * pSize / 2],
                                                   pSize / 2 / FLOAT_VECTOR_SIZE, // repeat
                                                   1,                             // dstBlockStride
                                                   1,                             // srcBlockStride
                                                   uint16_t(8),                   // dstRepeatStride
                                                   uint16_t(4)                    // srcRepeatStride
        );
    }

    PIPE_BARRIER(V);
    // 2 for Repeatimes above 255

    if (initGgDm == 0) {
        AscendC::Duplicate<half>(diagUbuf_tensor, half(0), fm * fm);

        exp_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                            tvUbuf_tensor.ReinterpretCast<float>(), mD64, 1, 1, uint16_t(8),
                                            uint16_t(8));

        PIPE_BARRIER(V);
 
        conv_v<ArchType::ASCEND_V200, float, half>(dmUbuf_tensor[Pingflag * UB_HALF_LINE_SIZE],
                                                   tvUbuf_tensor.ReinterpretCast<float>(),
                                                   mD64, 1, 1, uint16_t(4),
                                                   uint16_t(8));

        mul_v<ArchType::ASCEND_V200, float>(glUbuf_tensor, tvUbuf_tensor.ReinterpretCast<float>(), glUbuf_tensor,
                                            mD64, // repeat
                                            1,    // dstBlockStride
                                            1,    // src0BlockStride
                                            1,    // src1BlockStride
                                            8,    // dstRepeatStride
                                            8,    // src0RepeatStride
                                            8     // src1RepeatStride
        );

        SET_FLAG(V, S, EVENT_ID0);
        WAIT_FLAG(V, S, EVENT_ID0);
    }

    for (int32_t vexpIdx = 0; vexpIdx < 2; ++vexpIdx) {
        exp_v<ArchType::ASCEND_V200, float>(ls32Ubuf_tensor[vexpIdx * pSize / 2], ls32Ubuf_tensor[vexpIdx * pSize / 2],
                                            pSize / 2 / FLOAT_VECTOR_SIZE, // repeat
                                            1,                             // dstBlockStride
                                            1,                             // srcBlockStride
                                            8,                             // dstRepeatStride
                                            8                              // srcRepeatStride
        );
    }

    if (initGgDm == 0) {

        for (uint32_t count = 0; count < fm; count += 1)
        {            
            // NZ格式下的偏移
            uint32_t diagMatOffset = count / BLOCK_SIZE * BLOCK_SIZE * fm + count * BLOCK_SIZE + count % BLOCK_SIZE;
            diagUbuf_tensor.SetValue(diagMatOffset, dmUbuf_tensor.GetValue(count));
        }
        
        SET_FLAG(S, MTE3, EVENT_ID3);
        WAIT_FLAG(S, MTE3, EVENT_ID3);

        ub_to_l1<ArchType::ASCEND_V200, half>(l1dmDiagPingBuf_tensor,
                                              diagUbuf_tensor, 
                                              1, 
                                              fm * fm / BLOCK_SIZE, 
                                              0,
                                              0);

        SET_FLAG(MTE3, MTE1, Pingflag + 2);
    }

    PIPE_BARRIER(V);

    SET_FLAG(MTE3, V, EVENT_ID3);
    WAIT_FLAG(MTE3, V, EVENT_ID3);
    
    // 2 for Repeatimes above 255
    for (int32_t vconvIdx = 0; vconvIdx < 2; ++vconvIdx) {
        conv_v<ArchType::ASCEND_V200, float, half>(lpUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + vconvIdx * pSize / 2],
                                                   ls32Ubuf_tensor[vconvIdx * pSize / 2],
                                                   pSize / 2 / FLOAT_VECTOR_SIZE, // repeat
                                                   1,                             // dstBlockStride
                                                   1,                             // srcBlockStride
                                                   4,                             // dstRepeatStride
                                                   8                              // srcRepeatStride
        );
    }
    PIPE_BARRIER(V);
    SET_FLAG(V, MTE3, Pingflag);
    if (__n0 / BLOCK_SIZE > 1) {
        add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), ls32Ubuf_tensor,
                                            ls32Ubuf_tensor[fm * BLOCK_SIZE],
                                            fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                            1,                                   // dstBlockStride
                                            1,                                   // src0BlockStride
                                            1,                                   // src1BlockStride
                                            8,                                   // dstRepeatStride
                                            8,                                   // src0RepeatStride
                                            8                                    // src1RepeatStride
        );
        PIPE_BARRIER(V);
    } else {
        ub_to_ub<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), ls32Ubuf_tensor,
                                               0,                   // sid
                                               1,                   // nBurst
                                               fm * BLOCK_SIZE / 8, // lenBurst
                                               0,                   // srcGap
                                               0                    // dstGap
        );
        PIPE_BARRIER(V);
    }
    for (int32_t rowsumIdx = 2; rowsumIdx < (__n0 / BLOCK_SIZE); ++rowsumIdx) {
        add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                            tvUbuf_tensor.ReinterpretCast<float>(),
                                            ls32Ubuf_tensor[rowsumIdx * fm * BLOCK_SIZE],
                                            fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                            1,                                   // dstBlockStride
                                            1,                                   // src0BlockStride
                                            1,                                   // src1BlockStride
                                            8,                                   // dstRepeatStride
                                            8,                                   // src0RepeatStride
                                            8                                    // src1RepeatStride
        );
        PIPE_BARRIER(V);
    }
    SetMasknorm();
    if (__n0 % BLOCK_SIZE > 0) {
        __set_mask(__n0 % BLOCK_SIZE);
        if (__n0 / BLOCK_SIZE > 0) {
            add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                                tvUbuf_tensor.ReinterpretCast<float>(),
                                                ls32Ubuf_tensor[__n0 / BLOCK_SIZE * fm * BLOCK_SIZE],
                                                fm, // repeat
                                                1,  // dstBlockStride
                                                1,  // src0BlockStride
                                                1,  // src1BlockStride
                                                2,  // dstRepeatStride
                                                2,  // src0RepeatStride
                                                2   // src1RepeatStride
            );
            PIPE_BARRIER(V);
            SetVectorMask<int8_t>(0x0, 0xffff);
        }
    } else {
        SetVectorMask<int8_t>(0x0, 0xffff);
    }

    cadd_v<ArchType::ASCEND_V200, float>(llUbuf_tensor[Pingflag * UB_FLOAT_LINE_SIZE],
                                         tvUbuf_tensor.ReinterpretCast<float>(), fm,
                                         1,  // dstRepeatStride
                                         1,  // srcBlockStride
                                         2); // srcRepeatStride, fp32 2 block

    PIPE_BARRIER(V);
    // Must Sync, otherwise error
    SET_FLAG(V, MTE1, EVENT_ID0);
    SetMasknorm();
    SetVectorMask<int8_t>(0xffffffffffffffff, 0xffffffffffffffff);

    if (initGgDm == 1) {
        ub_to_ub<ArchType::ASCEND_V200, float>(glUbuf_tensor, llUbuf_tensor[Pingflag * UB_FLOAT_LINE_SIZE],
                                               0,      // sid
                                               1,      // nBurst
                                               fm / 8, // lenBurst
                                               0,      // srcGap
                                               0       // dstGap
        );
    } else if (initGgDm == 0) {
        add_v<ArchType::ASCEND_V200, float>(glUbuf_tensor, glUbuf_tensor, llUbuf_tensor[Pingflag * UB_FLOAT_LINE_SIZE],
                                            mD64, // repeat
                                            1,    // dstBlockStride
                                            1,    // src0BlockStride
                                            1,    // src1BlockStride
                                            8,    // dstRepeatStride
                                            8,    // src0RepeatStride
                                            8     // src1RepeatStride
        );
    }

    PIPE_BARRIER(V);
    // 3. ################ Softmax Ping Ends #######################

    // 5. ################ Bmm2 Ping Starts #######################
#if __CCE_AICORE__ == 100
    // V PRELOAD PING for 910
    WAIT_FLAG(MTE1, MTE2, Pingflag + 2);
    if (kvCopyStride <= STRIDE_UPPER_BOUND + fn) {
        gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
            l1vPingBuf_tensor, gmSrcv_tensor[(int64_t)srcvOffset], fn, fn, kvCopyStride, fk, fk, fk);
    } else {
        for (int32_t l1vBurstIdx = 0; l1vBurstIdx < (fk / BLOCK_SIZE); ++l1vBurstIdx) {
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1vPingBuf_tensor[l1vBurstIdx * fn * BLOCK_SIZE],
                gmSrcv_tensor[(int64_t)srcvOffset + l1vBurstIdx * kvCopyStride * BLOCK_SIZE], fn, fn, fn, BLOCK_SIZE,
                BLOCK_SIZE, BLOCK_SIZE);
        }
    }
    SET_FLAG(MTE2, MTE1, Pingflag + 2);
    WAIT_FLAG(MTE2, MTE1, Pingflag + 2);
#else
    WAIT_FLAG(MTE2, MTE1, Pingflag + 4);
#endif
    WAIT_FLAG(M, MTE1, Pingflag + 2);
    // 16 is blocksize in format zN
    if (fk == 16) {
        l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
            l0bBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l1vPingBuf_tensor, 0,
            fn / BLOCK_SIZE, // repeat
            0,
            1, // srcStride
            0,
            0 // dstStride
        );
    } else {
        for (int32_t l0bLoadIdx = 0; l0bLoadIdx < (fn / BLOCK_SIZE); ++l0bLoadIdx) { // Nz -> nZ
            l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0bBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + l0bLoadIdx * fk * BLOCK_SIZE],
                l1vPingBuf_tensor[l0bLoadIdx * CUBE_MATRIX_SIZE], 0,
                fk / BLOCK_SIZE, // repeat
                0,
                fn / BLOCK_SIZE, // srcStride
                0,
                0 // dstStride
            );
        }
    }
    SET_FLAG(MTE1, M, Pingflag + 2);
#if __CCE_AICORE__ == 100
    SET_FLAG(MTE1, MTE2, Pingflag + 2);
#else
    SET_FLAG(MTE1, MTE2, Pingflag + 6);
#endif
    // BMM2 P LOAD Ping
    WAIT_FLAG(V, MTE3, Pingflag);
    WAIT_FLAG(MTE1, MTE3, Pingflag);
    if (__m0 == 1) {
        ub_to_l1<ArchType::ASCEND_V200, half>(l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                              lpUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], fn / BLOCK_SIZE, 1, fm - 1,
                                              0);
    } else {
        ub_to_l1<ArchType::ASCEND_V200, half>(l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                              lpUbuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 1, pSize / BLOCK_SIZE, 0,
                                              0);
    }
    SET_FLAG(MTE3, V, Pingflag);
    SET_FLAG(MTE3, MTE1, Pingflag);
    WAIT_FLAG(MTE3, MTE1, Pingflag);
    WAIT_FLAG(M, MTE1, Pingflag);
    // 16 is blocksize in format zN
    if (__m0 == 1) {
        l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
            l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 0,
            1, // repeat
            0,
            1, // srcStride
            0,
            0 // dstStride
        );
    } else if (fn == BLOCK_SIZE) {
        l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
            l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 0,
            fm / BLOCK_SIZE, // repeat
            0,
            1, // srcStride
            0,
            0 // dstStride
        );
    } else {
        for (int32_t l0aLoadIdx = 0; l0aLoadIdx < (fm / BLOCK_SIZE); ++l0aLoadIdx) { // (fm, fn)
            l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * fn * BLOCK_SIZE],
                l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * CUBE_MATRIX_SIZE], 0,
                fn / BLOCK_SIZE, // repeat
                0,
                fm / BLOCK_SIZE, // srcStride
                0,
                0 // dstStride
            );
        }
    }
    SET_FLAG(MTE1, M, Pingflag);
    SET_FLAG(MTE1, MTE3, Pingflag);
    WAIT_FLAG(MTE1, M, Pingflag);
    // 4. bmm2 partr
    WAIT_FLAG(MTE1, M, Pingflag + 2);
    WAIT_FLAG(V, M, Pingflag + 2);

    mmad<ArchType::ASCEND_V200, __fp16, __fp16, float, false>(
        l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
        l0aBuf_tensor.ReinterpretCast<__fp16>()[Pingflag * L0AB_HALF_BUF_SIZE],
        l0bBuf_tensor.ReinterpretCast<__fp16>()[Pingflag * L0AB_HALF_BUF_SIZE], __m0, fk, __n0, 1);

    SET_FLAG(M, MTE1, Pingflag);
    SET_FLAG(M, MTE1, Pingflag + 2);

    // online softmax
    if (initGgDm == 0) {
        WAIT_FLAG(MTE3, MTE1, Pingflag + 2);
        WAIT_FLAG(M, MTE1, Pingflag);
        if (__m0 == 1) {
            l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 
                l1dmDiagPingBuf_tensor, 
                0,
                1, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );

        } else {
            for (int32_t l0aLoadIdx = 0; l0aLoadIdx < (fm / BLOCK_SIZE); ++l0aLoadIdx) { // (fm, fk) Nz-> zZ
                l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * fm * BLOCK_SIZE],
                    l1dmDiagPingBuf_tensor[l0aLoadIdx * CUBE_MATRIX_SIZE], 
                    0,
                    fm / BLOCK_SIZE, // repeat
                    0,
                    fm / BLOCK_SIZE, // srcStride
                    0,
                    0 // dstStride
                );
            }
        }

        // WAIT_FLAG(MTE3, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, Pingflag + 2);
        if (fk == BLOCK_SIZE) {
            l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0bBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 
                l1oTempBuf_tensor, 
                0,
                fm / BLOCK_SIZE, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );
        } else {
            for (int32_t l0bLoadIdx = 0; l0bLoadIdx < (fm / BLOCK_SIZE); ++l0bLoadIdx) { // Nz -> nZ
                l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0bBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + l0bLoadIdx * fk * BLOCK_SIZE],
                    l1oTempBuf_tensor[l0bLoadIdx * CUBE_MATRIX_SIZE], 
                    0,
                    fk / BLOCK_SIZE, // repeat
                    0,
                    fm / BLOCK_SIZE, // srcStride
                    0,
                    0 // dstStride
                );
            }
        }

        SET_FLAG(MTE1, M, Pingflag);
        WAIT_FLAG(MTE1, M, Pingflag);

        mmad<ArchType::ASCEND_V200, __fp16, __fp16, float, false>(
            l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
            l0aBuf_tensor.ReinterpretCast<__fp16>()[Pingflag * L0AB_HALF_BUF_SIZE],
            l0bBuf_tensor.ReinterpretCast<__fp16>()[Pingflag * L0AB_HALF_BUF_SIZE], 
            __m0, 
            fk, 
            __m0, 
            0
        );

        SET_FLAG(M, MTE1, Pingflag);
        SET_FLAG(M, MTE1, Pingflag + 2);
    }
    
    SET_FLAG(M, V, Pingflag);
    if (wrapO == 1) {
        SET_FLAG(MTE1, MTE2, Pingflag);
        if (__n1 == -1) {
            SET_FLAG(MTE1, MTE2, Pongflag);
        }
    } 

    if (__n1 != -1) {
        WAIT_FLAG(M, V, Pingflag);
        l0c_to_ub<ArchType::ASCEND_V200, float, half>(toUbuf_tensor, l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 1,
                                                       oSize / CUBE_MATRIX_SIZE, 0, 0);

        SET_FLAG(V, MTE3, Pingflag);
        WAIT_FLAG(V, MTE3, Pingflag);

        ub_to_l1<ArchType::ASCEND_V200, half>(l1oTempBuf_tensor,
                                              toUbuf_tensor, 
                                              1, 
                                              oSize / BLOCK_SIZE,
                                              0,
                                              0);

        SET_FLAG(MTE3, MTE1, EVENT_ID0);
    }
    
    // 5. ################ Bmm2 Ping Ends #######################
    initGgDm = 0;
    // 4. ################ Softmax Pong Starts #######################
    if (__n1 != -1) {
        WAIT_FLAG(M, V, Pongflag);
        WAIT_FLAG(MTE3, V, Pongflag);
        l0c_to_ub<ArchType::ASCEND_V200, float, half>(lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                                      l0cBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 1,
                                                      pSize_b / CUBE_MATRIX_SIZE, 0, 0);
        PIPE_BARRIER(V);
        SET_FLAG(V, M, Pongflag);

        if (scaleType == 1) {
            // expand logn to block
            ExpandToBlockHalf(tvUbuf_tensor, logn_ub_tensor, fm); // (fm,) -> (fm, 16)
            PIPE_BARRIER(V);
            // loop for column, repeat for row
            for (uint32_t fn_block_idx = 0; fn_block_idx < (__n1 / VECTOR_SIZE); ++fn_block_idx) {
                mul_v<ArchType::ASCEND_V200, half>(
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + fn_block_idx * fm * VECTOR_SIZE],
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + fn_block_idx * fm * VECTOR_SIZE],
                    tvUbuf_tensor.ReinterpretCast<half>(),
                    __m0,                    // repeat
                    fm,                        // dstBlockStride
                    fm,                        // src0BlockStride
                    0,                        // src1BlockStride
                    1,  // dstRepeatStride
                    1,  // src0RepeatStride
                    1                         // src1RepeatStride
                );
            }
            if (__n1 % VECTOR_SIZE > 0) {
                __set_mask(__n1 % VECTOR_SIZE);
                mul_v<ArchType::ASCEND_V200, half>(
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + __n1 / VECTOR_SIZE * fm * VECTOR_SIZE],
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + __n1 / VECTOR_SIZE * fm * VECTOR_SIZE],
                    tvUbuf_tensor.ReinterpretCast<half>(),
                    __m0,                    // repeat
                    fm,                        // dstBlockStride
                    fm,                        // src0BlockStride
                    0,                        // src1BlockStride
                    1,  // dstRepeatStride
                    1,  // src0RepeatStride
                    1                         // src1RepeatStride
                );
                __set_mask(VECTOR_SIZE);
            }
            PIPE_BARRIER(V);
        }

        // 3.1. mask(attention score * tor)
        muls_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                            lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], tor,
                                            pSize_b / 128, // repeat
                                            1,             // dstBlockStride
                                            1,             // srcBlockStride
                                            uint16_t(8),   // dstRepeatStride
                                            uint16_t(8)    // srcRepeatStride
        );
        PIPE_BARRIER(V);
        WAIT_FLAG(V, MTE1, EVENT_ID0);
        if ((gmSrcm != nullptr) && (add_mask_n1 == 1)) {
            // Mask Pong Load UB
#if __CCE_AICORE__ != 100
            WAIT_FLAG(MTE2, MTE1, Pongflag + 2);
#endif
            l1_to_ub<ArchType::ASCEND_V200, half>(loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                                                  l1maskBufAddr_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                                  1,                    // nBurst, 次数
                                                  fm * bn / BLOCK_SIZE, // lenBurst
                                                  0,                    // srcStride，尾-头,32byte
                                                  0);
            SET_FLAG(MTE1, MTE2, Pongflag + 2);
            SET_FLAG(MTE1, V, Pongflag);
            WAIT_FLAG(MTE1, V, Pongflag);
            if (gmSrcAlibiCoeff != nullptr) {
                if (srcmOffset1) {
                    if (isSqrt == 1) {
                        mul_v<ArchType::ASCEND_V200, half>(
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            fm * fn / VECTOR_SIZE, // repeat
                            1,                     // dstBlockStride
                            1,                     // src0BlockStride
                            1,                     // src1BlockStride
                            8,                     // dstRepeatStride
                            8,                     // src0RepeatStride
                            8                      // src1RepeatStride
                        );
                        PIPE_BARRIER(V);
                    }
                    adds_v<ArchType::ASCEND_V200, half>(
                        loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                        loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE], (half)delta1,
                        fm * fn / VECTOR_SIZE, // repeat
                        1,                     // dstBlockStride
                        1,                     // src0BlockStride
                        8,                     // dstRepeatStride
                        8                      // src0RepeatStride
                    );
                    PIPE_BARRIER(V);
                    if (isSqrt == 1) {
                        sqrt_v<ArchType::ASCEND_V200, half>(
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                            fm * fn / VECTOR_SIZE, // repeat
                            1,                     // dstBlockStride
                            1,                     // src0BlockStride
                            8,                     // dstRepeatStride
                            8                      // src0RepeatStride
                        );
                        PIPE_BARRIER(V);
                    }
                }
                muls_v<ArchType::ASCEND_V200, half>(
                    loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                    loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE], (half)alibi_coeff,
                    fm * fn / VECTOR_SIZE, // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    8,                     // dstRepeatStride
                    8                      // src0RepeatStride
                );
                PIPE_BARRIER(V);
            }
            add_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                               lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                               loUbuf_tensor.ReinterpretCast<half>()[Pongflag * L0AB_HALF_BUF_SIZE],
                                               fm * bn / VECTOR_SIZE, // repeat
                                               1,                     // dstBlockStride
                                               1,                     // src0BlockStride
                                               1,                     // src1BlockStride
                                               8,                     // dstRepeatStride
                                               8,                     // src0RepeatStride
                                               8                      // src1RepeatStride
            );
            PIPE_BARRIER(V);
        }
        // 3. softmax part
        if (__n1 / BLOCK_SIZE > 1) { // 前两个(fm, 16)求最大值
            max_v<ArchType::ASCEND_V200, half>(tvUbuf_tensor, lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                               lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + fm * BLOCK_SIZE],
                                               fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                               1,                             // dstBlockStride
                                               1,                             // src0BlockStride
                                               1,                             // src1BlockStride
                                               8,                             // dstRepeatStride
                                               8,                             // src0RepeatStride
                                               8                              // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, half>(tvUbuf_tensor, lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                                  0,  // sid
                                                  1,  // nBurst
                                                  fm, // lenBurst
                                                  0,  // srcGap
                                                  0   // dstGap
            );

            PIPE_BARRIER(V);
        }
        for (int32_t rowmaxIdx = 2; rowmaxIdx < (__n1 / BLOCK_SIZE); ++rowmaxIdx) { // 循环比较(fm, 16)
            max_v<ArchType::ASCEND_V200, half>(
                tvUbuf_tensor, tvUbuf_tensor,
                lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + rowmaxIdx * fm * BLOCK_SIZE],
                fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                1,                             // dstBlockStride
                1,                             // src0BlockStride
                1,                             // src1BlockStride
                8,                             // dstRepeatStride
                8,                             // src0RepeatStride
                8                              // src1RepeatStride
            );

            PIPE_BARRIER(V);
        }
        if (__n1 % BLOCK_SIZE > 0) {
            __set_mask(__n1 % BLOCK_SIZE);
            if (__n1 / BLOCK_SIZE > 0) {
                max_v<ArchType::ASCEND_V200, half>(
                    tvUbuf_tensor, tvUbuf_tensor,
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + __n1 / BLOCK_SIZE * fm * BLOCK_SIZE],
                    fm, // repeat
                    1,  // dstBlockStride
                    1,  // src0BlockStride
                    1,  // src1BlockStride
                    1,  // dstRepeatStride
                    1,  // src0RepeatStride
                    1   // src1RepeatStride
                );
                max_v<ArchType::ASCEND_V200, half>(
                    tvUbuf_tensor, tvUbuf_tensor,
                    lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + __n1 / BLOCK_SIZE * fm * BLOCK_SIZE], fm, 1, 1, 1, 1,
                    1, 1);
                PIPE_BARRIER(V);
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
        }
        if (__n1 < BLOCK_SIZE) {
            __set_vcg_mask(__n1);
        }
        cgmax_v<ArchType::ASCEND_V200, half>(lmUbuf_tensor, tvUbuf_tensor, fm * BLOCK_SIZE / VECTOR_SIZE, 1, 1, 8);
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        PIPE_BARRIER(V);

        if (initGgDm == 0) { // 需要update m_j
            max_v<ArchType::ASCEND_V200, half>(hmUbuf_tensor, lmUbuf_tensor, gmUbuf_tensor,
                                               mD128, // repeat
                                               1,     // dstBlockStride
                                               1,     // src0BlockStride
                                               1,     // src1BlockStride
                                               8,     // dstRepeatStride
                                               8,     // src0RepeatStride
                                               8      // src1RepeatStride
            );

            PIPE_BARRIER(V);
            sub_v<ArchType::ASCEND_V200, half>(dmUbuf_tensor[Pongflag * UB_HALF_LINE_SIZE], gmUbuf_tensor,
                                               hmUbuf_tensor,
                                               mD128, // repeat
                                               1,     // dstBlockStride
                                               1,     // src0BlockStride
                                               1,     // src1BlockStride
                                               8,     // dstRepeatStride
                                               8,     // src0RepeatStride
                                               8      // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, half>(hmUbuf_tensor, lmUbuf_tensor,
                                                  0,               // sid
                                                  1,               // nBurst
                                                  fm / BLOCK_SIZE, // lenBurst
                                                  0,               // srcGap
                                                  0                // dstGap
            );
            PIPE_BARRIER(V);
        }
        ub_to_ub<ArchType::ASCEND_V200, half>(gmUbuf_tensor, hmUbuf_tensor,
                                              0,               // sid
                                              1,               // nBurst
                                              fm / BLOCK_SIZE, // lenBurst
                                              0,               // srcGap
                                              0                // dstGap
        );
        ub_to_ub<ArchType::ASCEND_V200, half>(gmUbuf_tensor, hmUbuf_tensor, 0, 1, fm / BLOCK_SIZE, 0, 0);
        PIPE_BARRIER(V);
        ExpandToBlockHalf(tvUbuf_tensor, hmUbuf_tensor, fm);                // (fm,) -> (fm, 16)
        for (int32_t vsubIdx = 0; vsubIdx < (bn / BLOCK_SIZE); ++vsubIdx) { // (fm, bn)
            sub_v<ArchType::ASCEND_V200, half>(lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                                               lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + vsubIdx * fm * BLOCK_SIZE],
                                               tvUbuf_tensor,
                                               fm * BLOCK_SIZE / VECTOR_SIZE, // repeat
                                               1,                             // dstBlockStride
                                               1,                             // src0BlockStride
                                               1,                             // src1BlockStride
                                               8,                             // dstRepeatStride
                                               8,                             // src0RepeatStride
                                               8                              // src1RepeatStride
            );
        }
        PIPE_BARRIER(V);
        // 2 for Repeatimes above 255
        for (int32_t vconvIdx = 0; vconvIdx < 2; ++vconvIdx) {
            conv_v<ArchType::ASCEND_V200, half, float>(
                ls32Ubuf_tensor[vconvIdx * pSize_b / 2],
                lsUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + vconvIdx * pSize_b / 2],
                pSize_b / 2 / FLOAT_VECTOR_SIZE, // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                uint16_t(8),                     // dstRepeatStride
                uint16_t(4)                      // srcRepeatStride
            );
        }

        if (initGgDm == 0) {
            conv_v<ArchType::ASCEND_V200, half, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                                       dmUbuf_tensor[Pongflag * UB_HALF_LINE_SIZE], mD64, 1, 1, uint16_t(8),
                                                       uint16_t(4));
        }
        PIPE_BARRIER(V);

        if (initGgDm == 0) {
            AscendC::Duplicate<half>(diagUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], half(0), fm * fm);
            
            exp_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                                tvUbuf_tensor.ReinterpretCast<float>(), mD64, 1, 1, uint16_t(8),
                                                uint16_t(8));

            PIPE_BARRIER(V);

            conv_v<ArchType::ASCEND_V200, float, half>(dmUbuf_tensor[Pongflag * UB_HALF_LINE_SIZE],
                                                       tvUbuf_tensor.ReinterpretCast<float>(),
                                                       mD64, 1, 1, uint16_t(4),
                                                       uint16_t(8));

            mul_v<ArchType::ASCEND_V200, float>(glUbuf_tensor, 
                                                tvUbuf_tensor.ReinterpretCast<float>(), 
                                                glUbuf_tensor,
                                                mD64, // repeat
                                                1,    // dstBlockStride
                                                1,    // src0BlockStride
                                                1,    // src1BlockStride
                                                8,    // dstRepeatStride
                                                8,    // src0RepeatStride
                                                8     // src1RepeatStride
            );

            SET_FLAG(V, S, EVENT_ID0);
            WAIT_FLAG(V, S, EVENT_ID0);
        }

        for (int32_t vexpIdx = 0; vexpIdx < 2; ++vexpIdx) {
            exp_v<ArchType::ASCEND_V200, float>(ls32Ubuf_tensor[vexpIdx * pSize_b / 2],
                                                ls32Ubuf_tensor[vexpIdx * pSize_b / 2],
                                                pSize_b / 2 / FLOAT_VECTOR_SIZE, // repeat
                                                1,                               // dstBlockStride
                                                1,                               // srcBlockStride
                                                8,                               // dstRepeatStride
                                                8                                // srcRepeatStride
            );
        }

        if (initGgDm == 0) {

            for (uint32_t count = 0; count < fm; count += 1)
            {            
                // NZ格式偏移
                uint32_t diagMatOffset = Pongflag * L0AB_HALF_BUF_SIZE + count / BLOCK_SIZE * BLOCK_SIZE * fm + count * BLOCK_SIZE + count % BLOCK_SIZE;
                diagUbuf_tensor.SetValue(diagMatOffset, dmUbuf_tensor.GetValue(Pongflag * UB_HALF_LINE_SIZE + count));
            }

            SET_FLAG(S, MTE3, EVENT_ID3);
            WAIT_FLAG(S, MTE3, EVENT_ID3);

            ub_to_l1<ArchType::ASCEND_V200, half>(l1dmDiagPongBuf_tensor,
                                                  diagUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 
                                                  1, 
                                                  fm * fm / BLOCK_SIZE, 
                                                  0,
                                                  0);

            SET_FLAG(MTE3, MTE1, Pongflag + 2);
        }

        PIPE_BARRIER(V);
        SET_FLAG(MTE3, V, EVENT_ID3);
        WAIT_FLAG(MTE3, V, EVENT_ID3);

        // 2 for double buffer
        for (int32_t vconvIdx = 0; vconvIdx < 2; ++vconvIdx) {
            conv_v<ArchType::ASCEND_V200, float, half>(
                lpUbuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + vconvIdx * pSize_b / 2],
                ls32Ubuf_tensor[vconvIdx * pSize_b / 2],
                pSize_b / 2 / FLOAT_VECTOR_SIZE, // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                4,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
        }
        PIPE_BARRIER(V);
        SET_FLAG(V, MTE3, Pongflag);
        if (__n1 / BLOCK_SIZE > 1) {
            add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), ls32Ubuf_tensor,
                                                ls32Ubuf_tensor[fm * BLOCK_SIZE],
                                                fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                                1,                                   // dstBlockStride
                                                1,                                   // src0BlockStride
                                                1,                                   // src1BlockStride
                                                8,                                   // dstRepeatStride
                                                8,                                   // src0RepeatStride
                                                8                                    // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            ub_to_ub<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(), ls32Ubuf_tensor,
                                                   0,                   // sid
                                                   1,                   // nBurst
                                                   fm * BLOCK_SIZE / 8, // lenBurst
                                                   0,                   // srcGap
                                                   0                    // dstGap
            );
            PIPE_BARRIER(V);
        }
        for (int32_t rowsumIdx = 2; rowsumIdx < (__n1 / BLOCK_SIZE); ++rowsumIdx) {
            add_v<ArchType::ASCEND_V200, float>(tvUbuf_tensor.ReinterpretCast<float>(),
                                                tvUbuf_tensor.ReinterpretCast<float>(),
                                                ls32Ubuf_tensor[rowsumIdx * fm * BLOCK_SIZE],
                                                fm * BLOCK_SIZE / FLOAT_VECTOR_SIZE, // repeat
                                                1,                                   // dstBlockStride
                                                1,                                   // src0BlockStride
                                                1,                                   // src1BlockStride
                                                8,                                   // dstRepeatStride
                                                8,                                   // src0RepeatStride
                                                8                                    // src1RepeatStride
            );
            PIPE_BARRIER(V);
        }
        SetMasknorm();
        if (__n1 % BLOCK_SIZE > 0) {
            __set_mask(__n1 % BLOCK_SIZE);
            if (__n1 / BLOCK_SIZE > 0) {
                add_v<ArchType::ASCEND_V200, float>(
                    tvUbuf_tensor.ReinterpretCast<float>(), tvUbuf_tensor.ReinterpretCast<float>(),
                    ls32Ubuf_tensor[__n1 / BLOCK_SIZE * fm * BLOCK_SIZE], fm, 1, 1, 1, 2, 2, 2);
                PIPE_BARRIER(V);
                SetVectorMask<int8_t>(0x0, 0xffff);
            }
        } else {
            SetVectorMask<int8_t>(0x0, 0xffff);
        }
        cadd_v<ArchType::ASCEND_V200, float>(llUbuf_tensor[Pongflag * UB_FLOAT_LINE_SIZE],
                                             tvUbuf_tensor.ReinterpretCast<float>(), fm,
                                             1,  // dstRepeatStride
                                             1,  // srcBlockStride
                                             2); // srcRepeatStride, fp32 2 block
        PIPE_BARRIER(V);
        SET_FLAG(V, MTE1, EVENT_ID0);
        SetMasknorm();
        SetVectorMask<int8_t>(0xffffffffffffffff, 0xffffffffffffffff);

        if (initGgDm == 1) {
            ub_to_ub<ArchType::ASCEND_V200, float>(glUbuf_tensor, llUbuf_tensor[Pongflag * UB_FLOAT_LINE_SIZE],
                                                    0,      // sid
                                                    1,      // nBurst
                                                    fm / 8, // lenBurst
                                                    0,      // srcGap
                                                    0       // dstGap
            );
        } else if (initGgDm == 0) {
            add_v<ArchType::ASCEND_V200, float>(glUbuf_tensor, glUbuf_tensor, llUbuf_tensor[Pongflag * UB_FLOAT_LINE_SIZE],
                                                mD64, // repeat
                                                1,    // dstBlockStride
                                                1,    // src0BlockStride
                                                1,    // src1BlockStride
                                                8,    // dstRepeatStride
                                                8,    // src0RepeatStride
                                                8     // src1RepeatStride
            );
        }
        initGgDm = 0;
    }
    // 4. ################ Softmax Pong Ends #######################
}

template <typename T, typename SType, PrecType prec_type1, PrecType prec_type2>
__aicore__ inline void UnpadFlashAttentionCommon<T, SType, prec_type1, prec_type2>::FlashAttentionNzPrefillCompute(
    const int32_t fm, const int32_t fn, const int32_t fk, const int32_t bn, const int32_t __m0, const int32_t __n0,
    const int32_t __n1, const int32_t pp_n_scalar, const int32_t q_tight, const int32_t add_mask_n0,
    const int32_t add_mask_n1, const int32_t long_seq, const SType alibi_coeff, const SType delta0, const SType delta1,
    const uint32_t scale_type, const uint32_t alibi_left_align)
{
    int32_t Pingflag = 0; // manual PingPong attempt
    int32_t Pongflag = 1;

    const uint32_t l1q_buf_addr_offset = 0;
    const uint32_t l1kpv_buf_addr_offset = 4 * L1_UINT8_BLOCK_SIZE;
    const uint32_t l1diag_buf_addr_offset = UB_UINT8_BLOCK_SIZE; // 32k

    AscendC::LocalTensor<half> l1qBuf_tensor =
        l1qBufAddr_tensor.ReinterpretCast<uint8_t>()[l1q_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1kPingBuf_tensor =
        l1kBufAddr_tensor.ReinterpretCast<uint8_t>()[Pingflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1kPongBuf_tensor =
        l1kBufAddr_tensor.ReinterpretCast<uint8_t>()[Pongflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1vPingBuf_tensor =
        l1vBufAddr_tensor.ReinterpretCast<uint8_t>()[Pingflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1vPongBuf_tensor =
        l1vBufAddr_tensor.ReinterpretCast<uint8_t>()[Pongflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1pPingBuf_tensor =
        l1pBufAddr_tensor.ReinterpretCast<uint8_t>()[Pingflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1pPongBuf_tensor =
        l1pBufAddr_tensor.ReinterpretCast<uint8_t>()[Pongflag * l1kpv_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1dmDiagPingBuf_tensor = 
        l1diagBufAddr_tensor.ReinterpretCast<uint8_t>()[Pingflag * l1diag_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1dmDiagPongBuf_tensor =
        l1diagBufAddr_tensor.ReinterpretCast<uint8_t>()[Pongflag * l1diag_buf_addr_offset].ReinterpretCast<half>();
    AscendC::LocalTensor<half> l1oTempBuf_tensor = l1oBufAddr_tensor.ReinterpretCast<half>();

    gmSrcq_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(gmSrcq));
    gmSrck_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(gmSrck));
    gmSrcv_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(gmSrcv));
    gmSrcm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(gmSrcm));
    gmDsto_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(gmDsto));

    // 4 for ping-pong memory offset in L1
    __cbuf__ uint8_t *l1qBuf = l1qBufAddr;
    // 4 for ping-pong memory offset in L1
    __cbuf__ uint8_t *l1kPingBuf = l1kBufAddr + Pingflag * 4 * L1_UINT8_BLOCK_SIZE;
    __cbuf__ uint8_t *l1kPongBuf = l1kBufAddr + Pongflag * 4 * L1_UINT8_BLOCK_SIZE;
    // 4 for ping-pong memory offset in L1
    __cbuf__ uint8_t *l1vPingBuf = l1vBufAddr + Pingflag * 4 * L1_UINT8_BLOCK_SIZE;
    __cbuf__ uint8_t *l1vPongBuf = l1vBufAddr + Pongflag * 4 * L1_UINT8_BLOCK_SIZE;
    // 4 for ping-pong memory offset in L1
    __cbuf__ uint8_t *l1pPingBuf = l1pBufAddr + Pingflag * 4 * L1_UINT8_BLOCK_SIZE;
    __cbuf__ uint8_t *l1pPongBuf = l1pBufAddr + Pongflag * 4 * L1_UINT8_BLOCK_SIZE;

    int32_t oSize = fm * fk;
    int32_t mD64 = (fm + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE;
    int32_t mD128 = (fm + VECTOR_SIZE - 1) / VECTOR_SIZE;
    int32_t initGgDm = (initG == 1) ? 1 : 0; // n_idx =0->1
    int32_t initGgO = (initG == 1) ? 1 : 0;

    int32_t pSize = fm * fn;
    int32_t pSize_b = fm * bn;
    // 1. ################ Bmm1 Ping Start #######################
    // 1.1 ################ QK Ping LOAD ################
    if (initGgO != 0) {
        WAIT_FLAG(MTE1, MTE2, Pingflag);
        WAIT_FLAG(MTE1, MTE2, Pongflag);
        if (__m0 == 1) {
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1qBuf_tensor, gmSrcq_tensor[(int64_t)srcqOffset], 1, 1, ntokensQ, fk, fk, fk);
        } else if (ntokensQ <= STRIDE_UPPER_BOUND + fm) { // (fm, fk)
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1qBuf_tensor, gmSrcq_tensor[(int64_t)srcqOffset], fm, fm, ntokensQ, fk, fk, fk);
        } else {
            for (int32_t l1qBurstIdx = 0; l1qBurstIdx < (fk / BLOCK_SIZE); ++l1qBurstIdx) {
                gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                    l1qBuf_tensor[l1qBurstIdx * fm * BLOCK_SIZE],
                    gmSrcq_tensor[(int64_t)srcqOffset + l1qBurstIdx * ntokensQ * BLOCK_SIZE], fm, fm, fm, BLOCK_SIZE,
                    BLOCK_SIZE, BLOCK_SIZE);
            }
        }
        SET_FLAG(MTE2, MTE1, Pingflag);
        if (__n1 != -1) {
            SET_FLAG(MTE2, MTE1, Pongflag);
        }
    }
#if __CCE_AICORE__ != 100
    //  Mask Preload L1
    if (gmSrcm != nullptr) {
        if (add_mask_n0 == 1) {
            WAIT_FLAG(MTE1, MTE2, Pingflag + 2);
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1maskBufAddr_tensor[Pingflag * L0AB_HALF_BUF_SIZE], gmSrcm_tensor[srcmOffset0], fm, fm, maskStride,  fn,
                fn, fn);
            SET_FLAG(MTE2, MTE1, Pingflag + 2);
        }
        if (__n1 != -1) {
            if (add_mask_n1 == 1) {
                WAIT_FLAG(MTE1, MTE2, Pongflag + 2);
                gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                    l1maskBufAddr_tensor[Pongflag * L0AB_HALF_BUF_SIZE], gmSrcm_tensor[srcmOffset1], fm, fm, maskStride,
                    bn, bn, bn);
                SET_FLAG(MTE2, MTE1, Pongflag + 2);
            }
        }
    }
#endif
    WAIT_FLAG(M, MTE1, Pingflag);
    if (initGgO == 1) {
        WAIT_FLAG(MTE2, MTE1, Pingflag);
    }
    if (__m0 == 1) {
        l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
            l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l1qBuf_tensor, 0,
            1, // repeat
            0,
            1, // srcStride
            0,
            0 // dstStride
        );

    } else if (fk == BLOCK_SIZE) {
        l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
            l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l1qBuf_tensor, 0,
            fm / BLOCK_SIZE, // repeat
            0,
            1, // srcStride
            0,
            0 // dstStride
        );

    } else {
        for (int32_t l0aLoadIdx = 0; l0aLoadIdx < (fm / BLOCK_SIZE); ++l0aLoadIdx) { // (fm, fk) Nz-> zZ
            l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * fk * BLOCK_SIZE],
                l1qBuf_tensor[l0aLoadIdx * CUBE_MATRIX_SIZE], 0,
                fk / BLOCK_SIZE, // repeat
                0,
                fm / BLOCK_SIZE, // srcStride
                0,
                0 // dstStride
            );
        }
    }

    SET_FLAG(MTE1, M, Pingflag);
#if __CCE_AICORE__ == 100
    WAIT_FLAG(MTE1, MTE2, Pingflag + 2);
#else
    WAIT_FLAG(MTE1, MTE2, Pingflag + 4);
#endif
    if (kvCopyStride <= STRIDE_UPPER_BOUND + fn) {
        gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
            l1kPingBuf_tensor, gmSrck_tensor[(int64_t)srckOffset], fn, fn, kvCopyStride, fk, fk, fk);
    } else {
        for (int32_t l1kBurstIdx = 0; l1kBurstIdx < (fk / BLOCK_SIZE); ++l1kBurstIdx) {
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1kPingBuf_tensor[l1kBurstIdx * fn * BLOCK_SIZE],
                gmSrck_tensor[(int64_t)srckOffset + l1kBurstIdx * kvCopyStride * BLOCK_SIZE], fn, fn, fn, BLOCK_SIZE,
                BLOCK_SIZE, BLOCK_SIZE);
        }
    }
    SET_FLAG(MTE2, MTE1, Pingflag);
    WAIT_FLAG(MTE2, MTE1, Pingflag);

    WAIT_FLAG(M, MTE1, Pingflag + 2);
    l1_to_l0_b<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
        l0bBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l1kPingBuf_tensor, 0,
        fk * fn / CUBE_MATRIX_SIZE, // repeat
        0,
        1, // srcStride
        0,
        0 // dstStride
    );
#if __CCE_AICORE__ == 100
    SET_FLAG(MTE1, MTE2, Pingflag + 2);
#else
    SET_FLAG(MTE1, MTE2, Pingflag + 4);
#endif
    SET_FLAG(MTE1, M, Pingflag + 2);

#if __CCE_AICORE__ != 100
    // V PRELOAD PING
    WAIT_FLAG(MTE1, MTE2, Pingflag + 6);
    if (kvCopyStride <= STRIDE_UPPER_BOUND + fn) {
        gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
            l1vPingBuf_tensor, gmSrcv_tensor[(int64_t)srcvOffset], fn, fn, kvCopyStride, fk, fk, fk);
    } else {
        for (int32_t l1vBurstIdx = 0; l1vBurstIdx < (fk / BLOCK_SIZE); ++l1vBurstIdx) {
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1vPingBuf_tensor[l1vBurstIdx * fn * BLOCK_SIZE],
                gmSrcv_tensor[(int64_t)srcvOffset + l1vBurstIdx * kvCopyStride * BLOCK_SIZE], fn, fn, fn, BLOCK_SIZE,
                BLOCK_SIZE, BLOCK_SIZE);
        }
    }
    SET_FLAG(MTE2, MTE1, Pingflag + 4);
#endif
    // bmm1 ping
    WAIT_FLAG(MTE1, M, Pingflag + 2);
    WAIT_FLAG(MTE1, M, Pingflag);
    WAIT_FLAG(V, M, Pingflag);
    mmad<ArchType::ASCEND_V200, half, half, float, false>(
        l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
        l0bBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], __m0, __n0, fk, 1);

    SET_FLAG(M, MTE1, Pingflag);
    SET_FLAG(M, MTE1, Pingflag + 2);
    SET_FLAG(M, V, Pingflag);
    // 1. ################ Bmm1 Ping Ends #######################
    // 2. ################ Bmm1 Pong Starts #######################
    // 2.1 ################ QK Pong LOAD ################
    if (__n1 != -1) {
        WAIT_FLAG(M, MTE1, Pongflag);
        if (initGgO == 1) {
            WAIT_FLAG(MTE2, MTE1, Pongflag);
        }
        if (__m0 == 1) {
            l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0aBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], l1qBuf_tensor, 0,
                1, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );
        } else if (fk == BLOCK_SIZE) {
            l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0aBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], l1qBuf_tensor, 0,
                fm / BLOCK_SIZE, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );
        } else {
            for (int32_t l0aLoadIdx = 0; l0aLoadIdx < (fm / BLOCK_SIZE); ++l0aLoadIdx) { // (fm, fk) Nz-> zZ
                l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0aBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * fk * BLOCK_SIZE],
                    l1qBuf_tensor[l0aLoadIdx * CUBE_MATRIX_SIZE], 0,
                    fk / BLOCK_SIZE, // repeat
                    0,
                    fm / BLOCK_SIZE, // srcStride
                    0,
                    0 // dstStride
                );
            }
        }
        SET_FLAG(MTE1, M, Pongflag);
#if __CCE_AICORE__ == 100
        WAIT_FLAG(MTE1, MTE2, Pongflag + 2);
#else
        WAIT_FLAG(MTE1, MTE2, Pongflag + 4);
#endif
        if (kvCopyStride <= STRIDE_UPPER_BOUND + bn) {
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1kPongBuf_tensor, gmSrck_tensor[(int64_t)srckOffset + Pongflag * pp_n_scalar * BLOCK_SIZE], bn, bn,
                kvCopyStride, fk, fk, fk);
        } else {
            for (int32_t l1kBurstIdx = 0; l1kBurstIdx < (fk / BLOCK_SIZE); ++l1kBurstIdx) {
                gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                    l1kPongBuf_tensor[l1kBurstIdx * bn * BLOCK_SIZE],
                    gmSrck_tensor[(int64_t)srckOffset + Pongflag * pp_n_scalar * BLOCK_SIZE +
                                  l1kBurstIdx * kvCopyStride * BLOCK_SIZE],
                    bn, bn, bn, BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE);
            }
        }
        SET_FLAG(MTE2, MTE1, Pongflag);
        WAIT_FLAG(MTE2, MTE1, Pongflag);
        WAIT_FLAG(M, MTE1, Pongflag + 2);
        l1_to_l0_b<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
            l0bBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], l1kPongBuf_tensor, 0,
            fk * bn / CUBE_MATRIX_SIZE, // repeat
            0,
            1, // srcStride
            0,
            0 // dstStride
        );
#if __CCE_AICORE__ == 100
        SET_FLAG(MTE1, MTE2, Pongflag + 2);
#else
        SET_FLAG(MTE1, MTE2, Pongflag + 4);
#endif
        SET_FLAG(MTE1, M, Pongflag + 2);

#if __CCE_AICORE__ != 100
        WAIT_FLAG(MTE1, MTE2, Pongflag + 6);
        if (kvCopyStride <= STRIDE_UPPER_BOUND + bn) {
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1vPongBuf_tensor, gmSrcv_tensor[(int64_t)srcvOffset + Pongflag * pp_n_scalar * BLOCK_SIZE], bn, bn,
                kvCopyStride, fk, fk, fk);
        } else {
            for (int32_t l1vBurstIdx = 0; l1vBurstIdx < (fk / BLOCK_SIZE); ++l1vBurstIdx) {
                gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                    l1vPongBuf_tensor[l1vBurstIdx * bn * BLOCK_SIZE],
                    gmSrcv_tensor[(int64_t)srcvOffset + Pongflag * pp_n_scalar * BLOCK_SIZE +
                                  l1vBurstIdx * kvCopyStride * BLOCK_SIZE],
                    bn, bn, bn, BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE);
            }
        }
        SET_FLAG(MTE2, MTE1, Pongflag + 4);
#endif
        WAIT_FLAG(MTE1, M, Pongflag + 2);
        WAIT_FLAG(MTE1, M, Pongflag);
        WAIT_FLAG(V, M, Pongflag);
        mmad<ArchType::ASCEND_V200, half, half, float, false>(
            l0cBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], l0aBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
            l0bBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], __m0, __n1, fk, 1);

        SET_FLAG(M, MTE1, Pongflag);
        SET_FLAG(M, V, Pongflag);
        SET_FLAG(M, MTE1, Pongflag + 2);
    }
    // 2. ################ Bmm1 Pong Ends #######################
#if __CCE_AICORE__ == 100
    //  Mask Preload L1 for 910
    if (gmSrcm != nullptr) {
        if (add_mask_n0 == 1) {
            WAIT_FLAG(MTE1, MTE2, Pingflag + 2);
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1maskBufAddr_tensor[Pingflag * L0AB_HALF_BUF_SIZE], gmSrcm_tensor[srcmOffset0], fm, fm, maskStride, fn,
                fn, fn);
            SET_FLAG(MTE2, MTE1, Pingflag + 2);
            WAIT_FLAG(MTE2, MTE1, Pingflag + 2);
        }
        if (__n1 != -1) {
            if (add_mask_n1 == 1) {
                WAIT_FLAG(MTE1, MTE2, Pongflag + 2);
                gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                    l1maskBufAddr_tensor[Pongflag * L0AB_HALF_BUF_SIZE], gmSrcm_tensor[srcmOffset1], fm, fm, maskStride,
                    bn, bn, bn); //maskStride
                SET_FLAG(MTE2, MTE1, Pongflag + 2);
                WAIT_FLAG(MTE2, MTE1, Pongflag + 2);
            }
        }
    }
#endif
    SoftMax(fm, fn, fk, bn, __m0, __n0, __n1, add_mask_n0, add_mask_n1,
            alibi_coeff, delta0, delta1, scale_type, alibi_left_align, initGgDm
            );
    if (cubeUpdateO == 1) {
        initGgO = 0;
    }
    
    // 5. ################ Bmm2 Ping Starts #######################
    if (cubeUpdateO == 0) {
#if __CCE_AICORE__ == 100
        // V PRELOAD PING for 910
        WAIT_FLAG(MTE1, MTE2, Pingflag + 2);
        if (kvCopyStride <= STRIDE_UPPER_BOUND + fn) {
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1vPingBuf_tensor, gmSrcv_tensor[(int64_t)srcvOffset], fn, fn, kvCopyStride, fk, fk, fk);
        } else {
            for (int32_t l1vBurstIdx = 0; l1vBurstIdx < (fk / BLOCK_SIZE); ++l1vBurstIdx) {
                gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                    l1vPingBuf_tensor[l1vBurstIdx * fn * BLOCK_SIZE],
                    gmSrcv_tensor[(int64_t)srcvOffset + l1vBurstIdx * kvCopyStride * BLOCK_SIZE], fn, fn, fn, BLOCK_SIZE,
                    BLOCK_SIZE, BLOCK_SIZE);
            }
        }
        SET_FLAG(MTE2, MTE1, Pingflag + 2);
        WAIT_FLAG(MTE2, MTE1, Pingflag + 2);
#else
        WAIT_FLAG(MTE2, MTE1, Pingflag + 4);
#endif
        WAIT_FLAG(M, MTE1, Pingflag + 2);
        // 16 is blocksize in format zN
        if (fk == 16) {
            l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0bBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l1vPingBuf_tensor, 0,
                fn / BLOCK_SIZE, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );
        } else {
            for (int32_t l0bLoadIdx = 0; l0bLoadIdx < (fn / BLOCK_SIZE); ++l0bLoadIdx) { // Nz -> nZ
                l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0bBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + l0bLoadIdx * fk * BLOCK_SIZE],
                    l1vPingBuf_tensor[l0bLoadIdx * CUBE_MATRIX_SIZE], 0,
                    fk / BLOCK_SIZE, // repeat
                    0,
                    fn / BLOCK_SIZE, // srcStride
                    0,
                    0 // dstStride
                );
            }
        }
        SET_FLAG(MTE1, M, Pingflag + 2);
    #if __CCE_AICORE__ == 100
        SET_FLAG(MTE1, MTE2, Pingflag + 2);
    #else
        SET_FLAG(MTE1, MTE2, Pingflag + 6);
    #endif
        // BMM2 P LOAD Ping
        WAIT_FLAG(V, MTE3, Pingflag);
        WAIT_FLAG(MTE1, MTE3, Pingflag);
        if (__m0 == 1) {
            ub_to_l1<ArchType::ASCEND_V200, half>(l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                                lpUbuf_tensor[Pingflag * lpUbufSize], fn / BLOCK_SIZE, 1, fm - 1,
                                                0);
        } else {
            ub_to_l1<ArchType::ASCEND_V200, half>(l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
                                                lpUbuf_tensor[Pingflag * lpUbufSize], 1, pSize / BLOCK_SIZE, 0,
                                                0);
        }
        SET_FLAG(MTE3, V, Pingflag);
        SET_FLAG(MTE3, MTE1, Pingflag);
        WAIT_FLAG(MTE3, MTE1, Pingflag);
        WAIT_FLAG(M, MTE1, Pingflag);
        // 16 is blocksize in format zN
        if (__m0 == 1) {
            l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 0,
                1, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );
        } else if (fn == BLOCK_SIZE) {
            l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 0,
                fm / BLOCK_SIZE, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );
        } else {
            for (int32_t l0aLoadIdx = 0; l0aLoadIdx < (fm / BLOCK_SIZE); ++l0aLoadIdx) { // (fm, fn)
                l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0aBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * fn * BLOCK_SIZE],
                    l1pPingBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * CUBE_MATRIX_SIZE], 0,
                    fn / BLOCK_SIZE, // repeat
                    0,
                    fm / BLOCK_SIZE, // srcStride
                    0,
                    0 // dstStride
                );
            }
        }
        SET_FLAG(MTE1, M, Pingflag);
        SET_FLAG(MTE1, MTE3, Pingflag);
        WAIT_FLAG(MTE1, M, Pingflag);
        // 4. bmm2 partr
        WAIT_FLAG(MTE1, M, Pingflag + 2);
        WAIT_FLAG(V, M, Pingflag + 2);
        mmad<ArchType::ASCEND_V200, __fp16, __fp16, float, false>(
            l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE],
            l0aBuf_tensor.ReinterpretCast<__fp16>()[Pingflag * L0AB_HALF_BUF_SIZE],
            l0bBuf_tensor.ReinterpretCast<__fp16>()[Pingflag * L0AB_HALF_BUF_SIZE], __m0, fk, __n0, 1);
        SET_FLAG(M, V, Pingflag);
        SET_FLAG(M, MTE1, Pingflag);
        SET_FLAG(M, MTE1, Pingflag + 2);
        if (wrapO == 1) {
            SET_FLAG(MTE1, MTE2, Pingflag);
            if (__n1 == -1) {
                SET_FLAG(MTE1, MTE2, Pongflag);
            }
        }
    }
    // 5. ################ Bmm2 Ping Ends #######################
    // 6. ################ Bmm2 Pong Starts #######################
    if (__n1 != -1) {
#if __CCE_AICORE__ == 100
        WAIT_FLAG(MTE1, MTE2, Pongflag + 2);
        if (kvCopyStride <= STRIDE_UPPER_BOUND + bn) {
            gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                l1vPongBuf_tensor, gmSrcv_tensor[(int64_t)srcvOffset + Pongflag * pp_n_scalar * BLOCK_SIZE], bn, bn,
                kvCopyStride, fk, fk, fk);
        } else {
            for (int32_t l1vBurstIdx = 0; l1vBurstIdx < (fk / BLOCK_SIZE); ++l1vBurstIdx) {
                gm_to_l1<ArchType::ASCEND_V200, half, DataFormat::NZ, DataFormat::NZ>(
                    l1vPongBuf_tensor[l1vBurstIdx * bn * BLOCK_SIZE],
                    gmSrcv_tensor[(int64_t)srcvOffset + Pongflag * pp_n_scalar * BLOCK_SIZE +
                                  l1vBurstIdx * kvCopyStride * BLOCK_SIZE],
                    bn, bn, bn, BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE);
            }
        }
        SET_FLAG(MTE2, MTE1, Pongflag + 2);
        WAIT_FLAG(MTE2, MTE1, Pongflag + 2);
#else
        WAIT_FLAG(MTE2, MTE1, Pongflag + 4);
#endif
        WAIT_FLAG(M, MTE1, Pongflag + 2);
        // 16 is blocksize in format zN
        if (fk == 16) {
            l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0bBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], l1vPongBuf_tensor, 0,
                bn / BLOCK_SIZE, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );
        } else {
            for (int32_t l0bLoadIdx = 0; l0bLoadIdx < (bn / BLOCK_SIZE); ++l0bLoadIdx) { // Nz -> nZ
                l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0bBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + l0bLoadIdx * fk * BLOCK_SIZE],
                    l1vPongBuf_tensor[l0bLoadIdx * CUBE_MATRIX_SIZE], 0,
                    fk / BLOCK_SIZE, // repeat
                    0,
                    bn / BLOCK_SIZE, // srcStride
                    0,
                    0 // dstStride
                );
            }
        }
#if __CCE_AICORE__ == 100
        SET_FLAG(MTE1, MTE2, Pongflag + 2);
#else
        SET_FLAG(MTE1, MTE2, Pongflag + 6);
#endif
        SET_FLAG(MTE1, M, Pongflag + 2);
        // BMM2 P LOAD Pong
        WAIT_FLAG(MTE1, MTE3, Pongflag);
        WAIT_FLAG(V, MTE3, Pongflag);
        if (__m0 == 1) {
            ub_to_l1<ArchType::ASCEND_V200, half>(l1pPongBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                                  lpUbuf_tensor[Pongflag * lpUbufSize], bn / BLOCK_SIZE, 1,
                                                  fm - 1, 0);
        } else {
            ub_to_l1<ArchType::ASCEND_V200, half>(l1pPongBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                                                  lpUbuf_tensor[Pongflag * lpUbufSize], 1, pSize_b / BLOCK_SIZE,
                                                  0, 0);
        }
        SET_FLAG(MTE3, V, Pongflag);
        SET_FLAG(MTE3, MTE1, Pongflag);
        WAIT_FLAG(MTE3, MTE1, Pongflag);
        WAIT_FLAG(M, MTE1, Pongflag);

        // 16 is blocksize in format zN
        if (__m0 == 1) {
            l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0aBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], l1pPongBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 0,
                1, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );
        } else if (bn == BLOCK_SIZE) {
            l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0aBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], l1pPongBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 0,
                fm / BLOCK_SIZE, // repeat
                0,
                1, // srcStride
                0,
                0 // dstStride
            );
        } else {
            for (int32_t l0aLoadIdx = 0; l0aLoadIdx < (fm / BLOCK_SIZE); ++l0aLoadIdx) { // (fm, bn)
                l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0aBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * bn * BLOCK_SIZE],
                    l1pPongBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * CUBE_MATRIX_SIZE], 0,
                    bn / BLOCK_SIZE, // repeat
                    0,
                    fm / BLOCK_SIZE, // srcStride
                    0,
                    0 // dstStride
                );
            }
        }
        SET_FLAG(MTE1, M, Pongflag);
        SET_FLAG(MTE1, MTE3, Pongflag);
        WAIT_FLAG(MTE1, M, Pongflag);
        // 4. bmm2 part
        WAIT_FLAG(MTE1, M, Pongflag + 2);
        WAIT_FLAG(V, M, Pongflag + 2);
        mmad<ArchType::ASCEND_V200, __fp16, __fp16, float, false>(
            l0cBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
            l0aBuf_tensor.ReinterpretCast<__fp16>()[Pongflag * L0AB_HALF_BUF_SIZE],
            l0bBuf_tensor.ReinterpretCast<__fp16>()[Pongflag * L0AB_HALF_BUF_SIZE], __m0, fk, __n1, 1);

        SET_FLAG(M, MTE1, Pongflag);
        SET_FLAG(M, MTE1, Pongflag + 2);

        // online softmax
        if (cubeUpdateO == 1 && initGgO == 0) {
            WAIT_FLAG(MTE3, MTE1, Pongflag + 2);
            WAIT_FLAG(M, MTE1, Pongflag);
            if (__m0 == 1) {
                l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0aBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 
                    l1dmDiagPongBuf_tensor, 
                    0,
                    1, // repeat
                    0,
                    1, // srcStride
                    0,
                    0 // dstStride
                );

            } else {
                for (int32_t l0aLoadIdx = 0; l0aLoadIdx < (fm / BLOCK_SIZE); ++l0aLoadIdx) { // (fm, fk) Nz-> zZ
                    l1_to_l0_a<ArchType::ASCEND_V200, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0aBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + l0aLoadIdx * fm * BLOCK_SIZE],
                        l1dmDiagPongBuf_tensor[l0aLoadIdx * CUBE_MATRIX_SIZE], 
                        0,
                        fm / BLOCK_SIZE, // repeat
                        0,
                        fm / BLOCK_SIZE, // srcStride
                        0,
                        0 // dstStride
                    );
                }
            }

            WAIT_FLAG(MTE3, MTE1, EVENT_ID0);
            WAIT_FLAG(M, MTE1, Pongflag + 2);
            if (fk == BLOCK_SIZE) {
                l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0bBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 
                    l1oTempBuf_tensor, 
                    0,
                    fm / BLOCK_SIZE, // repeat
                    0,
                    1, // srcStride
                    0,
                    0 // dstStride
                );
            } else {
                for (int32_t l0bLoadIdx = 0; l0bLoadIdx < (fm / BLOCK_SIZE); ++l0bLoadIdx) { // Nz -> nZ
                    l1_to_l0_b<ArchType::ASCEND_V200, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0bBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE + l0bLoadIdx * fk * BLOCK_SIZE],
                        l1oTempBuf_tensor[l0bLoadIdx * CUBE_MATRIX_SIZE], 
                        0,
                        fk / BLOCK_SIZE, // repeat
                        0,
                        fm / BLOCK_SIZE, // srcStride
                        0,
                        0 // dstStride
                    );
                }
            }

            SET_FLAG(MTE1, M, Pongflag);
            WAIT_FLAG(MTE1, M, Pongflag);

            mmad<ArchType::ASCEND_V200, __fp16, __fp16, float, false>(
                l0cBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE],
                l0aBuf_tensor.ReinterpretCast<__fp16>()[Pongflag * L0AB_HALF_BUF_SIZE],
                l0bBuf_tensor.ReinterpretCast<__fp16>()[Pongflag * L0AB_HALF_BUF_SIZE], 
                __m0, 
                fk, 
                __m0, 
                0
            );

            SET_FLAG(M, MTE1, Pongflag);
            SET_FLAG(M, MTE1, Pongflag + 2);
        }
        if (cubeUpdateO == 1) {
            initGgO = 0;
        }
        
        SET_FLAG(M, V, Pongflag);
        if (wrapO == 1) {
            SET_FLAG(MTE1, MTE2, Pongflag);
        }
    }
    // 6. ################ Bmm2 Pong Ends #######################
    // 7. ################ Update Ping Starts #######################
    WAIT_FLAG(V, MTE1, EVENT_ID0);
    if (cubeUpdateO == 0) {
        WAIT_FLAG(M, V, Pingflag);
        l0c_to_ub<ArchType::ASCEND_V200, float, T>(loUbuf_tensor.ReinterpretCast<T>(),
                                                   l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 1,
                                                   oSize / CUBE_MATRIX_SIZE, 0, 0);
    } else if (__n1 == -1) {
        WAIT_FLAG(M, V, Pingflag);
        l0c_to_ub<ArchType::ASCEND_V200, float, float>(goUbuf_tensor, 
                                                       l0cBuf_tensor[Pingflag * L0AB_HALF_BUF_SIZE], 1,
                                                       oSize / CUBE_MATRIX_SIZE, 0, 0);
    }
    PIPE_BARRIER(V);
    SoftmaxUpdate(fm, fk, oSize, Pingflag, initGgO, mD64);
    initGgO = 0;
    // 7. ################ Update Ping Ends #######################
    // 8. ################ Update Pong Starts #######################
    if (__n1 != -1) {
        WAIT_FLAG(V, MTE1, EVENT_ID0);
        WAIT_FLAG(M, V, Pongflag);

        if (cubeUpdateO == 0) {
            l0c_to_ub<ArchType::ASCEND_V200, float, T>(loUbuf_tensor.ReinterpretCast<T>(), l0cBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 1,
                                                       oSize / CUBE_MATRIX_SIZE, 0, 0);
        } else if (wrapO == 0) {
            l0c_to_ub<ArchType::ASCEND_V200, float, half>(toUbuf_tensor, l0cBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 1,
                                                          oSize / CUBE_MATRIX_SIZE, 0, 0);

            SET_FLAG(V, MTE3, Pongflag);
            WAIT_FLAG(V, MTE3, Pongflag);

            ub_to_l1<ArchType::ASCEND_V200, half>(l1oTempBuf_tensor,
                                                  toUbuf_tensor, 
                                                  1, 
                                                  oSize / BLOCK_SIZE,
                                                  0,
                                                  0);
            
        } else {
            l0c_to_ub<ArchType::ASCEND_V200, float, float>(goUbuf_tensor, l0cBuf_tensor[Pongflag * L0AB_HALF_BUF_SIZE], 1,
                                                           oSize / CUBE_MATRIX_SIZE, 0, 0);
        }
        PIPE_BARRIER(V);
        // 5. update for outer loop
        SoftmaxUpdate(fm, fk, oSize, Pongflag, initGgO, mD64);
        SET_FLAG(V, M, Pongflag + 2);
        PIPE_BARRIER(V);
        initGgO = 0;
    }
    SET_FLAG(V, M, Pingflag + 2);
    // 8. ################ Update Pong Ends #######################
    // 9. ################ Line Output Starts #####################
    UpdateOutput(fm, fk, oSize, mD64, __m0);
    // 9. ################ Line Output Ends #####################
}

template <>
__aicore__ inline void UnpadFlashAttentionCommon<float, half, PrecType::BMM1_FP16_EXP_FP32 , PrecType::BMM1_FP16_EXP_FP32>
                    ::Run(AscendC::LocalTensor<int32_t> tiling_para_ub_tensor,
                           AscendC::GlobalTensor<int32_t> tiling_para_gm_tensor,
                           AscendC::LocalTensor<int32_t> layerId_ub_tensor,
                           __gm__ uint8_t *__restrict__ alibi_coeff_gm,
                           uint32_t mask_type, uint32_t window_len, uint32_t long_seq,
                           uint64_t stride_qo, uint64_t stride_kv, int64_t head_mask_stride,
                           int64_t batch_mask_stride,
                           uint32_t start_batch, uint32_t end_batch,
                           int32_t start_blk, int32_t end_blk,
                           uint32_t is_triu, uint32_t alibi_compress_offset, int32_t group_num,
                           uint32_t mask_stride, uint32_t q_tokens, int32_t embd,
                           uint32_t q_tight, uint32_t scaleType,
                           half tor, int32_t kv_copy_stride, uint32_t is_sqrt,
                           int64_t heads, uint32_t max_seqlen, uint32_t batch_size, int32_t kv_real_heads, const uint32_t alibi_left_align)
{
    if (gmSrcLayerid != nullptr) {
        stride_kv = max_seqlen * embd;
        kv_copy_stride = max_seqlen;
        uint64_t stride_batch_kv = batch_size * max_seqlen * kv_real_heads * embd * 2;
        uint32_t layerIdGm = *(__ubuf__ int32_t *)layerId_ub_tensor.GetPhyAddr();
        gmSrck = stride_batch_kv * layerIdGm + gmSrck;
        gmSrcv = stride_batch_kv * layerIdGm + gmSrcv;
    }

    SetEncoderParams(tor, kv_copy_stride, is_sqrt, 0);

    SET_FLAG(S, MTE2, EVENT_ID0);
    WAIT_FLAG(S, MTE2, EVENT_ID0);
    gm_to_ub<ArchType::ASCEND_V200, int32_t>(
        tiling_para_ub_tensor, tiling_para_gm_tensor[BATCH_TILING_OFFSET + start_batch * TILING_PARA_SIZE], 0, 1,
        ((end_batch - start_batch + 1) * 80 + 31) / 32, 0, 0);


    SET_FLAG(MTE2, S, EVENT_ID0);
    WAIT_FLAG(MTE2, S, EVENT_ID0);
    SyncStart();
    __ubuf__ int32_t *tiling_para_ub =
        (__ubuf__ int32_t *)tiling_para_ub_tensor.GetPhyAddr();
    int32_t cur_batch = 0;
    int64_t cur_bms = batch_mask_stride * start_batch;
    for (uint32_t curr_q_blk = start_blk; curr_q_blk < end_blk; curr_q_blk++) {
        // get tiling args
        int32_t offset_tiling = TILING_PARA_SIZE * cur_batch;
        int32_t q_seqlen_aligned = (int32_t)(*((__ubuf__ int32_t *)tiling_para_ub + offset_tiling));
        int32_t kv_seqlen_aligned = (int32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 1 + offset_tiling));
        uint32_t q_seqlen_real = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 13 + offset_tiling));
        uint32_t kv_seqlen_real = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 14 + offset_tiling));
        int32_t pp_m_scalar = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 2 + offset_tiling));
        int32_t pp_n_scalar = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 3 + offset_tiling));
        uint32_t addr_q_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 4 + offset_tiling));
        uint32_t addr_q_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 5 + offset_tiling));
        int64_t addr_q_scalar = (int64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32); // batch offset
        uint32_t addr_k_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 6 + offset_tiling));
        uint32_t addr_k_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 7 + offset_tiling));
        int64_t addr_k_scalar = (int64_t)(((uint64_t)addr_k_high32) << 32 | addr_k_loww32);
        uint32_t addr_v_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 8 + offset_tiling));
        uint32_t addr_v_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 9 + offset_tiling));
        int64_t addr_v_scalar = (int64_t)(((uint64_t)addr_v_high32) << 32 | addr_v_loww32);
        uint32_t addr_o_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 10 + offset_tiling));
        uint32_t addr_o_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 11 + offset_tiling));
        int64_t addr_o_scalar = (int64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
        uint32_t cur_total_qblk = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 12 + offset_tiling));
        uint32_t cur_proc_num = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 15 + offset_tiling));
        uint32_t cur_q_blk_id = curr_q_blk - (cur_total_qblk - cur_proc_num);
        // SWA calc mode condition
        uint32_t swa_mode = ((mask_type == AttentonMaskType::MASK_TYPE_SWA_NORM
                            || mask_type == AttentonMaskType::MASK_TYPE_SWA_COMPRESS)
                            && kv_seqlen_real > window_len) ? 1 : 0;
        is_triu = ((mask_type == AttentonMaskType::MASK_TYPE_SWA_NORM
                    || mask_type == AttentonMaskType::MASK_TYPE_SWA_COMPRESS)
                    && kv_seqlen_real <= window_len) ? 1 : is_triu;    // Regular Triu Mask

        int32_t m_loop = (q_seqlen_aligned + pp_m_scalar - 1) / pp_m_scalar;
        int32_t n_loop = 0;
        if (swa_mode) {
            n_loop = kv_seqlen_aligned > window_len + pp_n_scalar
                    ? ((window_len + pp_n_scalar - 1) / pp_n_scalar + 1)
                    : ((kv_seqlen_aligned + pp_n_scalar - 1) / pp_n_scalar);
        } else {
            n_loop = (kv_seqlen_aligned + pp_n_scalar - 1) / pp_n_scalar;
        }
        int32_t start = cur_q_blk_id * n_loop;
        int32_t end = start + n_loop;

        for (int32_t loop_idx = start; loop_idx < end; loop_idx += 2) {
            int32_t head_idx0 = loop_idx / (m_loop * n_loop);
            int32_t m_idx0 = loop_idx % (m_loop * n_loop) / n_loop;
            int32_t n_idx0 = loop_idx % (m_loop * n_loop) % n_loop;
            int32_t window_offset = 0; // window_offset will only be overwritten in swa mode.
            if (swa_mode) {
                window_offset = (m_idx0 + 1 > n_loop) ? (m_idx0 - n_loop + 1) : 0;
                is_triu = (window_offset == 0) ? 1 : 0;
            }
            if (is_triu == 1 && n_idx0 > m_idx0) {
                continue;
            }
            int32_t add_mask_n0 = ((long_seq == 0) || ((long_seq == 1) && (n_idx0 == m_idx0)) ||
                                   alibi_coeff_gm != nullptr ||
                                   (mask_type == AttentonMaskType::MASK_TYPE_ALIBI && alibi_compress_offset > 0))
                                      ? 1
                                      : 0;
            int32_t add_mask_n1 = ((long_seq == 0) || ((long_seq == 1) && (n_idx0 + 1 == m_idx0)) ||
                                   alibi_coeff_gm != nullptr ||
                                   (mask_type == AttentonMaskType::MASK_TYPE_ALIBI && alibi_compress_offset > 0))
                                      ? 1
                                      : 0;
            int64_t q_offset = addr_q_scalar + head_idx0 * stride_qo + m_idx0 * pp_m_scalar * BLOCK_SIZE;
            int64_t k_offset = addr_k_scalar + head_idx0 / group_num * stride_kv + (n_idx0 + window_offset) * pp_n_scalar * BLOCK_SIZE;
            int64_t v_offset = addr_v_scalar + head_idx0 / group_num * stride_kv + (n_idx0 + window_offset) * pp_n_scalar * BLOCK_SIZE;
            int64_t o_offset = addr_o_scalar + head_idx0 * stride_qo + m_idx0 * pp_m_scalar * BLOCK_SIZE;
            int64_t logn_offset = m_idx0 * pp_m_scalar;

            int64_t mask_offset0 = cur_bms + head_mask_stride * head_idx0;
            int64_t mask_offset1 = cur_bms + head_mask_stride * head_idx0;
            int32_t delta_uint = 0;
            int32_t delta0 = 0;
            int32_t delta1 = 0;
            half alibi_coeff = 1;
            if (alibi_coeff_gm != nullptr) {
                AsdopsBuffer<ArchType::ASCEND_V200> buf;
                AscendC::LocalTensor<half> alibi_coeff_ub_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>( 5 * UB_UINT8_BLOCK_SIZE + 28 * UB_UINT8_LINE_SIZE);
                AscendC::GlobalTensor<half> alibi_coeff_gm_tensor;
                alibi_coeff_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(alibi_coeff_gm));
                gm_to_ub<ArchType::ASCEND_V200, half>(alibi_coeff_ub_tensor,
                                                    alibi_coeff_gm_tensor, 0, 1, (heads + 15) / 16, 0, 0);
                SET_FLAG(MTE2, S, EVENT_ID0);
                WAIT_FLAG(MTE2, S, EVENT_ID0);
                alibi_coeff = *(__ubuf__ half *)(alibi_coeff_ub_tensor[head_idx0].GetPhyAddr());
                if (m_idx0 == n_idx0) {
                    mask_offset0 = 0;
                } else {
                    mask_offset0 = BASE_MASK_SIZE * BLOCK_SIZE;
                    delta_uint = m_idx0 * pp_m_scalar - n_idx0 * pp_n_scalar;
                    delta0 = delta_uint - BASE_MASK_SIZE;
                }

                if (m_idx0 == n_idx0 + 1) {
                    mask_offset1 = 0;
                } else {
                    mask_offset1 = BASE_MASK_SIZE * BLOCK_SIZE;
                    delta_uint = m_idx0 * pp_m_scalar - (n_idx0 + 1) * pp_n_scalar;
                    delta1 = delta_uint - BASE_MASK_SIZE;
                }
            } else if (mask_type == AttentonMaskType::MASK_TYPE_ALIBI && alibi_compress_offset > 0) {
                if (m_idx0 != n_idx0) {
                    mask_offset0 += (m_idx0 * pp_m_scalar - n_idx0 * pp_n_scalar) * BLOCK_SIZE;
                }

                if (m_idx0 != n_idx0 + 1) {
                    mask_offset1 += (m_idx0 * pp_m_scalar - (n_idx0 + 1) * pp_n_scalar) * BLOCK_SIZE;
                }
            } else if (mask_type == AttentonMaskType::MASK_TYPE_SWA_COMPRESS) { //SWA Compress mode - mask offset
                int32_t window_n_scalar = (window_len > 2 * pp_n_scalar) ? window_len / pp_n_scalar : 2;
                if (n_idx0 == m_idx0 - window_offset) {
                    mask_offset1 += mask_stride * pp_n_scalar;
                } else if ((n_idx0 < m_idx0 - window_offset) && ((m_idx0 - n_idx0) < (window_offset + window_n_scalar))) {
                    mask_offset0 += pp_m_scalar * BLOCK_SIZE;
                    add_mask_n0 = (window_len >= 2 * pp_n_scalar) ? 0 : 1;
                    if (n_idx0 + 1 < m_idx0 - window_offset){
                        mask_offset1 += pp_m_scalar * BLOCK_SIZE;
                        add_mask_n1 = (window_len >= 2 * pp_n_scalar) ? 0 : 1;
                    }
                } else if (n_idx0 == (m_idx0 - window_offset - window_n_scalar)) {
                    mask_offset0 += 2 * pp_m_scalar * BLOCK_SIZE;
                    mask_offset1 += pp_m_scalar * BLOCK_SIZE;
                    add_mask_n1 = (window_len >= 2 * pp_n_scalar) ? 0 : 1;
                } else if (n_idx0 == (m_idx0 - window_offset - window_n_scalar - 1)) {
                    mask_offset0 += 3 * pp_m_scalar * BLOCK_SIZE;
                    mask_offset1 += 2 * pp_m_scalar * BLOCK_SIZE;
                } else {
                    mask_offset0 += mask_stride * pp_n_scalar;
                    mask_offset1 += mask_stride * pp_n_scalar;
                }
            } else if (long_seq == 0) {
                mask_offset0 += (m_idx0 * pp_m_scalar * BLOCK_SIZE + (n_idx0 + window_offset) * mask_stride * pp_n_scalar);
                mask_offset1 += (m_idx0 * pp_m_scalar * BLOCK_SIZE + (n_idx0 + window_offset + 1) * mask_stride * pp_n_scalar);
            }
            int32_t wrap_o = (n_idx0 == (n_loop - 1) || (n_idx0 + 1) == (n_loop - 1)) ? 1 : 0;
            if (is_triu == 1) {
                wrap_o = (n_idx0 == m_idx0 || (n_idx0 + 1) == m_idx0) ? 1 : 0;
            }
            if (swa_mode) {
                if (window_offset == 0) {
                    wrap_o = (n_idx0 == m_idx0 || (n_idx0 + 1) == m_idx0) ? 1 : 0;
                } else {
                    wrap_o = (n_idx0 == (n_loop - 1) || (n_idx0 + 1) == (n_loop - 1)) ? 1 : 0;
                }
            }
            int32_t init_g = (n_idx0 == 0) ? 1 : 0;
            int32_t __m0 = (m_idx0 == (m_loop - 1)) ? (q_seqlen_real - m_idx0 * pp_m_scalar) : pp_m_scalar;
            int32_t __n0 = 0;
            int32_t __n1 = 0;
            if (swa_mode) {
                __n0 = ((n_idx0 + window_offset + 1) * pp_n_scalar <= kv_seqlen_real)
                        ? pp_n_scalar
                        : (kv_seqlen_real - (n_idx0 + window_offset) * pp_n_scalar);
                __n1 = (((n_idx0 + 1) + window_offset + 1) * pp_n_scalar <= kv_seqlen_real)
                        ? pp_n_scalar
                        : (kv_seqlen_real - ((n_idx0 + 1) + window_offset) * pp_n_scalar);
            } else {
                __n0 = (n_idx0 == (n_loop - 1)) ? (kv_seqlen_real - n_idx0 * pp_n_scalar) : pp_n_scalar;
                __n1 = ((n_idx0 + 1) == (n_loop - 1)) ? (kv_seqlen_real - (n_idx0 + 1) * pp_n_scalar) : pp_n_scalar;
            }
            int32_t __k0 = embd;
            int32_t round_m0 = (__m0 + 15) / 16 * 16;
            int32_t round_n0 = (__n0 + 15) / 16 * 16;
            int32_t round_k0 = (__k0 + 15) / 16 * 16;
            int32_t round_n1 = (__n1 + 15) / 16 * 16;

            if ((n_idx0 + 1) == (n_loop) || (n_idx0 == m_idx0 && is_triu == 1)) {
                __n1 = -1;
            }

            Init(round_m0, round_n0, round_k0, q_offset, k_offset, v_offset, mask_offset0, 
                                       mask_offset1, o_offset, init_g, wrap_o, q_tokens, mask_stride, logn_offset);

            FlashAttentionNzPrefillCompute(round_m0, round_n0, round_k0, round_n1, __m0, __n0,
                                                                 __n1, pp_n_scalar, q_tight, add_mask_n0, add_mask_n1,
                                                                 long_seq, alibi_coeff, delta0, delta1, scaleType, alibi_left_align);

        }
        if (cur_q_blk_id == cur_proc_num - 1) {
            cur_batch++;
            cur_bms += batch_mask_stride;
        }
    }
    SyncEnd();

}

template <>
__aicore__ inline void UnpadFlashAttentionCommon<float, float, PrecType::BMM1_FP32_EXP_FP32, PrecType::BMM1_FP32_EXP_FP32>
                    ::Run(AscendC::LocalTensor<int32_t> tiling_para_ub_tensor,
                           AscendC::GlobalTensor<int32_t> tiling_para_gm_tensor,
                           AscendC::LocalTensor<int32_t> layerId_ub_tensor,
                           __gm__ uint8_t *__restrict__ alibi_coeff_gm,
                           uint32_t mask_type, uint32_t window_len, uint32_t long_seq,
                           uint64_t stride_qo, uint64_t stride_kv, int64_t head_mask_stride,
                           int64_t batch_mask_stride,
                           uint32_t start_batch, uint32_t end_batch,
                           int32_t start_blk, int32_t end_blk,
                           uint32_t is_triu, uint32_t alibi_compress_offset, int32_t group_num,
                           uint32_t mask_stride, uint32_t q_tokens, int32_t embd,
                           uint32_t q_tight, uint32_t scaleType,
                           float tor, int32_t kv_copy_stride, uint32_t is_sqrt,
                           int64_t heads, uint32_t max_seqlen, uint32_t batch_size, int32_t kv_real_heads, const uint32_t alibi_left_align)
{
    if (gmSrcLayerid != nullptr) {
        stride_kv = max_seqlen * embd;
        kv_copy_stride = max_seqlen;
        uint64_t stride_batch_kv = batch_size * max_seqlen * kv_real_heads * embd * 2;
        uint32_t layerIdGm = *(__ubuf__ int32_t *)layerId_ub_tensor.GetPhyAddr();
        gmSrck = stride_batch_kv * layerIdGm + gmSrck;
        gmSrcv = stride_batch_kv * layerIdGm + gmSrcv;
    }
    SetEncoderParams(tor, kv_copy_stride, is_sqrt, 1);
    
    SET_FLAG(S, MTE2, EVENT_ID0);
    WAIT_FLAG(S, MTE2, EVENT_ID0);
    gm_to_ub<ArchType::ASCEND_V200, int32_t>(
        tiling_para_ub_tensor, tiling_para_gm_tensor[BATCH_TILING_OFFSET + start_batch * TILING_PARA_SIZE], 0, 1,
        ((end_batch - start_batch + 1) * 80 + 31) / 32, 0, 0);

    SET_FLAG(MTE2, S, EVENT_ID0);
    WAIT_FLAG(MTE2, S, EVENT_ID0);
    SyncStart();
    __ubuf__ int32_t *tiling_para_ub =
        (__ubuf__ int32_t *)tiling_para_ub_tensor.GetPhyAddr();
    int32_t cur_batch = 0;
    int64_t cur_bms = batch_mask_stride * start_batch;
    for (uint32_t curr_q_blk = start_blk; curr_q_blk < end_blk; curr_q_blk++) {
        // get tiling args
        int32_t offset_tiling = TILING_PARA_SIZE * cur_batch;
        int32_t q_seqlen_aligned = (int32_t)(*((__ubuf__ int32_t *)tiling_para_ub + offset_tiling));
        int32_t kv_seqlen_aligned = (int32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 1 + offset_tiling));
        uint32_t q_seqlen_real = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 13 + offset_tiling));
        uint32_t kv_seqlen_real = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 14 + offset_tiling));
        int32_t pp_m_scalar = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 2 + offset_tiling));
        int32_t pp_n_scalar = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 3 + offset_tiling));
        uint32_t addr_q_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 4 + offset_tiling));
        uint32_t addr_q_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 5 + offset_tiling));
        int64_t addr_q_scalar = (int64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32); // batch offset
        uint32_t addr_k_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 6 + offset_tiling));
        uint32_t addr_k_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 7 + offset_tiling));
        int64_t addr_k_scalar = (int64_t)(((uint64_t)addr_k_high32) << 32 | addr_k_loww32);
        uint32_t addr_v_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 8 + offset_tiling));
        uint32_t addr_v_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 9 + offset_tiling));
        int64_t addr_v_scalar = (int64_t)(((uint64_t)addr_v_high32) << 32 | addr_v_loww32);
        uint32_t addr_o_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 10 + offset_tiling));
        uint32_t addr_o_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 11 + offset_tiling));
        int64_t addr_o_scalar = (int64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
        uint32_t cur_total_qblk = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 12 + offset_tiling));
        uint32_t cur_proc_num = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 15 + offset_tiling));
        uint32_t cur_q_blk_id = curr_q_blk - (cur_total_qblk - cur_proc_num);

        int32_t m_loop = (q_seqlen_aligned + pp_m_scalar - 1) / pp_m_scalar;
        int32_t n_loop = (kv_seqlen_aligned + pp_n_scalar - 1) / pp_n_scalar;
        int32_t start = cur_q_blk_id * n_loop;
        int32_t end = start + n_loop;

        for (int32_t loop_idx = start; loop_idx < end; loop_idx += 2) {
            int32_t head_idx0 = loop_idx / (m_loop * n_loop);
            int32_t m_idx0 = loop_idx % (m_loop * n_loop) / n_loop;
            int32_t n_idx0 = loop_idx % (m_loop * n_loop) % n_loop;
            if (is_triu == 1 && n_idx0 > m_idx0 /2) {
                    continue;
            }
            int32_t add_mask_n0 = (((long_seq == 1) && (n_idx0 == m_idx0 /2)) ||
                                        alibi_coeff_gm != nullptr)
                                            ? 1
                                            : 0;
            int32_t add_mask_n1 = (((long_seq == 1) && (n_idx0 + 1 == m_idx0 /2)) ||
                                    alibi_coeff_gm != nullptr)
                                        ? 1
                                        : 0;
            int64_t q_offset = addr_q_scalar + head_idx0 * stride_qo + m_idx0 * pp_m_scalar * BLOCK_SIZE;
            int64_t k_offset = addr_k_scalar + head_idx0 / group_num * stride_kv + n_idx0 * pp_n_scalar * BLOCK_SIZE;
            int64_t v_offset = addr_v_scalar + head_idx0 / group_num * stride_kv + n_idx0 * pp_n_scalar * BLOCK_SIZE;
            int64_t o_offset = addr_o_scalar + head_idx0 * stride_qo + m_idx0 * pp_m_scalar * BLOCK_SIZE;
            int64_t logn_offset = m_idx0 * pp_m_scalar;

            int64_t mask_offset0 = cur_bms + head_mask_stride * head_idx0;
            int64_t mask_offset1 = cur_bms + head_mask_stride * head_idx0;
            int64_t delta_uint = 0;
            int64_t delta0 = 0;
            int64_t delta1 = 0;
            float alibi_coeff = 1;
            if (alibi_coeff_gm != nullptr) {
                AsdopsBuffer<ArchType::ASCEND_V200> buf;
                AscendC::LocalTensor<float> alibi_coeff_ub_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>( 5 * UB_UINT8_BLOCK_SIZE + 28 * UB_UINT8_LINE_SIZE);
                AscendC::GlobalTensor<float> alibi_coeff_gm_tensor;
                alibi_coeff_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(alibi_coeff_gm));
                gm_to_ub<ArchType::ASCEND_V200, float>(alibi_coeff_ub_tensor, alibi_coeff_gm_tensor, 0, 1, (heads + 7) / 8, 0,
                                                    0);
                SET_FLAG(MTE2, S, EVENT_ID0);
                WAIT_FLAG(MTE2, S, EVENT_ID0);
                alibi_coeff = (float)*(__ubuf__ float *)(alibi_coeff_ub_tensor[head_idx0].GetPhyAddr());
                if (alibi_left_align == 1) {
                    if (m_idx0 / 2 == n_idx0 ) {
                        if(m_idx0 % 2) {
                            mask_offset0 = 64 * BLOCK_SIZE;
                        } else {
                            mask_offset0 = 0;
                        }
                    } else {
                        if(m_idx0 % 2) {
                            mask_offset0 = 192 * BLOCK_SIZE;
                        } else {
                            mask_offset0 = 128 * BLOCK_SIZE;
                        }
                    }
                    delta0 = 128 * n_idx0;

                    if (m_idx0 / 2 == n_idx0 + 1 ) {
                        if(m_idx0 % 2) {
                            mask_offset1 = 64 * BLOCK_SIZE;
                        } else {
                            mask_offset1 = 0;
                        }
                    } else {
                        if(m_idx0 % 2) {
                            mask_offset1 = 192 * BLOCK_SIZE;
                        } else {
                            mask_offset1 = 128 * BLOCK_SIZE;
                        }
                    }
                    delta1 = 128 * (n_idx0 + 1);
                }
            } else if (long_seq == 0) {
                mask_offset0 += (m_idx0 * pp_m_scalar * BLOCK_SIZE + n_idx0 * mask_stride * pp_n_scalar);
                mask_offset1 += (m_idx0 * pp_m_scalar * BLOCK_SIZE + n_idx0 * mask_stride * pp_n_scalar);
            }
            int32_t wrap_o = (n_idx0 == (n_loop - 1) || (n_idx0 + 1) == (n_loop - 1)) ? 1 : 0;
            if (is_triu == 1) {
                wrap_o = (n_idx0 == m_idx0 / 2 || (n_idx0 + 1) == m_idx0 /2) ? 1 : 0;
            }
            int32_t init_g = (n_idx0 == 0) ? 1 : 0;
            int32_t __m0 = (m_idx0 == (m_loop - 1)) ? (q_seqlen_real - m_idx0 * pp_m_scalar) : pp_m_scalar;
            int32_t __n0 = (n_idx0 == (n_loop - 1)) ? (kv_seqlen_real - n_idx0 * pp_n_scalar) : pp_n_scalar;
            int32_t __n1 = ((n_idx0 + 1) == (n_loop - 1)) ? (kv_seqlen_real - (n_idx0 + 1) * pp_n_scalar) : pp_n_scalar;
            int32_t __k0 = embd;
            int32_t round_m0 = (__m0 + 15) / 16 * 16;
            int32_t round_n0 = (__n0 + 15) / 16 * 16;
            int32_t round_k0 = (__k0 + 15) / 16 * 16;
            int32_t round_n1 = (__n1 + 15) / 16 * 16;

            if ((n_idx0 + 1) == (n_loop) || (n_idx0 == (m_idx0 / 2) && is_triu == 1)) {
                    __n1 = -1;
            }
            Init(round_m0, round_n0, round_k0, q_offset, k_offset, v_offset, mask_offset0, mask_offset1, 
                                       o_offset, init_g, wrap_o, q_tokens, mask_stride, logn_offset);

            FlashAttentionNzPrefillCompute(round_m0, round_n0, round_k0, round_n1, __m0, __n0,
                                                                 __n1, pp_n_scalar, q_tight, add_mask_n0, add_mask_n1,
                                                                 long_seq, alibi_coeff, delta0, delta1, scaleType, alibi_left_align);

        }
        if (cur_q_blk_id == cur_proc_num - 1) {
            cur_batch++;
            cur_bms += batch_mask_stride;
        }
    }
    SyncEnd();
}


template <>
__aicore__ inline void UnpadFlashAttentionCommon<float, half, PrecType::BMM1_FP16_EXP_FP32 , PrecType::BMM2_ONLINE_SOFTMAX_FP16>
                    ::Run(AscendC::LocalTensor<int32_t> tiling_para_ub_tensor,
                           AscendC::GlobalTensor<int32_t> tiling_para_gm_tensor,
                           AscendC::LocalTensor<int32_t> layerId_ub_tensor,
                           __gm__ uint8_t *__restrict__ alibi_coeff_gm,
                           uint32_t mask_type, uint32_t window_len, uint32_t long_seq,
                           uint64_t stride_qo, uint64_t stride_kv, int64_t head_mask_stride,
                           int64_t batch_mask_stride,
                           uint32_t start_batch, uint32_t end_batch,
                           int32_t start_blk, int32_t end_blk,
                           uint32_t is_triu, uint32_t alibi_compress_offset, int32_t group_num,
                           uint32_t mask_stride, uint32_t q_tokens, int32_t embd,
                           uint32_t q_tight, uint32_t scaleType,
                           half tor, int32_t kv_copy_stride, uint32_t is_sqrt,
                           int64_t heads, uint32_t max_seqlen, uint32_t batch_size, int32_t kv_real_heads, const uint32_t alibi_left_align)
{
    if (gmSrcLayerid != nullptr) {
        stride_kv = max_seqlen * embd;
        kv_copy_stride = max_seqlen;
        uint64_t stride_batch_kv = batch_size * max_seqlen * kv_real_heads * embd * 2;
        uint32_t layerIdGm = *(__ubuf__ int32_t *)layerId_ub_tensor.GetPhyAddr();
        gmSrck = stride_batch_kv * layerIdGm + gmSrck;
        gmSrcv = stride_batch_kv * layerIdGm + gmSrcv;
    }

    SetEncoderParams(tor, kv_copy_stride, is_sqrt, 0);

    SET_FLAG(S, MTE2, EVENT_ID0);
    WAIT_FLAG(S, MTE2, EVENT_ID0);
    gm_to_ub<ArchType::ASCEND_V200, int32_t>(
        tiling_para_ub_tensor, tiling_para_gm_tensor[BATCH_TILING_OFFSET + start_batch * TILING_PARA_SIZE], 0, 1,
        ((end_batch - start_batch + 1) * 80 + 31) / 32, 0, 0);

    SET_FLAG(MTE2, S, EVENT_ID0);
    WAIT_FLAG(MTE2, S, EVENT_ID0);
    SyncStart();
    __ubuf__ int32_t *tiling_para_ub =
        (__ubuf__ int32_t *)tiling_para_ub_tensor.GetPhyAddr();
    int32_t cur_batch = 0;
    int64_t cur_bms = batch_mask_stride * start_batch;
    for (uint32_t curr_q_blk = start_blk; curr_q_blk < end_blk; curr_q_blk++) {
        // get tiling args
        int32_t offset_tiling = TILING_PARA_SIZE * cur_batch;
        int32_t q_seqlen_aligned = (int32_t)(*((__ubuf__ int32_t *)tiling_para_ub + offset_tiling));
        int32_t kv_seqlen_aligned = (int32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 1 + offset_tiling));
        uint32_t q_seqlen_real = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 13 + offset_tiling));
        uint32_t kv_seqlen_real = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 14 + offset_tiling));
        int32_t pp_m_scalar = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 2 + offset_tiling));
        int32_t pp_n_scalar = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 3 + offset_tiling));
        uint32_t addr_q_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 4 + offset_tiling));
        uint32_t addr_q_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 5 + offset_tiling));
        int64_t addr_q_scalar = (int64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32); // batch offset
        uint32_t addr_k_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 6 + offset_tiling));
        uint32_t addr_k_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 7 + offset_tiling));
        int64_t addr_k_scalar = (int64_t)(((uint64_t)addr_k_high32) << 32 | addr_k_loww32);
        uint32_t addr_v_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 8 + offset_tiling));
        uint32_t addr_v_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 9 + offset_tiling));
        int64_t addr_v_scalar = (int64_t)(((uint64_t)addr_v_high32) << 32 | addr_v_loww32);
        uint32_t addr_o_high32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 10 + offset_tiling));
        uint32_t addr_o_loww32 = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 11 + offset_tiling));
        int64_t addr_o_scalar = (int64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
        uint32_t cur_total_qblk = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 12 + offset_tiling));
        uint32_t cur_proc_num = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 15 + offset_tiling));
        uint32_t cur_q_blk_id = curr_q_blk - (cur_total_qblk - cur_proc_num);
        // SWA calc mode condition
        uint32_t swa_mode = ((mask_type == AttentonMaskType::MASK_TYPE_SWA_NORM
                            || mask_type == AttentonMaskType::MASK_TYPE_SWA_COMPRESS)
                            && kv_seqlen_real > window_len) ? 1 : 0;
        is_triu = ((mask_type == AttentonMaskType::MASK_TYPE_SWA_NORM
                    || mask_type == AttentonMaskType::MASK_TYPE_SWA_COMPRESS)
                    && kv_seqlen_real <= window_len) ? 1 : is_triu;    // Regular Triu Mask

        int32_t m_loop = (q_seqlen_aligned + pp_m_scalar - 1) / pp_m_scalar;
        int32_t n_loop = 0;
        if (swa_mode) {
            n_loop = kv_seqlen_aligned > window_len + pp_n_scalar
                    ? ((window_len + pp_n_scalar - 1) / pp_n_scalar + 1)
                    : ((kv_seqlen_aligned + pp_n_scalar - 1) / pp_n_scalar);
        } else {
            n_loop = (kv_seqlen_aligned + pp_n_scalar - 1) / pp_n_scalar;
        }
        int32_t start = cur_q_blk_id * n_loop;
        int32_t end = start + n_loop;

        for (int32_t loop_idx = start; loop_idx < end; loop_idx += 2) {
            int32_t head_idx0 = loop_idx / (m_loop * n_loop);
            int32_t m_idx0 = loop_idx % (m_loop * n_loop) / n_loop;
            int32_t n_idx0 = loop_idx % (m_loop * n_loop) % n_loop;
            int32_t window_offset = 0; // window_offset will only be overwritten in swa mode.
            if (swa_mode) {
                window_offset = (m_idx0 + 1 > n_loop) ? (m_idx0 - n_loop + 1) : 0;
                is_triu = (window_offset == 0) ? 1 : 0;
            }
            if (is_triu == 1 && n_idx0 > m_idx0) {
                continue;
            }
            int32_t add_mask_n0 = ((long_seq == 0) || ((long_seq == 1) && (n_idx0 == m_idx0)) ||
                                   alibi_coeff_gm != nullptr ||
                                   (mask_type == AttentonMaskType::MASK_TYPE_ALIBI && alibi_compress_offset > 0))
                                      ? 1
                                      : 0;
            int32_t add_mask_n1 = ((long_seq == 0) || ((long_seq == 1) && (n_idx0 + 1 == m_idx0)) ||
                                   alibi_coeff_gm != nullptr ||
                                   (mask_type == AttentonMaskType::MASK_TYPE_ALIBI && alibi_compress_offset > 0))
                                      ? 1
                                      : 0;
            int64_t q_offset = addr_q_scalar + head_idx0 * stride_qo + m_idx0 * pp_m_scalar * BLOCK_SIZE;
            int64_t k_offset = addr_k_scalar + head_idx0 / group_num * stride_kv + (n_idx0 + window_offset) * pp_n_scalar * BLOCK_SIZE;
            int64_t v_offset = addr_v_scalar + head_idx0 / group_num * stride_kv + (n_idx0 + window_offset) * pp_n_scalar * BLOCK_SIZE;
            int64_t o_offset = addr_o_scalar + head_idx0 * stride_qo + m_idx0 * pp_m_scalar * BLOCK_SIZE;
            int64_t logn_offset = m_idx0 * pp_m_scalar;

            int64_t mask_offset0 = cur_bms + head_mask_stride * head_idx0;
            int64_t mask_offset1 = cur_bms + head_mask_stride * head_idx0;
            int32_t delta_uint = 0;
            int32_t delta0 = 0;
            int32_t delta1 = 0;
            half alibi_coeff = 1;
            if (alibi_coeff_gm != nullptr) {
                AsdopsBuffer<ArchType::ASCEND_V200> buf;
                AscendC::LocalTensor<half> alibi_coeff_ub_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>( 5 * UB_UINT8_BLOCK_SIZE + 28 * UB_UINT8_LINE_SIZE);
                AscendC::GlobalTensor<half> alibi_coeff_gm_tensor;
                alibi_coeff_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(alibi_coeff_gm));
                gm_to_ub<ArchType::ASCEND_V200, half>(alibi_coeff_ub_tensor,
                                                    alibi_coeff_gm_tensor, 0, 1, (heads + 15) / 16, 0, 0);
                SET_FLAG(MTE2, S, EVENT_ID0);
                WAIT_FLAG(MTE2, S, EVENT_ID0);
                alibi_coeff = *(__ubuf__ half *)(alibi_coeff_ub_tensor[head_idx0].GetPhyAddr());
                if (m_idx0 == n_idx0) {
                    mask_offset0 = 0;
                } else {
                    mask_offset0 = BASE_MASK_SIZE * BLOCK_SIZE;
                    delta_uint = m_idx0 * pp_m_scalar - n_idx0 * pp_n_scalar;
                    delta0 = delta_uint - BASE_MASK_SIZE;
                }

                if (m_idx0 == n_idx0 + 1) {
                    mask_offset1 = 0;
                } else {
                    mask_offset1 = BASE_MASK_SIZE * BLOCK_SIZE;
                    delta_uint = m_idx0 * pp_m_scalar - (n_idx0 + 1) * pp_n_scalar;
                    delta1 = delta_uint - BASE_MASK_SIZE;
                }
            } else if (mask_type == AttentonMaskType::MASK_TYPE_ALIBI && alibi_compress_offset > 0) {
                if (m_idx0 != n_idx0) {
                    mask_offset0 += (m_idx0 * pp_m_scalar - n_idx0 * pp_n_scalar) * BLOCK_SIZE;
                }

                if (m_idx0 != n_idx0 + 1) {
                    mask_offset1 += (m_idx0 * pp_m_scalar - (n_idx0 + 1) * pp_n_scalar) * BLOCK_SIZE;
                }
            } else if (mask_type == AttentonMaskType::MASK_TYPE_SWA_COMPRESS) { //SWA Compress mode - mask offset
                int32_t window_n_scalar = (window_len > 2 * pp_n_scalar) ? window_len / pp_n_scalar : 2;
                if (n_idx0 == m_idx0 - window_offset) {
                    mask_offset1 += mask_stride * pp_n_scalar;
                } else if ((n_idx0 < m_idx0 - window_offset) && ((m_idx0 - n_idx0) < (window_offset + window_n_scalar))) {
                    mask_offset0 += pp_m_scalar * BLOCK_SIZE;
                    add_mask_n0 = (window_len >= 2 * pp_n_scalar) ? 0 : 1;
                    if (n_idx0 + 1 < m_idx0 - window_offset){
                        mask_offset1 += pp_m_scalar * BLOCK_SIZE;
                        add_mask_n1 = (window_len >= 2 * pp_n_scalar) ? 0 : 1;
                    }
                } else if (n_idx0 == (m_idx0 - window_offset - window_n_scalar)) {
                    mask_offset0 += 2 * pp_m_scalar * BLOCK_SIZE;
                    mask_offset1 += pp_m_scalar * BLOCK_SIZE;
                    add_mask_n1 = (window_len >= 2 * pp_n_scalar) ? 0 : 1;
                } else if (n_idx0 == (m_idx0 - window_offset - window_n_scalar - 1)) {
                    mask_offset0 += 3 * pp_m_scalar * BLOCK_SIZE;
                    mask_offset1 += 2 * pp_m_scalar * BLOCK_SIZE;
                } else {
                    mask_offset0 += mask_stride * pp_n_scalar;
                    mask_offset1 += mask_stride * pp_n_scalar;
                }
            } else if (long_seq == 0) {
                mask_offset0 += (m_idx0 * pp_m_scalar * BLOCK_SIZE + (n_idx0 + window_offset) * mask_stride * pp_n_scalar);
                mask_offset1 += (m_idx0 * pp_m_scalar * BLOCK_SIZE + (n_idx0 + window_offset + 1) * mask_stride * pp_n_scalar);
            }
            int32_t wrap_o = (n_idx0 == (n_loop - 1) || (n_idx0 + 1) == (n_loop - 1)) ? 1 : 0;
            if (is_triu == 1) {
                wrap_o = (n_idx0 == m_idx0 || (n_idx0 + 1) == m_idx0) ? 1 : 0;
            }
            if (swa_mode) {
                if (window_offset == 0) {
                    wrap_o = (n_idx0 == m_idx0 || (n_idx0 + 1) == m_idx0) ? 1 : 0;
                } else {
                    wrap_o = (n_idx0 == (n_loop - 1) || (n_idx0 + 1) == (n_loop - 1)) ? 1 : 0;
                }
            }
            int32_t init_g = (n_idx0 == 0) ? 1 : 0;
            int32_t __m0 = (m_idx0 == (m_loop - 1)) ? (q_seqlen_real - m_idx0 * pp_m_scalar) : pp_m_scalar;
            int32_t __n0 = 0;
            int32_t __n1 = 0;
            if (swa_mode) {
                __n0 = ((n_idx0 + window_offset + 1) * pp_n_scalar <= kv_seqlen_real)
                        ? pp_n_scalar
                        : (kv_seqlen_real - (n_idx0 + window_offset) * pp_n_scalar);
                __n1 = (((n_idx0 + 1) + window_offset + 1) * pp_n_scalar <= kv_seqlen_real)
                        ? pp_n_scalar
                        : (kv_seqlen_real - ((n_idx0 + 1) + window_offset) * pp_n_scalar);
            } else {
                __n0 = (n_idx0 == (n_loop - 1)) ? (kv_seqlen_real - n_idx0 * pp_n_scalar) : pp_n_scalar;
                __n1 = ((n_idx0 + 1) == (n_loop - 1)) ? (kv_seqlen_real - (n_idx0 + 1) * pp_n_scalar) : pp_n_scalar;
            }
            int32_t __k0 = embd;
            int32_t round_m0 = (__m0 + 15) / 16 * 16;
            int32_t round_n0 = (__n0 + 15) / 16 * 16;
            int32_t round_k0 = (__k0 + 15) / 16 * 16;
            int32_t round_n1 = (__n1 + 15) / 16 * 16;

            if ((n_idx0 + 1) == (n_loop) || (n_idx0 == m_idx0 && is_triu == 1)) {
                __n1 = -1;
            }

            Init(round_m0, round_n0, round_k0, q_offset, k_offset, v_offset, mask_offset0, mask_offset1, o_offset, 
                                       init_g, wrap_o, q_tokens, mask_stride, logn_offset, CUBE_UPDATE_O_ENABLED);

            FlashAttentionNzPrefillCompute(round_m0, round_n0, round_k0, round_n1, __m0, __n0,
                                                                 __n1, pp_n_scalar, q_tight, add_mask_n0, add_mask_n1,
                                                                 long_seq, alibi_coeff, delta0, delta1, scaleType, alibi_left_align);

        }
        if (cur_q_blk_id == cur_proc_num - 1) {
            cur_batch++;
            cur_bms += batch_mask_stride;
        }
    }
    SyncEnd();

}

extern "C" __global__ __aicore__ void
flash_attention_prefill(__gm__ uint8_t *__restrict__ q_gm, __gm__ uint8_t *__restrict__ k_gm,
                        __gm__ uint8_t *__restrict__ v_gm, __gm__ uint8_t *__restrict__ layerID_gm,
                        __gm__ uint8_t *__restrict__ mask_gm, __gm__ uint8_t *__restrict__ alibi_coeff_gm,
                        __gm__ uint8_t *__restrict__ logn_gm,
                        __gm__ uint8_t *__restrict__ o_gm, __gm__ uint8_t *__restrict__ tiling_para_gm)
{
    SetMasknorm();
    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
    SetPadding<int8_t>(uint16_t(0));
    SetAtomicnone();
    AsdopsBuffer<ArchType::ASCEND_V200> buf;
    AscendC::LocalTensor<int32_t> layerId_ub_tensor =
        buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(0);
    if (layerID_gm != nullptr) {
        AscendC::GlobalTensor<int32_t> layerID_gm_tensor;
        layerID_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(layerID_gm));
        gm_to_ub<ArchType::ASCEND_V200, int32_t>(layerId_ub_tensor, layerID_gm_tensor, 0, 1, 1, 0, 0);
    }
    const uint32_t tiling_para_ub_offset = 4 * UB_UINT8_BLOCK_SIZE + 9 * UB_UINT8_LINE_SIZE;
    AscendC::LocalTensor<int32_t> tiling_para_ub_tensor =
        buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(tiling_para_ub_offset);
    AscendC::GlobalTensor<int32_t> tiling_para_gm_tensor;
    tiling_para_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(tiling_para_gm));
    __ubuf__ int32_t *tiling_para_ub =
        (__ubuf__ int32_t *)tiling_para_ub_tensor.GetPhyAddr(); // use left ub space to store tilingData
#if __CCE_AICORE__ == 100
    gm_to_ub<ArchType::ASCEND_V200, int32_t>(tiling_para_ub_tensor, tiling_para_gm_tensor, 0, 1, 24, 0, 0);
#else
    gm_to_ub<ArchType::ASCEND_V200, int32_t>(tiling_para_ub_tensor, tiling_para_gm_tensor, 0, 1, 16, 0, 0);
#endif

    SET_FLAG(MTE2, S, EVENT_ID0);
    WAIT_FLAG(MTE2, S, EVENT_ID0);

    uint32_t batch_size = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub));
    uint32_t q_tokens = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 1));
    uint32_t kv_head = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 2));
    int64_t heads = (int32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 3));
    int32_t embd = (int32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 4));
    uint32_t max_seqlen = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 5));
    half tor = (half)(*((__ubuf__ float *)tiling_para_ub + 6));
    uint32_t mask_stride = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 7));
    int64_t head_mask_stride = (int64_t)(*((__ubuf__ int32_t *)tiling_para_ub + 8));
    int64_t batch_mask_stride = (int64_t)(*((__ubuf__ int32_t *)tiling_para_ub + 9));
    uint32_t q_tight = (int64_t)(*((__ubuf__ int32_t *)tiling_para_ub + 10));
    uint32_t is_triu = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 11));
#if __CCE_AICORE__ == 100
    uint32_t data_dim_order = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 141));
    uint32_t alibi_left_align = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 182));
    uint32_t window_len = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 183));
    uint32_t scaleType = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 185));
    uint32_t is_sqrt = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 188));
    uint32_t alibi_compress_offset = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 189));
    uint32_t mask_type = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 190));
    uint32_t long_seq = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 191));
#else
    uint32_t data_dim_order = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 48));
    uint32_t prec_type = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 117));
    uint32_t alibi_left_align = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 118));
    uint32_t window_len = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 119));
    uint32_t scaleType = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 121));
    uint32_t is_sqrt = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 124));
    uint32_t alibi_compress_offset = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 125));
    uint32_t mask_type = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 126));
    uint32_t long_seq = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + 127));
#endif
    uint32_t start_batch = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + block_idx * 4 + 12));
    uint32_t end_batch = (uint32_t)(*((__ubuf__ int32_t *)tiling_para_ub + block_idx * 4 + 13));
    int64_t start_blk = (int64_t)(*((__ubuf__ int32_t *)tiling_para_ub + block_idx * 4 + 14));
    int64_t end_blk = (int64_t)(*((__ubuf__ int32_t *)tiling_para_ub + block_idx * 4 + 15));
    int32_t kv_real_heads = kv_head > 0 ? kv_head : heads;
    int32_t group_num = heads / kv_real_heads;
    int32_t kv_copy_stride = q_tokens;
    uint64_t stride_qo = data_dim_order == 1 ? static_cast<int64_t>(max_seqlen) * embd : static_cast<int64_t>(q_tokens) * embd;
    uint64_t stride_kv = data_dim_order == 1 ? static_cast<int64_t>(max_seqlen) * embd : static_cast<int64_t>(q_tokens) * embd;
    if (alibi_compress_offset) {
        head_mask_stride = head_mask_stride * alibi_compress_offset;
    } else {
        head_mask_stride = head_mask_stride * max_seqlen;
    }
    batch_mask_stride = batch_mask_stride * max_seqlen;
    if (TILING_KEY_IS(8)) {
        UnpadFlashAttentionCommon<float, float, PrecType::BMM1_FP32_EXP_FP32 , PrecType::BMM1_FP32_EXP_FP32> flashAttentionEncoder(q_gm, k_gm, v_gm, mask_gm, layerID_gm, alibi_coeff_gm, logn_gm, o_gm);
        float tor = (float)(*((__ubuf__ float *)tiling_para_ub + 6));
        flashAttentionEncoder.Run(
            tiling_para_ub_tensor,
            tiling_para_gm_tensor,
            layerId_ub_tensor,
            alibi_coeff_gm,
            mask_type, window_len, long_seq,
            stride_qo, stride_kv, head_mask_stride, batch_mask_stride,
            start_batch, end_batch, start_blk, end_blk,
            is_triu, alibi_compress_offset, group_num,
            mask_stride, q_tokens, embd, q_tight, scaleType,
            tor, kv_copy_stride, is_sqrt, heads,
            max_seqlen, batch_size, kv_real_heads, alibi_left_align);
    } else if (TILING_KEY_IS(0)) {
        UnpadFlashAttentionCommon<float, half, PrecType::BMM1_FP16_EXP_FP32, PrecType::BMM1_FP16_EXP_FP32> flashAttentionEncoder(q_gm, k_gm, v_gm, mask_gm, layerID_gm, alibi_coeff_gm, logn_gm, o_gm);
        flashAttentionEncoder.Run(
            tiling_para_ub_tensor,
            tiling_para_gm_tensor,
            layerId_ub_tensor,
            alibi_coeff_gm,
            mask_type, window_len, long_seq,
            stride_qo, stride_kv, head_mask_stride, batch_mask_stride,
            start_batch, end_batch, start_blk, end_blk,
            is_triu, alibi_compress_offset, group_num,
            mask_stride, q_tokens, embd, q_tight, scaleType,
            tor, kv_copy_stride, is_sqrt, heads,
            max_seqlen, batch_size, kv_real_heads, alibi_left_align);
    } else if (TILING_KEY_IS(32)) {
        UnpadFlashAttentionCommon<float, half, PrecType::BMM1_FP16_EXP_FP32, PrecType::BMM2_ONLINE_SOFTMAX_FP16> flashAttentionEncoder(q_gm, k_gm, v_gm, mask_gm, layerID_gm, alibi_coeff_gm, logn_gm, o_gm);
        flashAttentionEncoder.Run(
            tiling_para_ub_tensor,
            tiling_para_gm_tensor,
            layerId_ub_tensor,
            alibi_coeff_gm,
            mask_type, window_len, long_seq,
            stride_qo, stride_kv, head_mask_stride, batch_mask_stride,
            start_batch, end_batch, start_blk, end_blk,
            is_triu, alibi_compress_offset, group_num,
            mask_stride, q_tokens, embd, q_tight, scaleType,
            tor, kv_copy_stride, is_sqrt, heads,
            max_seqlen, batch_size, kv_real_heads, alibi_left_align);
    }
}