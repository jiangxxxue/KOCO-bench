# 从零开始为 ATB 库开发一个 Add 算子
本教程以能跑起来为第一目标，先不追求性能极致，力求让第一次接触ATB的同学 30 min 内能在本地看到结果。
## ATB算子开发交付件
ATB的算子开发流程如图所示：  
![算子开发流程](images/开发流程.png)
### 算子功能
两个输入张量在指定维度相加成一个输出张量。  
![AddcustomOperation](images/AddcustomOperation.png)
### 新增文件
- 在`src/kernels/kernels`下新增`addcustom`目录，该目录下主要存放算子实现部分的代码，文件具体内容见后文，目录结构如下：
    ```
    addcustom
    ├── op_kernel			        // kernel侧实现文件(包括核函数入口、以及实现文件)	
    │   └── addcustom.cpp
    ├── tiling			            // 新增算子tiling
    │   ├── addcustom_tiling.cpp	// tiling实现核心算法
    │   ├── addcustom_tiling.h		// 算子tiling接口
    │   └── tiling_data.h			// tiling和kernel传递结构体tiling_data的定义 
    ├── CMakeLists.txt			    // 新增算子编译CMake
    ├── addcustom_kernel.cpp		// 校验
    └── addcustom_operation.cpp		// shape验证
    ```

- 在`src/ops_infer`下新增目录`addcustom`，该目录下主要存放addcustom算子接入ATB框架部分的代码，文件具体内容见后文，目录结构如下：
    ```
    addcustom
    ├── addcustom_operation.cpp	    // atb接口实现
    ├── addcustom_operation.h	
    ├── addcustom_ops_runner.cpp	// 算子组图
    └── addcustom_ops_runner.h
    ```
- 新增`src/kernels/include/asdops/params/addcustom.h`文件定义 `Addcustom` 操作的参数结构体，内容如下：
    ```c++
    #ifndef ATBOPS_PARAMS_ADDCUSTOM_H
    #define ATBOPS_PARAMS_ADDCUSTOM_H

    #include <cstdint>
    #include <string>
    #include <sstream>
    #include <mki/utils/SVector/SVector.h>

    namespace AsdOps {
    namespace OpParam {
    struct Addcustom {
        int addcustomDim = 0;

        bool operator==(const Addcustom &other) const
        {
            return this->addcustomDim == other.addcustomDim;
        }
    };

    } // namespace OpParam
    } // namespace AtbOps

    #endif
    ```
### 修改文件
- 将新增的头文件`src/kernels/include/asdops/params/addcustom.h`加入到`src/kernels/include/asdops/params/params.h`的头文件中:
    ```c++
    #include "asdops/params/addcustom.h"
    ```
- `src/kernels/configs/kernels/op_list.yaml`文件直接删除或者增加以下内容（这一操作非常重要，将新增算子信息加入列表，后续构建才会将新增算子的实现和接口真正编译进去）：
    ```
    AddcustomOperation:
        AddcustomKernel:
            ascend910b: true
     ```
- 在`src/atb/runner/runner_type.h`中为RunnerType增加枚举值`RUNNER_TYPE_ADDCUSTOM`；
- 在`include/atb/infer_op_params.h`增加以下内容：
    ```c++
    struct AddcustomParam {
        int addcustomDim = 0;
        uint8_t rsv[12] = {0};
    };
    ```
- 在`tests/framework/c++/atb_torch/operation/operation_funcs.cpp`新增以下内容：
  - 把 JSON 描述的`Addcustom`参数反序列化成C++结构体，然后用它创建或更新算子：
    ```c++
    static atb::Status AddcustomOperationCreate(const nlohmann::json &paramJson, atb::Operation **op)
    {
        atb::infer::AddcustomParam param;
        ATB_LOG(INFO) << "AddcustomParam axis:" << param.addcustomDim;
        if (paramJson.contains("addcustomDim")) {
            param.addcustomDim = paramJson["addcustomDim"].get<int>();
        }
        if (paramJson.contains("rsv")) {
            for (size_t i = 0; i < paramJson["rsv"].size(); i++) {
                param.rsv[i] = paramJson["rsv"].at(i).get<int8_t>();
            }
        }
        return CreateOperation(param, op);
    }
    ```
  - 在该文件的`g_funcMap`中增加对应键值对：
    ```c++
    {"AddcustomOperation", &AddcustomOperationCreate},
    ```
- `ops_configs/atb_ops_info.ini`增加算子描述：
    ```
    [AddcustomOperation]
    input0.name=x
    input0.dtype=int32
    input0.format=nd
    input1.name=y
    input1.dtype=int32
    input1.format=nd
    output0.name=output
    output0.dtype=int32
    output0.format=nd
    ```
- `src/atb/utils/param_to_json.cpp`增加如下内容：
    ```c++
    template <> nlohmann::json OpParamToJson(const infer::AddcustomParam &opParam)
    {
        nlohmann::json paramsJson;
        paramsJson["addcustomDim"] = opParam.addcustomDim;
        return paramsJson;
    }
    ```
- `src/atb/core/param_compare.cpp`文件中增加`ASDOPS_PARAM_COMPARE_MAP`的键值对：
    ```c++
    {typeid(AsdOps::OpParam::Addcustom).hash_code(), ParamCompareFuncImpl<AsdOps::OpParam::Addcustom>},
    ```
## 环境准备

您可参考[环境准备](../README.md#环境构建)进行编译编译和测试环境搭建，环境准备好之后，就可以开始ATB的算子开发之旅。
## ATB算子实现
ATB算子实现主要包括：kernel侧算子实现和host侧tiling实现。
### tiling开发
tiling开发的核心概念：`TilingData`、`Workspace`、`TilingKey`、`BlockDim`等，可访问[术语表-CANN商用版8.2.RC1-昇腾社区](https://www.hiascend.com/document/detail/zh/canncommercial/82RC1/opdevg/Ascendcopdevg/atlas_ascendc_10_00013.html)查看。

#### tiling_data.h

文件路径：`src/kernels/kernels/addcustom/tiling/tiling_data.h`  
主要功能：描述算子的输入输出数据的数据结构定义。

```c++
#ifndef ASCEND_OPS_ADDCUSTOM_TILING_DATA
#define ASCEND_OPS_ADDCUSTOM_TILING_DATA

#include <cstdint>

namespace AsdOps {
struct AddcustomTilingData {
    uint32_t totalLength;  // 总数据长度
    uint32_t tileNum;      // Tiling 块数
};
}
#endif  // ASCEND_OPS_ADD_CUSTOM_TILING_DATA
```
#### addcustom_tiling.h
文件路径：`src/kernels/kernels/addcustom/tiling/addcustom_tiling.h`  
主要功能：tiling过程主要是完成数据的切分，因此其主体函数是实现切分功能的函数，这里则是函数声明。
```c++
#ifndef ASCEND_OPS_ADDCUSTOM_TILING_H
#define ASCEND_OPS_ADDCUSTOM_TILING_H

#include <mki/launch_param.h>
#include <mki/kernel_info.h>
#include <mki/utils/status/status.h>

namespace AsdOps {
using namespace Mki;
Status AddcustomTiling(const LaunchParam &launchParam, KernelInfo &kernelInfo);
} // namespace AsdOps

#endif
```


#### addcustom_tiling.cpp
文件路径：`src/kernels/kernels/addcustom/tiling/addcustom_tiling.cpp`  
主要功能：实现切分功能的主体函数  
```c++
#include "addcustom_tiling.h"
#include <mki/utils/assert/assert.h>
#include <mki/utils/log/log.h>
#include <mki/utils/platform/platform_info.h>
#include <mki/utils/math/math.h>
#include <mki/utils/SVector/SVector.h>
#include "asdops/params/addcustom.h"
#include "tiling_data.h"

// 定义最小的块长度
constexpr uint32_t MIN_BLOCK_LENGTH = 1;

namespace AsdOps {
Status AddcustomTiling(const LaunchParam &launchParam, KernelInfo &kernelInfo)
{
    AddcustomTilingData *tilingDataPointer =
        reinterpret_cast<AddcustomTilingData *>(kernelInfo.GetTilingHostAddr());
    MKI_CHECK(tilingDataPointer != nullptr, "tilingDataPtr should not be empty",
              return Status::FailStatus(ERROR_INVALID_VALUE, "tilingDataPtr should not be empty"));

    if (launchParam.GetParam().Type() != typeid(OpParam::Addcustom)) {
        return Status::FailStatus(
            ERROR_ATTR_INVALID_TYPE,
            "Failed to check addcustom param, type of specificParam is not equals to OpParam::Addcustom");
    }

    // 获取输入张量的维度
    const uint32_t totalLength = launchParam.GetInTensor(0).desc.dims.at(0);
    MKI_LOG(INFO) << "Total length is " << totalLength;

    // 计算核心数
    uint32_t coreNum = PlatformInfo::Instance().GetCoreNum(CoreType::CORE_TYPE_VECTOR);
    MKI_LOG(INFO) << "Core number is " << coreNum;

    // 计算每个核心的块数
    uint32_t blockDims = std::min<uint32_t>((totalLength + MIN_BLOCK_LENGTH - 1) / MIN_BLOCK_LENGTH, coreNum);
    tilingDataPointer->tileNum = blockDims;
    tilingDataPointer->totalLength = totalLength;

    MKI_LOG(INFO) << "BlockDims is " << blockDims;
    MKI_LOG(INFO) << "Total length is " << tilingDataPointer->totalLength;
    MKI_LOG(INFO) << "Tile number is " << tilingDataPointer->tileNum;

    kernelInfo.SetBlockDim(blockDims);
    return Status::OkStatus();
}
} // namespace AsdOps
```
### kernel开发
kernel相关的`Compute`、`CopyIn`、`CopyOut`等概念，可访问[术语表-CANN商用版8.2.RC1-昇腾社区](https://www.hiascend.com/document/detail/zh/canncommercial/82RC1/opdevg/Ascendcopdevg/atlas_ascendc_10_00013.html)查看。
#### addcustom.cpp
文件路径：`src/kernels/kernels/addcustom/op_kernel/addcustom.cpp`  
主要功能：根据tiling信息完成所有数据的搬运和计算。
```c++
#include "kernel_operator.h"
#include "kernels/utils/kernel/kernel_utils.h"
#include "kernels/addcustom/tiling/tiling_data.h"
using namespace AscendC;

static constexpr uint32_t BUFFER_NUM = 2;
static constexpr uint32_t MAX_UB_SIZE = 188 * 1024; // double buffer, 每块94KB共188KB

class Addcustom {
public:
    __aicore__ inline Addcustom() {}
    __aicore__ inline void Init(GM_ADDR x, GM_ADDR y, GM_ADDR z, AsdOps::AddcustomTilingData *tdata)
    {
        this->totalLength = tdata->totalLength;
        this->tileNum = tdata->tileNum;
        this->blockLength = totalLength / GetBlockNum();
        this->tileLength = blockLength / tileNum / BUFFER_NUM;

        // 设置全局变量的起始地址与总长度
        xGm.SetGlobalBuffer((__gm__ float *)x, totalLength * sizeof(float));
        yGm.SetGlobalBuffer((__gm__ float *)y, totalLength * sizeof(float));
        zGm.SetGlobalBuffer((__gm__ float *)z, totalLength * sizeof(float));

        // 初始化管道和队列
        pipe.InitBuffer(inQueueX, BUFFER_NUM, tileLength * sizeof(float));
        pipe.InitBuffer(inQueueY, BUFFER_NUM, tileLength * sizeof(float));
        pipe.InitBuffer(outQueueZ, BUFFER_NUM, tileLength * sizeof(float));
    }
    __aicore__ inline void Process()
    {
        for (int32_t i = 0; i < tileNum; i++) {
            CopyIn(i);
            Compute(i);
            CopyOut(i);
        }
    }

private:
    __aicore__ inline void CopyIn(int32_t progress)
    {
        // 分配局部内存
        LocalTensor<float> xLocal = inQueueX.AllocTensor<float>();
        LocalTensor<float> yLocal = inQueueY.AllocTensor<float>();

        // 从全局内存复制数据到局部内存
        DataCopy(xLocal, xGm[progress * tileLength], tileLength * sizeof(float));
        DataCopy(yLocal, yGm[progress * tileLength], tileLength * sizeof(float));

        // 将局部内存加入队列
        inQueueX.EnQue(xLocal);
        inQueueY.EnQue(yLocal);
    }
    __aicore__ inline void Compute(int32_t progress)
    {
        // 从队列中获取局部内存
        LocalTensor<float> xLocal = inQueueX.DeQue<float>();
        LocalTensor<float> yLocal = inQueueY.DeQue<float>();
        LocalTensor<float> zLocal = outQueueZ.AllocTensor<float>();

        // 执行逐元素相加
        Add(zLocal, xLocal, yLocal, tileLength);

        // 将结果加入输出队列
        outQueueZ.EnQue(zLocal);

        // 释放输入局部内存
        inQueueX.FreeTensor(xLocal);
        inQueueY.FreeTensor(yLocal);
    }
    __aicore__ inline void CopyOut(int32_t progress)
    {
        // 从输出队列中获取局部内存
        LocalTensor<float> zLocal = outQueueZ.DeQue<float>();

        // 将结果从局部内存复制到全局内存
        DataCopy(zGm[progress * tileLength], zLocal, tileLength * sizeof(float));

        // 释放局部内存
        outQueueZ.FreeTensor(zLocal);
    }

private:
    TPipe pipe;
    TQue<QuePosition::VECIN, BUFFER_NUM> inQueueX, inQueueY;
    TQue<QuePosition::VECOUT, BUFFER_NUM> outQueueZ;
    GlobalTensor<float> xGm;
    GlobalTensor<float> yGm;
    GlobalTensor<float> zGm;
    uint32_t totalLength;
    uint32_t tileNum;
    uint32_t blockLength;
    uint32_t tileLength;
};

inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata, AsdOps::AddcustomTilingData *tilingdata)
{
    tilingdata->totalLength = ((const __gm__ uint32_t *)p_tilingdata)[0];
    tilingdata->tileNum = ((const __gm__ uint32_t *)p_tilingdata)[1];
}

extern "C" __global__ __aicore__ void addcustom(GM_ADDR x, GM_ADDR y, GM_ADDR z, GM_ADDR tiling)
{
    Addcustom op;
    AsdOps::AddcustomTilingData tdata;
    InitTilingData(tiling, &tdata);
    op.Init(x, y, z, &tdata);
    op.Process();
}
```
#### addcustom_kernel.cpp
文件路径：`src/kernels/kernels/addcustom/addcustom_kernel.cpp`
主要功能：启动kernel侧实现前，对输入和输出进行了检查，以及进行kernel侧的初始化。
```c++
#include <mki/base/kernel_base.h>
#include <mki_loader/op_register.h>
#include <mki/utils/assert/assert.h>
#include <mki/utils/log/log.h>
#include "asdops/params/params.h"
#include "kernels/addcustom/tiling/addcustom_tiling.h"
#include "kernels/addcustom/tiling/tiling_data.h"

namespace AsdOps {

class AddcustomKernel : public KernelBase {
public:
    explicit AddcustomKernel(const std::string &kernelName, const BinHandle *handle) noexcept
        : KernelBase(kernelName, handle)
    {
    }

    /* --------- 框架回调 --------- */
    bool CanSupport(const LaunchParam &launchParam) const override
    {
        MKI_CHECK(launchParam.GetParam().Type() == typeid(OpParam::Addcustom),
                     "Addcustom valid: param type invalid", return false);
        return true;
    }

    uint64_t GetTilingSize(const LaunchParam &launchParam) const override
    {
        (void)launchParam;
        return sizeof(AddcustomTilingData);
    }

    Status InitImpl(const LaunchParam &launchParam) override
    {
        return AddcustomTiling(launchParam, kernelInfo_);   // 复用前面写的 tiling 函数
    }
};

/* ---------- 注册到框架 ---------- */
REG_KERNEL_BASE(AddcustomKernel);

}   // namespace AsdOps
```
#### addcustom_operation.cpp

文件路径：`src/kernels/kernels/addcustom/addcustom_operation.cpp`  
主要功能：选择最佳kernel函数。

```c++
#include <mki/base/operation_base.h>
#include <mki_loader/op_register.h>
#include <mki/utils/log/log.h>
#include "asdops/params/params.h"

namespace AsdOps {
using namespace Mki;

static constexpr int32_t INPUT_NUM  = 2;
static constexpr int32_t OUTPUT_NUM = 1;

class AddcustomOperation : public OperationBase {
public:
    explicit AddcustomOperation(const std::string &opName) noexcept : OperationBase(opName) {}

    /* ---------- 选择内核 ---------- */
    Kernel *GetBestKernel(const LaunchParam &launchParam) const override
    {
        MKI_CHECK(IsConsistent(launchParam), "Fail to check consistent", return nullptr);

        MKI_CHECK(launchParam.GetParam().Type() == typeid(OpParam::Addcustom),
                     "OpParam invalid", return nullptr);

        auto dtype = launchParam.GetInTensor(0).desc.dtype;
        MKI_CHECK(dtype == TENSOR_DTYPE_INT32,
                     "Only int32 supported", return nullptr);

        return GetKernelByName("AddcustomKernel");   // 内核名字必须与注册一致
    }

    /* ---------- 张量数量 ---------- */
    int64_t GetInputNum(const Any &specificParam) const override
    {
        (void)specificParam;
        return INPUT_NUM;
    }

    int64_t GetOutputNum(const Any &specificParam) const override
    {
        (void)specificParam;
        return OUTPUT_NUM;
    }

protected:
    /* ---------- 形状推导 ---------- */
    Status InferShapeImpl(const LaunchParam &launchParam,
                          SVector<Tensor> &outTensors) const override
    {
        MKI_CHECK(launchParam.GetParam().Type() == typeid(OpParam::Addcustom),
                     "no match param type",
                     return Status::FailStatus(ERROR_INFERSHAPE_ERROR, "OpParam invalid"));

        /* 输出形状 = 输入形状 */
        outTensors[0].desc = launchParam.GetInTensor(0).desc;
        return Status::OkStatus();
    }
};

/* ---------- 注册 ---------- */
REG_OPERATION(AddcustomOperation);

}  // namespace AsdOps
```
## ATB框架接入

ATB算子开发相比其它非ATB算子开发的过程，在ATB算子实现之后，需要开发接口，将算子实现接入ATB。接入过程包含`Runner`和`Operation`的开发。  
**Operation**：`Operation` 是 ATB 加速库中面向用户的前端接口，用于创建和管理算子。用户通过创建 `Operation` 对象来执行算子。  
**Runner**： `Operation`的执行单元，可以理解为`Operation` 的后端处理逻辑。`Runner` 负责具体执行算子的计算任务，包括调用内核函数、管理设备内存、处理 Tiling 数据等。
### Runner

#### addcustom_ops_runner.h
文件路径：`src/ops_infer/addcustom/addcustom_ops_runner.h`  
主要功能：后端处理逻辑函数声明头文件。
```c++
#ifndef ATB_ADDCUSTOM_OPS_RUNNER_H
#define ATB_ADDCUSTOM_OPS_RUNNER_H

#include "atb/infer_op_params.h"
#include "atb/runner/ops_runner.h"

namespace atb {

class AddcustomOpsRunner : public OpsRunner {
public:
    explicit AddcustomOpsRunner(const infer::AddcustomParam &param);
    ~AddcustomOpsRunner() override = default;

protected:
    Status SetupKernelGraph(const OpsTensorPack &opsTensorPack) override;
    Status SetupKernelGraphAddInt32();
private:
    infer::AddcustomParam param_;
};

}  // namespace atb
#endif  // ATB_ADDCUSTOM_OPS_RUNNER_H
```
#### addcustom_ops_runner.cpp
文件路径：`src/ops_infer/addcustom/addcustom_ops_runner.cpp`  
主要功能：后端处理逻辑。
```c++
#include "addcustom_ops_runner.h"
#include <asdops/params/params.h>
#include <atb/utils/log.h>

namespace atb {

AddcustomOpsRunner::AddcustomOpsRunner(const infer::AddcustomParam &param)
    : OpsRunner("AddcustomOpsRunner", RUNNER_TYPE_ADDCUSTOM), param_(param)
{
    ATB_LOG(INFO) << "AddcustomOpsRunner::AddcustomOpsRunner";
}

/* ---------- 框架回调 ---------- */
Status AddcustomOpsRunner::SetupKernelGraph(const OpsTensorPack &opsTensorPack)
{
    (void)opsTensorPack;   // 无需额外校验

    const size_t inTensorNum  = 2;
    const size_t outTensorNum = 1;
    const size_t nodeNum      = 1;

    kernelGraph_.inTensors.resize(inTensorNum);
    kernelGraph_.outTensors.resize(outTensorNum);
    kernelGraph_.internalTensors.clear();
    kernelGraph_.nodes.resize(nodeNum);

    return SetupKernelGraphAddInt32();
}

/* ---------- 建立计算图 ---------- */
Status AddcustomOpsRunner::SetupKernelGraphAddInt32()
{
    ATB_LOG(INFO) << GetLogPrefix() << "AddcustomOpsRunner::SetupKernelGraphAddInt32";

    /* 输入 0：x，输入 1：y */
    Mki::Tensor &xTensor = kernelGraph_.inTensors[0];
    Mki::Tensor &yTensor = kernelGraph_.inTensors[1];

    /* 输出 0：z = x + y */
    Mki::Tensor &zTensor = kernelGraph_.outTensors[0];

    /* 单个 Addcustom 计算节点 */
    KernelGraphNode &addNode = kernelGraph_.nodes[0];

    AsdOps::OpParam::Addcustom addParam{param_.addcustomDim};   // 如无需可填 0
    addNode.opDesc    = {0, "AddcustomOperation", addParam};
    addNode.inTensors = {&xTensor, &yTensor};
    addNode.outTensors = {&zTensor};

    return NO_ERROR;
}

}  // namespace atb
```
### Operation

#### addcustom_operation.h
文件路径：`src/ops_infer/addcustom/addcustom_operation.h`  
主要功能：创建和管理算子函数的声明头文件。
```c++
#ifndef ATB_Addcustom_OPERATION_H
#define ATB_Addcustom_OPERATION_H

#include "atb/infer_op_params.h"
#include "atb/operation/operation_base.h"

namespace atb {

class AddcustomOperation : public OperationBase {
public:
    explicit AddcustomOperation(const infer::AddcustomParam &param);
    ~AddcustomOperation() override = default;

    uint32_t GetInputNum()  const override;
    uint32_t GetOutputNum() const override;

protected:
    Status InferShapeImpl(const SVector<TensorDesc> &inTensorDescs,
                          SVector<TensorDesc> &outTensorDescs) const override;
    Status InferShapeCheckImpl(const SVector<TensorDesc> &inTensorDescs) const override;
    Status SetupCheckImpl(const SVector<Tensor> &inTensors,
                          const SVector<Tensor> &outTensors) const override;
    std::shared_ptr<Runner> CreateRunner(Context &context) const override;
    nlohmann::json GetParamJson() const override;

private:
    Status InTensorDescsCheck(const SVector<TensorDesc> &inTensorDescs) const;

private:
    infer::AddcustomParam param_;
};

} // namespace atb
#endif
```
#### addcustom_operation.cpp
文件路径：`src/ops_infer/addcustom/addcustom_operation.cpp`  
主要功能：创建和管理算子函数的实现。
```c++
#include "addcustom_operation.h"

#include "addcustom_ops_runner.h"
#include "atb/core/atb_operation_ir_cfg.h"
#include "atb/core/op_param_funcs.h"
#include "atb/utils/operation_util.h"
#include "atb/utils/param_to_json.h"
#include "atb/utils/singleton.h"
#include "atb/utils/tensor_check.h"

namespace atb {

/* ---------- 工厂函数 ---------- */
template <>
Status CreateOperation(const infer::AddcustomParam &opParam, Operation **operation)
{
    if (operation == nullptr) return ERROR_INVALID_PARAM;
    OP_PARAM_RSV_CHECK(opParam);

    *operation = new (std::nothrow) AddcustomOperation(opParam);
    if (*operation == nullptr) {
        ATB_LOG(ERROR) << "failed to new AddcustomOperation";
        return ERROR_INTERNAL_ERROR;
    }
    return NO_ERROR;
}

/* ---------- 构造 / 析构 ---------- */
AddcustomOperation::AddcustomOperation(const infer::AddcustomParam &param)
    : OperationBase("AddcustomOperation"), param_(param)
{
    operationIr_ = GetSingleton<AtbOperationIrCfg>().GetOperationIr("AddcustomOperation");
}

/* ---------- 输入/输出数量 ---------- */
uint32_t AddcustomOperation::GetInputNum() const { return 2; }
uint32_t AddcustomOperation::GetOutputNum() const { return 1; }

/* ---------- shape 推导 ---------- */
Status AddcustomOperation::InferShapeImpl(const SVector<TensorDesc> &inTensorDescs,
                                          SVector<TensorDesc> &outTensorDescs) const
{
    outTensorDescs[0] = inTensorDescs[0];  // 输出与输入 0 完全一致
    return NO_ERROR;
}

/* ---------- 校验 ---------- */
Status AddcustomOperation::InferShapeCheckImpl(const SVector<TensorDesc> &inTensorDescs) const
{
    return InTensorDescsCheck(inTensorDescs);
}

Status AddcustomOperation::SetupCheckImpl(const SVector<Tensor> &inTensors, const SVector<Tensor> &outTensors) const
{
    SVector<TensorDesc> inTensorDescs;
    OperationUtil::InTensorsToInTensorDescs(inTensors, inTensorDescs);
    ATB_LOG(DEBUG) << "outTensors size:" << outTensors.size();

    return InTensorDescsCheck(inTensorDescs);
}

/* ---------- 创建 Runner ---------- */
std::shared_ptr<Runner> AddcustomOperation::CreateRunner(Context &context) const
{
    (void)context;
    return std::make_shared<AddcustomOpsRunner>(param_);
}

/* ---------- 参数 JSON ---------- */
nlohmann::json AddcustomOperation::GetParamJson() const { return OpParamToJson(param_); }

/* ---------- 具体校验 ---------- */
Status AddcustomOperation::InTensorDescsCheck(const SVector<TensorDesc> &inTensorDescs) const
{
    // 数量
    if (inTensorDescs.size() != 2) {
        ATB_LOG(ERROR) << GetLogPrefix() << "Addcustom expect 2 inputs";
        return ERROR_INVALID_PARAM;
    }

    // shape
    if (inTensorDescs[0].shape.dimNum != inTensorDescs[1].shape.dimNum) {
        ATB_LOG(ERROR) << GetLogPrefix() << "Addcustom inputs must have same shape";
        return ERROR_INVALID_PARAM;
    }
    return NO_ERROR;
}

}  // namespace atb
```

## 构建与测试

### CMakeLists.txt
文件路径：`src/kernels/kernels/addcustom/CMakeLists.txt`  
主要功能：文件编译。
```
set(addcustom_srcs
    ${CMAKE_CURRENT_LIST_DIR}/addcustom_operation.cpp
    ${CMAKE_CURRENT_LIST_DIR}/addcustom_kernel.cpp
    ${CMAKE_CURRENT_LIST_DIR}/tiling/addcustom_tiling.cpp
)

add_operation(AddcustomOperation "${addcustom_srcs}")

add_kernel(addcustom ascend910b vector
    op_kernel/addcustom.cpp
    AddcustomKernel)
```

### ATB编译与环境变量设置
ATB仓的构建脚本文件为`scripts/build.sh`，脚本使用的基本命令是：
```shell
bash scripts/build.sh --no-pybind
```
编译后需要设置环境变量：

```shell
source output/atb/set_env.sh
```

### 测试
`example/op_demo`目录下，存放了多个不依赖测试框架、即编可用的算子调用Demo示例。算子开发完成之后，用户可编写用例对算子进行功能测试，编写方法可参考[算子使用指导](https://www.hiascend.com/document/detail/zh/canncommercial/82RC1/acce/ascendtb/ascendtb_0046.html)。  
新建`example/op_demo/addcustom`文件夹，算子调用示例的目录结构如下：
```
addcustom
├── addcustom_demo.cpp	// 调用示例的代码文件
└── build.sh            // 编译文件
```
#### addcustom_demo.cpp
文件路径：`example/op_demo/addcustom/addcustom_demo.cpp`  
主体功能：创建`AddcustomOperation`并进行调用。
```c++
#include "../demo_util.h"

const int32_t DEVICE_ID = 0;
const uint32_t DIM_0 = 8;
const uint32_t DIM_1 = 512;

/**
 * @brief 准备 Addcustom 输入 tensor（两输入同 shape）
 */
atb::Status PrepareInTensor(atb::Context *contextPtr, aclrtStream stream, atb::SVector<atb::Tensor> &inTensors)
{
    // 输入 x：[8, 512]，int32，全 1
    atb::Tensor x;
    CHECK_STATUS(CreateTensorFromVector(contextPtr, stream, std::vector<int32_t>(DIM_0 * DIM_1, 1), ACL_INT32,
                                        aclFormat::ACL_FORMAT_ND, {DIM_0, DIM_1}, x));

    // 输入 y：[8, 512]，int32，全 2
    atb::Tensor y;
    CHECK_STATUS(CreateTensorFromVector(contextPtr, stream, std::vector<int32_t>(DIM_0 * DIM_1, 2), ACL_INT32,
                                        aclFormat::ACL_FORMAT_ND, {DIM_0, DIM_1}, y));

    inTensors = {x, y};
    return atb::ErrorType::NO_ERROR;
}

/**
 * @brief 创建 AddcustomOperation
 */
atb::Status CreateAddcustomOperation(atb::Operation **addOp)
{
    atb::infer::AddcustomParam param;
    param.addcustomDim = 0;
    CHECK_STATUS(atb::CreateOperation(param, addOp));
    return atb::ErrorType::NO_ERROR;
}

int main(int argc, char **argv)
{
    CHECK_STATUS(aclInit(nullptr));
    int32_t deviceId = 0;
    CHECK_STATUS(aclrtSetDevice(deviceId));

    atb::Context *context = nullptr;
    CHECK_STATUS(atb::CreateContext(&context));
    void *stream = nullptr;
    CHECK_STATUS(aclrtCreateStream(&stream));
    context->SetExecuteStream(stream);

    /* 创建 op */
    atb::Operation *addOp = nullptr;
    CHECK_STATUS(CreateAddcustomOperation(&addOp));

    /* 准备输入输出 */
    atb::VariantPack variantPack;
    CHECK_STATUS(PrepareInTensor(context, stream, variantPack.inTensors));

    atb::Tensor output;
    CHECK_STATUS(CreateTensorFromVector(context, stream, std::vector<int32_t>(DIM_0 * DIM_1, 0),  // 占位即可
                                        ACL_INT32, aclFormat::ACL_FORMAT_ND, {DIM_0, DIM_1}, output));
    variantPack.outTensors = {output};

    /* Setup -> Execute */
    uint64_t workspaceSize = 0;
    CHECK_STATUS(addOp->Setup(variantPack, workspaceSize, context));
    uint8_t *workspacePtr = nullptr;
    if (workspaceSize > 0) {
        CHECK_STATUS(aclrtMalloc((void **)&workspacePtr, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST));
    }
    CHECK_STATUS(addOp->Execute(variantPack, workspacePtr, workspaceSize, context));
    CHECK_STATUS(aclrtSynchronizeStream(stream));

    /* 释放资源 */
    for (auto &t : variantPack.inTensors) aclrtFree(t.deviceData);
    for (auto &t : variantPack.outTensors) aclrtFree(t.deviceData);
    if (workspacePtr) {
        aclrtFree(workspacePtr);
    }
    atb::DestroyOperation(addOp);
    aclrtDestroyStream(stream);
    DestroyContext(context);
    aclFinalize();
    std::cout << "Addcustom demo success!" << std::endl;
    return 0;
}
```
#### build.sh
文件路径：`example/op_demo/addcustom/build.sh`  
主要功能：`编译并执行addcustom_demo`。
``
```shell
cxx_abi=$(python3 -c '
try:
    import torch
    print("1" if torch.compiled_with_cxx11_abi() else "0")
except ImportError:
    print("1")
')

echo "Using cxx_abi=$cxx_abi"

g++ -D_GLIBCXX_USE_CXX11_ABI=$cxx_abi -I "${ATB_HOME_PATH}/include" -I "${ASCEND_HOME_PATH}/include" -L "${ATB_HOME_PATH}/lib" -L "${ASCEND_HOME_PATH}/lib64" \
addcustom_demo.cpp ../demo_util.h -l atb -l ascendcl -o addcustom_demo
./addcustom_demo
```

#### 调用示例执行

```shell
bash build.sh
```
执行成功输出内容如下：
```
Using cxx_abi=0
Addcustom demo success!
```
#### 性能和精度测试
调用示例对算子进行了功能测试，若您想了解关于算子性能测试和精度测试的内容，可前往：
- [精度测试](开发指南.md#精度测试)
- [性能测试](开发指南.md#性能测试)