/*
* Copyright (c) 2024 Huawei Technologies Co., Ltd.
* This file is a part of the CANN Open Software.
* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

#include "kernels/utils/kernel/common.h"
#include "kernels/utils/kernel/common_func.h"
#include "kernels/utils/kernel/simd.h"
#include "kernels/utils/kernel/iterator.h"
#include "kernels/utils/kernel/mma.h"
#include "kernels/utils/kernel/utils.h"
#include "kernel_operator.h"

#ifdef __DAV_C220_VEC__
#include "fa_common.cce"
#endif

#ifdef __CCE_KT_TEST__
#define __aicore__
#else
#define __aicore__ [aicore]
#endif

#define KVOFFSET_PA true

// FFTS Flag
constexpr int32_t QK_READY = 1;
constexpr int32_t SOFTMAX_READY = 2;
constexpr int32_t UPDATE_READY = 3;
constexpr int32_t BIT_SHIFT = 8;
constexpr int32_t SOFTMAX_MAX_LENGTH = 256;
constexpr int32_t TMP_SIZE = 32768 * 8;               // 128 * 256

#ifdef __DAV_C220_CUBE__
constexpr int32_t L0AB_HALF_BUF_SIZE = 16384;  // 128 * 128
constexpr int32_t BLOCK_SIZE = 16;
constexpr int32_t CUBE_MATRIX_SIZE = 256;         // 16 * 16
constexpr int32_t L0AB_UINT8_BLOCK_SIZE = 32768;  // 128 * 128 * 2B
constexpr int32_t KV_DB_SIZE = 65536;  // 128 * 128 * 2B

template<typename IN_DATA_TYPE, typename QKV_DT = IN_DATA_TYPE, typename O_DT = IN_DATA_TYPE>
class FlashAttentionEncoderHighPrecision {
public:
    __aicore__ __attribute__((always_inline)) inline FlashAttentionEncoderHighPrecision(
        __gm__ uint8_t *__restrict__ sync, __gm__ uint8_t *__restrict__ q_gm,
        __gm__ uint8_t *__restrict__ k_gm, __gm__ uint8_t *__restrict__ v_gm,
        __gm__ uint8_t *__restrict__ block_table_gm, __gm__ uint8_t *__restrict__ s_gm,
        __gm__ uint8_t *__restrict__ p_gm, __gm__ uint8_t *__restrict__ o_tmp_gm,
        __gm__ uint8_t *__restrict__ tiling_para_gm) :
        q_gm(q_gm), s_gm(s_gm), p_gm(p_gm), o_tmp_gm(o_tmp_gm),
        tiling_para_gm(tiling_para_gm)
    {
        SetFftsBaseAddr((unsigned long)sync);
        SetPadding<uint64_t>(0);
        SetAtomicnone();
        SetNdpara(1, 0, 0);
        SetMasknorm();

        this->batch_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm));
        this->max_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1));
        this->q_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2));
        this->embd = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3));
        this->kv_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 4));
        this->is_triu_mask = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 8));
        this->total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9));
        this->tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 14));
        this->tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 15));
        this->tilingKey = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 16));
        this->max_kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 18));
        this->window_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 27));
        this->max_num_blocks_per_query = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 28));
        this->group_num = q_heads / kv_heads;
        this->stride_qo = q_heads * embd;
        this->stride_kv = kv_heads * embd;

        this->batch_stride_kv = batch_size * max_kv_seqlen * kv_heads * embd * sizeof(QKV_DT);
        this->__k = embd;
        this->round_k = (__k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

        this->k_gm = k_gm;
        this->v_gm = v_gm;
        this->block_table_gm = block_table_gm;

        q_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(this->q_gm));
        k_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(this->k_gm));
        v_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(this->v_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->s_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(this->p_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->o_tmp_gm));

        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(M, MTE1, EVENT_ID2);
        SET_FLAG(M, MTE1, EVENT_ID3);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);
    }

    __aicore__ __attribute__((always_inline)) inline ~FlashAttentionEncoderHighPrecision()
    {
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(M, MTE1, EVENT_ID2);
        WAIT_FLAG(M, MTE1, EVENT_ID3);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        PIPE_BARRIER(ALL);
    }

    __aicore__ __attribute__((always_inline)) inline uint32_t GetTilingKey()
    {
        return this->tilingKey;
    }

    __aicore__ __attribute__((always_inline)) inline void LoadDataToCa(
        AscendC::LocalTensor<QKV_DT> dst_tensor, AscendC::LocalTensor<QKV_DT> src_tensor,
        uint32_t round_k, uint32_t qk_round_m, uint32_t qk_m)
    {
        uint32_t round_row = RoundUp<uint32_t>(round_k, 32 / sizeof(QKV_DT));
        if (qk_m == 1) {
            l1_to_l0_a<ArchType::ASCEND_V220, QKV_DT, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                dst_tensor,
                src_tensor,
                0,
                NumMatrixsRoundUp<QKV_DT>(round_row), // repeat
                0,
                1,                                                   // srcStride
                0,
                0                                                   // dstStride
            );
        } else {
            l1_to_l0_a<ArchType::ASCEND_V220, QKV_DT, false, DataFormat::NZ, DataFormat::ZZ>(
                dst_tensor, src_tensor, qk_round_m, round_row, 0, 0, 0, 0);
        }
    }

    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        uint64_t cur_batch = 0;
        uint32_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 13 + offset_tiling));
        uint32_t process_num = total_q_blk_num * q_heads;
        uint32_t next_process = 0;
        for (uint32_t process = block_idx; process < process_num; process = next_process) {
            while (process >= cur_total_q_blk_num * q_heads) {
                cur_batch++;
                pre_total_q_blk_num = cur_total_q_blk_num;
                offset_tiling += tiling_para_size;
                cur_total_q_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 13 + offset_tiling));
            }
            next_process = process + block_num;
            if (is_triu_mask) {
                uint32_t curr_iter = process / block_num;
                next_process = curr_iter % 2 == 1 ? (curr_iter + 1) * block_num + block_idx : (curr_iter + 2) * block_num - 1 - block_idx;
            }
            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            if (q_seqlen == 0 || kv_seqlen == 0) {
                continue;
            }
            // 128 * 128 / 128 
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_q_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 4 + offset_tiling));
            uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 5 + offset_tiling));
            uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);
            uint32_t addr_k_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 6 + offset_tiling));
            uint32_t addr_k_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 7 + offset_tiling));
            uint64_t addr_k_scalar = (uint64_t)(((uint64_t)addr_k_high32) << 32 | addr_k_loww32);
            uint32_t addr_v_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 8 + offset_tiling));
            uint32_t addr_v_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
            uint64_t addr_v_scalar = (uint64_t)(((uint64_t)addr_v_high32) << 32 | addr_v_loww32);
            uint32_t process_idx = process - pre_total_q_blk_num * q_heads;
            uint32_t m_idx = process_idx / q_heads;
            uint64_t head_idx = process_idx % q_heads;
            uint32_t prefixLen = kv_seqlen - q_seqlen;
            uint32_t prefixIdx = prefixLen / pp_n_scalar;

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qk_m = (m_idx == (m_loop - 1)) ? (q_seqlen - m_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t qk_round_m = (qk_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            /**************** pre_load *****************/
            uint32_t qk_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t pingpong_flag = 0;
            uint32_t offset = pingpong_flag * L0AB_HALF_BUF_SIZE;

            uint64_t q_offset = addr_q_scalar + head_idx * embd + m_idx * pp_m_scalar * stride_qo;
            uint32_t kv_block_size = pp_n_scalar;
            uint64_t stride_table = cur_batch * max_num_blocks_per_query;
            // Only need load Q once
            if (qk_m == 1) {
                gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::ND>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    1,
                    0,
                    0,
                    RoundUp<uint32_t>(round_k, 32 / sizeof(QKV_DT)),              // lenBurst
                    0,
                    0
                );
            } else {
                gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    qk_m,       // nValue
                    qk_round_m, // dstNzC0Stride
                    0,
                    __k,        // dValue
                    0,
                    stride_qo  // srcDValue
                );
            }
            SET_FLAG(MTE2, MTE1, pingpong_flag);
            WAIT_FLAG(MTE2, MTE1, pingpong_flag);

            uint32_t sv_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
            uint64_t v_offset = addr_v_scalar + (head_idx / group_num) * embd;
            int vIdx = 0;
            uint32_t n_end = n_loop;
            // when prefix exists, qseqlen < 128 and kvseqlen > 128, n_end = n_loop
            if (is_triu_mask && pp_m_scalar >= pp_n_scalar) {
                n_end = m_idx + prefixIdx + 1;
            }

            uint32_t window_start = (window_size + pp_n_scalar - 1) / pp_n_scalar;
            uint32_t n_start = 0;
            uint32_t s_block_stack = n_end > 4 ? 2 : 1; // Currently not splitting K
            uint32_t launch_delay = s_block_stack * 2;
            uint32_t vect_mod = 2 * launch_delay;
            for (uint32_t n_idx = n_start; n_idx < n_end + launch_delay; n_idx += s_block_stack) {
                if (n_idx < n_end) {
                    for (uint32_t split_idx = 0; split_idx < s_block_stack && n_idx + split_idx < n_end; split_idx++) {
                        int32_t block_id0 = (int32_t)(*((__gm__ int32_t *)block_table_gm + stride_table + n_idx + split_idx));
                        uint64_t k_offset = (int64_t)block_id0 * kv_block_size * kv_heads * embd + (int64_t)(head_idx / group_num) * embd;
                        pingpong_flag = (n_idx + split_idx - n_start) % 2;
                        offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
                        if (n_idx + split_idx == (n_loop - 1)) {
                            qk_n = (kv_seqlen - (n_idx + split_idx) * pp_n_scalar);
                            qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        }
                        bool last_split = split_idx == s_block_stack - 1 || n_idx + split_idx == n_end - 1;
                        WAIT_FLAG(M, MTE1, pingpong_flag);
                        LoadDataToCa(l0a_buf_tensor[offset], l1q_buf_addr_tensor, round_k, qk_round_m, qk_m);
                        // *** Prepare K to L1
                        SET_FLAG(MTE1, M, pingpong_flag);
                        WAIT_FLAG(MTE1, MTE2, pingpong_flag);
                        gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                            l1k_buf_addr_tensor[offset],
                            k_gm_tensor[k_offset],
                            qk_n,        // nValue
                            qk_round_n,  // dstNzC0Stride
                            0,            // dstNzMatrixStride, unused
                            __k,         // dValue
                            0,            // dstNzMatrixStride, unused
                            stride_kv   // srcDValue
                        );

                        SET_FLAG(MTE2, MTE1, pingpong_flag);
                        WAIT_FLAG(M, MTE1, pingpong_flag + 2);
                        WAIT_FLAG(MTE2, MTE1, pingpong_flag);
                        l1_to_l0_b<ArchType::ASCEND_V220, QKV_DT, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0b_buf_tensor[offset],
                            l1k_buf_addr_tensor[offset],
                            0,
                            NumMatrixsRoundUp<QKV_DT>(RoundUp<uint32_t>(round_k, 32 / sizeof(QKV_DT)) * qk_round_n), // repeat
                            0,
                            1,                                        // srcStride
                            0,
                            0                                        // dstStride
                        );
                        SET_FLAG(MTE1, MTE2, pingpong_flag);
                        SET_FLAG(MTE1, M, pingpong_flag + 2);
                        WAIT_FLAG(MTE1, M, pingpong_flag);
                        WAIT_FLAG(MTE1, M, pingpong_flag + 2);
                        if (split_idx == 0) {
                            WAIT_FLAG(FIX, M, EVENT_ID0);
                            WAIT_FLAG(FIX, M, EVENT_ID1);
                        }
                        mmad<ArchType::ASCEND_V220, QKV_DT, QKV_DT, float, false>(
                            l0c_buf_tensor[split_idx * qk_round_m * pp_n_scalar],
                            l0a_buf_tensor[offset],
                            l0b_buf_tensor[offset],
                            qk_m,  // m
                            qk_n,  // n
                            __k,   // k
                            1      // cmatrixInitVal
                        );
                        SET_FLAG(M, MTE1, pingpong_flag);
                        SET_FLAG(M, MTE1, pingpong_flag + 2);
                    }
                    SET_FLAG(M, FIX, EVENT_ID0);
                    WAIT_FLAG(M, FIX, EVENT_ID0);
                    uint32_t sv_n_triu = n_end * pp_n_scalar;
                    if (n_idx + s_block_stack > n_end - 1) {
                        sv_n = sv_n_triu > kv_seqlen ? kv_seqlen - n_idx * pp_n_scalar : sv_n_triu - n_idx * pp_n_scalar;
                    } else {
                        sv_n = pp_n_scalar * s_block_stack;
                    }
                    uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                    // copy S to gm
                    l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx - n_start) % vect_mod * TMP_SIZE / vect_mod],
                        l0c_buf_tensor,
                        qk_m,        // MSize
                        sv_round_n,  // NSize
                        qk_round_m,  // srcStride
                        sv_round_n  // dstStride_dst_D
                    );
                    SET_FLAG(FIX, M, EVENT_ID0);
                    SET_FLAG(FIX, M, EVENT_ID1);
                    FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY);
                }
                if (n_idx >= launch_delay + n_start) {
                    int32_t block_id0 = (int32_t)(*((__gm__ int32_t *)block_table_gm + stride_table + vIdx));
                    int64_t v_offset = (int64_t)block_id0 * kv_block_size * kv_heads * embd + (int64_t)(head_idx / group_num) * embd;
                    uint32_t l0c_pingpong_flag = (n_idx - n_start) % 2;
                    uint32_t l0c_offset = l0c_pingpong_flag * L0AB_HALF_BUF_SIZE;
                    uint32_t sv_n_triu = n_end * pp_n_scalar;
                    if (n_idx + s_block_stack > n_end + launch_delay - 1) {
                        sv_n = sv_n_triu > kv_seqlen ? kv_seqlen - (n_idx - launch_delay) * pp_n_scalar : sv_n_triu - (n_idx - launch_delay) * pp_n_scalar;
                    } else {
                        sv_n = pp_n_scalar * s_block_stack;
                    }
                    uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                    WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
                    if (sv_round_n <= pp_n_scalar) {
                        gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                            l1v_buf_addr_tensor, v_gm_tensor[v_offset],
                            sv_n,       // nValue
                            sv_round_n, // dstNzC0Stride
                            0,          // dstNzMatrixStride, unused
                            __k,        // dValue
                            0,          // dstNzMatrixStride, unused
                            stride_kv   // srcDValue
                        );
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                            l1v_buf_addr_tensor, v_gm_tensor[v_offset],
                            pp_n_scalar,       // nValue
                            sv_round_n, // dstNzC0Stride
                            0,          // dstNzMatrixStride, unused
                            __k,        // dValue
                            0,          // dstNzMatrixStride, unused
                            stride_kv   // srcDValue
                        );
                        PIPE_BARRIER(MTE1);
                        int32_t block_id1 = (int32_t)(*((__gm__ int32_t *)block_table_gm + stride_table + vIdx + 1));
                        int64_t v_offset1 = (int64_t)block_id1 * kv_block_size * kv_heads * embd + (int64_t)(head_idx / group_num) * embd;
                        gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                            l1v_buf_addr_tensor[pp_n_scalar * BLOCK_SIZE], v_gm_tensor[v_offset1],
                            sv_n - pp_n_scalar,       // nValue
                            sv_round_n, // dstNzC0Stride
                            0,          // dstNzMatrixStride, unused
                            __k,        // dValue
                            0,          // dstNzMatrixStride, unused
                            stride_kv   // srcDValue
                        );
                    }
                    vIdx += (sv_n + pp_n_scalar - 1) / pp_n_scalar;
                    SET_FLAG(MTE2, MTE1, EVENT_ID2);
                    WAIT_FLAG(MTE2, MTE1, EVENT_ID2);
                    WAIT_FLAG(M, MTE1, EVENT_ID2);
                    WAIT_FLAG(M, MTE1, EVENT_ID3);
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < sv_round_n / BLOCK_SIZE; ++l0b_load_idx) {
                        l1_to_l0_b<ArchType::ASCEND_V220, QKV_DT, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0b_buf_tensor[l0b_load_idx * round_k * BLOCK_SIZE],
                            l1v_buf_addr_tensor[l0b_load_idx * CUBE_MATRIX_SIZE], 0,
                            round_k / BLOCK_SIZE, // repeat
                            0,
                            sv_round_n / BLOCK_SIZE, // srcStride
                            0,
                            0 // dstStride
                        );
                    }
                    SET_FLAG(MTE1, M, EVENT_ID6);
                    SET_FLAG(MTE1, MTE2, EVENT_ID2);
                    WaitFlagDev(SOFTMAX_READY);
                    WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
                    if (qk_m == 1) {
                        gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::ND>(
                            l1p_buf_addr_tensor,
                            p_gm_tensor[((uint64_t)block_idx * TMP_SIZE +
                                         (n_idx - launch_delay - n_start) % vect_mod * TMP_SIZE / vect_mod) *
                                        2 / sizeof(QKV_DT)],
                            1, 0, 0, RoundUp<uint64_t>(sv_round_n, BlockSize<QKV_DT>()), // lenBurst
                            0, 0);
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                            l1p_buf_addr_tensor,
                            p_gm_tensor[((uint64_t)block_idx * TMP_SIZE +
                                         (n_idx - launch_delay - n_start) % vect_mod * TMP_SIZE / vect_mod) *
                                        2 / sizeof(QKV_DT)],
                            qk_m,                           // nValue
                            qk_round_m,                     // dstNzC0Stride
                            0,                              // dstNzMatrixStride, unused
                            sv_n,                           // dValue
                            0,                              // dstNzMatrixStride, unused
                            sv_round_n * 2 / sizeof(QKV_DT) // srcDValue
                        );
                    }
                    SET_FLAG(MTE2, MTE1, EVENT_ID3);
                    WAIT_FLAG(MTE2, MTE1, EVENT_ID3);
                    WAIT_FLAG(M, MTE1, EVENT_ID0);
                    WAIT_FLAG(M, MTE1, EVENT_ID1);
                    LoadDataToCa(l0a_buf_tensor, l1p_buf_addr_tensor,
                                 RoundUp<uint64_t>(sv_round_n, BlockSize<QKV_DT>()), qk_round_m, qk_m);
                    SET_FLAG(MTE1, M, EVENT_ID5);
                    SET_FLAG(MTE1, MTE2, EVENT_ID3);
                    WAIT_FLAG(MTE1, M, EVENT_ID5);
                    WAIT_FLAG(MTE1, M, EVENT_ID6);
                    WAIT_FLAG(FIX, M, l0c_pingpong_flag);
                    mmad<ArchType::ASCEND_V220, QKV_DT, QKV_DT, float, false>(
                        l0c_buf_tensor[l0c_offset], l0a_buf_tensor,
                        l0b_buf_tensor,
                        qk_m, // m
                        __k,  // n
                        sv_n, // k
                        1     // cmatrixInitVal
                    );
                    SET_FLAG(M, MTE1, EVENT_ID0);
                    SET_FLAG(M, MTE1, EVENT_ID1);
                    SET_FLAG(M, MTE1, EVENT_ID2);
                    SET_FLAG(M, MTE1, EVENT_ID3);
                    SET_FLAG(M, FIX, l0c_pingpong_flag);
                    WAIT_FLAG(M, FIX, l0c_pingpong_flag);
                    // copy O to gm
                    l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                        o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx - launch_delay - n_start) % vect_mod * TMP_SIZE / vect_mod],
                        l0c_buf_tensor[l0c_offset],
                        qk_m,        // MSize
                        round_k,     // NSize
                        qk_round_m,  // srcStride
                        round_k     // dstStride_dst_D
                    );
                    SET_FLAG(FIX, M, l0c_pingpong_flag);
                    FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY);
                }
            }
        }
    }

private:
    __gm__ uint8_t *__restrict__ q_gm{nullptr};
    __gm__ uint8_t *__restrict__ k_gm{nullptr};
    __gm__ uint8_t *__restrict__ v_gm{nullptr};
    __gm__ uint8_t *__restrict__ s_gm{nullptr};
    __gm__ uint8_t *__restrict__ p_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_para_gm{nullptr};
    __gm__ uint8_t *__restrict__ block_table_gm{nullptr};

    const uint32_t l1q_buf_addr_offset = 0;
    const uint32_t l1k_buf_addr_offset = 2 * L0AB_UINT8_BLOCK_SIZE;
    const uint32_t l1p_buf_addr_offset = 4 * L0AB_UINT8_BLOCK_SIZE;
    const uint32_t l1v_buf_addr_offset = 6 * L0AB_UINT8_BLOCK_SIZE;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;

    AscendC::LocalTensor<QKV_DT> l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, QKV_DT>(l1q_buf_addr_offset);
    AscendC::LocalTensor<QKV_DT> l1k_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, QKV_DT>(l1k_buf_addr_offset);
    AscendC::LocalTensor<QKV_DT> l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, QKV_DT>(l1p_buf_addr_offset);
    AscendC::LocalTensor<QKV_DT> l1v_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, QKV_DT>(l1v_buf_addr_offset);

    AscendC::GlobalTensor<QKV_DT> q_gm_tensor;
    AscendC::GlobalTensor<QKV_DT> k_gm_tensor;
    AscendC::GlobalTensor<QKV_DT> v_gm_tensor;
    AscendC::GlobalTensor<float> s_gm_tensor;
    AscendC::GlobalTensor<QKV_DT> p_gm_tensor;
    AscendC::GlobalTensor<float> o_tmp_gm_tensor;

    AscendC::LocalTensor<QKV_DT> l0a_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0A, QKV_DT>(0);
    AscendC::LocalTensor<QKV_DT> l0b_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0B, QKV_DT>(0);
    AscendC::LocalTensor<float> l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, float>(0);

    uint32_t batch_size{0};
    uint32_t max_seqlen{0};
    uint32_t max_kv_seqlen{0};
    uint32_t q_heads{0};
    uint32_t embd{0};
    uint32_t kv_heads{0};
    uint32_t is_triu_mask{0};
    uint32_t total_q_blk_num{0};
    uint32_t group_num{0};
    uint64_t stride_qo{0};
    uint64_t stride_kv{0};
    uint64_t batch_stride_kv{0};
    uint32_t __k{0};
    uint32_t round_k{0};
	uint32_t window_size{0};
    uint32_t tilingKey{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t max_num_blocks_per_query{0};
};

template<typename IN_DATA_TYPE, typename QKV_DT = IN_DATA_TYPE, typename O_DT = IN_DATA_TYPE, bool int8_flag = false, bool splitm = false>
class FlashAttentionEncoderHighPrecisionCubeOpt {
public:
    __aicore__ __attribute__((always_inline)) inline FlashAttentionEncoderHighPrecisionCubeOpt(
        __gm__ uint8_t *__restrict__ sync, __gm__ uint8_t *__restrict__ q_gm,
        __gm__ uint8_t *__restrict__ k_gm, __gm__ uint8_t *__restrict__ v_gm,
        __gm__ uint8_t *__restrict__ block_table_gm, __gm__ uint8_t *__restrict__ s_gm,
        __gm__ uint8_t *__restrict__ p_gm, __gm__ uint8_t *__restrict__ o_tmp_gm,
        __gm__ uint8_t *__restrict__ tiling_para_gm) :
        q_gm(q_gm), s_gm(s_gm), p_gm(p_gm), o_tmp_gm(o_tmp_gm),
        tiling_para_gm(tiling_para_gm)
    {
        SetFftsBaseAddr((unsigned long)sync);
        SetPadding<uint64_t>(0);
        SetAtomicnone();
        SetNdpara(1, 0, 0);
        SetMasknorm();

        this->batch_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm));
        this->max_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1));
        this->q_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2));
        this->embd = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3));
        this->kv_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 4));
        this->is_triu_mask = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 8));
        this->total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9));
        this->tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 14));
        this->tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 15));
        this->tilingKey = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 16));
        this->max_kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 18));
        this->window_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 27));
        this->max_num_blocks_per_query = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 28));
        this->group_num = q_heads / kv_heads;
        this->stride_qo = q_heads * embd;
        this->stride_kv = kv_heads * embd;

        if (data_shape_type == 1) {
            this->stride_qo = embd;
            this->stride_kv = embd;
        }
        if constexpr (splitm) {
            this->tmp_times = 2;
        }
        this->batch_stride_kv = batch_size * max_kv_seqlen * kv_heads * embd * sizeof(QKV_DT);
        this->__k = embd;
        this->round_k = (__k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
        
        this->k_gm = k_gm;
        this->v_gm = v_gm;
        this->block_table_gm = block_table_gm;
        
        q_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(this->q_gm));
        k_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(this->k_gm));
        v_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(this->v_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->s_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(this->p_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->o_tmp_gm));
        
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(MTE1, MTE2, EVENT_ID4);
        SET_FLAG(MTE1, MTE2, EVENT_ID5);
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(M, MTE1, EVENT_ID2);
        SET_FLAG(M, MTE1, EVENT_ID3);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);
    }

    __aicore__ __attribute__((always_inline)) inline ~FlashAttentionEncoderHighPrecisionCubeOpt()
    {
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID5);
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(M, MTE1, EVENT_ID2);
        WAIT_FLAG(M, MTE1, EVENT_ID3);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        PIPE_BARRIER(ALL);
    }

    __aicore__ __attribute__((always_inline)) inline uint32_t GetTilingKey()
    {
        return this->tilingKey;
    }

    __aicore__ __attribute__((always_inline)) inline void LoadDataToCa(
        AscendC::LocalTensor<QKV_DT> dst_tensor, AscendC::LocalTensor<QKV_DT> src_tensor,
        uint32_t round_k, uint32_t qk_round_m, uint32_t qk_m)
    {
        uint32_t round_row = RoundUp<uint32_t>(round_k, 32 / sizeof(QKV_DT));
        if (qk_m == 1) {
            l1_to_l0_a<ArchType::ASCEND_V220, QKV_DT, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                dst_tensor,
                src_tensor,
                0,
                NumMatrixsRoundUp<QKV_DT>(round_row), // repeat
                0,
                1,                                                   // srcStride
                0,
                0                                                   // dstStride
            );
        } else {
            l1_to_l0_a<ArchType::ASCEND_V220, QKV_DT, false, DataFormat::NZ, DataFormat::ZZ>(
                dst_tensor, src_tensor, qk_round_m, round_row, 0, 0, 0, 0);
        }
    }
    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        uint64_t cur_batch = 0;
        uint64_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 13 + offset_tiling));
        uint64_t process_num = (uint64_t)total_q_blk_num * q_heads;
        uint64_t next_process = 0;
        for (uint64_t process = block_idx; process < process_num; process = next_process) {
            while (process >= (uint64_t)cur_total_q_blk_num * q_heads) {
                cur_batch++;
                pre_total_q_blk_num = cur_total_q_blk_num;
                offset_tiling += tiling_para_size;
                cur_total_q_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 13 + offset_tiling));
            }
            next_process = process + block_num;
            if (is_triu_mask) {
                uint64_t curr_iter = process / block_num;
                next_process = curr_iter % 2 == 1 ? (curr_iter + 1) * block_num + block_idx : (curr_iter + 2) * block_num - 1 - block_idx;
            }
            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            if (q_seqlen == 0 || kv_seqlen == 0) {
                continue;
            }
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_q_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 4 + offset_tiling));
            uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 5 + offset_tiling));
            uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);
            uint32_t addr_k_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 6 + offset_tiling));
            uint32_t addr_k_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 7 + offset_tiling));
            uint64_t addr_k_scalar = (uint64_t)(((uint64_t)addr_k_high32) << 32 | addr_k_loww32);
            uint32_t addr_v_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 8 + offset_tiling));
            uint32_t addr_v_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
            uint64_t addr_v_scalar = (uint64_t)(((uint64_t)addr_v_high32) << 32 | addr_v_loww32);
            uint64_t process_idx = process - pre_total_q_blk_num * q_heads;
            uint32_t m_idx = process_idx / q_heads;
            uint64_t head_idx = process_idx % q_heads;
            uint32_t prefixLen = kv_seqlen - q_seqlen;
            uint32_t prefixIdx = prefixLen / pp_n_scalar;

            // q_seqlen 向pp_m_scalar上取整
            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qk_m = (m_idx == (m_loop - 1)) ? (q_seqlen - m_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t qk_round_m = (qk_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            /**************** pre_load *****************/
            uint32_t qk_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t pingpong_flag = 0;
            uint32_t offset = pingpong_flag * L0AB_HALF_BUF_SIZE;

            uint64_t q_offset = addr_q_scalar + head_idx * embd + m_idx * pp_m_scalar * stride_qo;// + 17592186044416 / sizeof(QKV_DT);
            uint32_t kv_block_size = pp_n_scalar;
            uint64_t stride_table = cur_batch * max_num_blocks_per_query;

            // Only need load Q once
            // m方向最后一次，且一次可以搬运完
            if (qk_m == 1) {
                gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::ND>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    1,
                    0,
                    0,
                    RoundUp<uint32_t>(round_k, 32 / sizeof(QKV_DT)),              // lenBurst
                    0,
                    0
                );
            } else {
                gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    qk_m,       // nValue
                    qk_round_m, // dstNzC0Stride
                    0,
                    __k,        // dValue
                    0,
                    stride_qo  // srcDValue
                );
            }
            SET_FLAG(MTE2, MTE1, pingpong_flag);
            WAIT_FLAG(MTE2, MTE1, pingpong_flag);
            
            uint32_t sv_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
            uint64_t v_offset = addr_v_scalar + (head_idx / group_num) * embd;//  + 17592186044416 / sizeof(QKV_DT);
            int vIdx = 0;

            uint32_t n_end = n_loop;
            if (is_triu_mask) {
                n_end = m_idx + prefixIdx + 1;
            }
            uint32_t s_block_stack = n_end > 8 ? 4 : (n_end > 4 ? 2 : 1);
            uint32_t launch_delay = s_block_stack * 2;
            uint32_t vect_mod = 2 * launch_delay;
            uint32_t kv_pingpong_flag = 0;
            uint64_t kv_pingpong_offset = kv_pingpong_flag * KV_DB_SIZE;
            
            if constexpr (splitm) {
                // 预留 splitm 
            } else {
                for (uint32_t n_idx = 0; n_idx < n_end + launch_delay; n_idx += s_block_stack) {
                    if (n_idx < n_end) {
                        uint32_t sv_n_triu = n_end * pp_n_scalar;
                        if (n_idx + s_block_stack > n_end - 1) {
                            sv_n = sv_n_triu > kv_seqlen ? kv_seqlen - n_idx * pp_n_scalar : sv_n_triu - n_idx * pp_n_scalar;
                        } else {
                            sv_n = pp_n_scalar * s_block_stack;
                        }
                        uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        for (uint32_t split_idx = 0; split_idx < s_block_stack && n_idx + split_idx < n_end; split_idx++) {
                            pingpong_flag = (n_idx + split_idx) % 2;
                            offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
                            int32_t block_id0 = (int32_t)(*((__gm__ int32_t *)block_table_gm + stride_table + n_idx + split_idx));
                            uint64_t k_offset = (int64_t)block_id0 * kv_block_size * kv_heads * embd + (int64_t)(head_idx / group_num) * embd;
                            if (n_idx + split_idx == (n_loop - 1)) {
                                qk_n = (kv_seqlen - (n_idx + split_idx) * pp_n_scalar);
                                qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                            }
                            bool last_split = split_idx == s_block_stack - 1 || n_idx + split_idx == n_end - 1;
                            WAIT_FLAG(MTE1, MTE2, pingpong_flag + 2 * kv_pingpong_flag);
                            gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                                l1k_buf_addr_tensor[kv_pingpong_offset + offset],
                                k_gm_tensor[k_offset],
                                qk_n,        // nValue
                                qk_round_n,  // dstNzC0Stride
                                0,            // dstNzMatrixStride, unused
                                __k,         // dValue
                                0,            // dstNzMatrixStride, unused
                                stride_kv   // srcDValue
                            );

                            SET_FLAG(MTE2, MTE1, pingpong_flag);
                            WAIT_FLAG(M, MTE1, pingpong_flag);
                            LoadDataToCa(l0a_buf_tensor[offset], l1q_buf_addr_tensor, round_k, qk_round_m, qk_m);
                            // *** Prepare K to L1
                            SET_FLAG(MTE1, M, pingpong_flag);
                            WAIT_FLAG(M, MTE1, pingpong_flag + 2);
                            WAIT_FLAG(MTE2, MTE1, pingpong_flag);
                            l1_to_l0_b<ArchType::ASCEND_V220, QKV_DT, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                l0b_buf_tensor[offset],
                                l1k_buf_addr_tensor[kv_pingpong_offset + offset],
                                0,
                                NumMatrixsRoundUp<QKV_DT>(RoundUp<uint32_t>(round_k, 32 / sizeof(QKV_DT)) * qk_round_n), // repeat
                                0,
                                1,                                        // srcStride
                                0,
                                0                                        // dstStride
                            );
                            SET_FLAG(MTE1, MTE2, pingpong_flag + 2 * kv_pingpong_flag);
                            SET_FLAG(MTE1, M, pingpong_flag + 2);
                            WAIT_FLAG(MTE1, M, pingpong_flag);
                            WAIT_FLAG(MTE1, M, pingpong_flag + 2);
                            WAIT_FLAG(FIX, M, pingpong_flag);
                            if constexpr (int8_flag) {
                                mmad<ArchType::ASCEND_V220, QKV_DT, QKV_DT, int32_t, false>(
                                    l0c_buf_tensor.ReinterpretCast<int32_t>()[pingpong_flag * L0AB_HALF_BUF_SIZE],
                                    l0a_buf_tensor[offset],
                                    l0b_buf_tensor[offset],
                                    qk_m,  // m
                                    qk_n,  // n
                                    __k,   // k
                                    1      // cmatrixInitVal
                                );
                            } else {
                                mmad<ArchType::ASCEND_V220, QKV_DT, QKV_DT, float, false>(
                                    l0c_buf_tensor[pingpong_flag * L0AB_HALF_BUF_SIZE],
                                    l0a_buf_tensor[offset],
                                    l0b_buf_tensor[offset],
                                    qk_m,  // m
                                    qk_n,  // n
                                    __k,   // k
                                    1      // cmatrixInitVal
                                );
                            }
                            SET_FLAG(M, MTE1, pingpong_flag);
                            SET_FLAG(M, MTE1, pingpong_flag + 2);
                            
                            SET_FLAG(M, FIX, pingpong_flag);
                            WAIT_FLAG(M, FIX, pingpong_flag);
                            // copy S to gm
                            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE * tmp_times + n_idx % vect_mod * TMP_SIZE * tmp_times / vect_mod + split_idx * pp_n_scalar],
                                l0c_buf_tensor[pingpong_flag * L0AB_HALF_BUF_SIZE],
                                qk_m,        // MSize
                                qk_round_n,  // NSize
                                qk_round_m,  // srcStride
                                sv_round_n  // dstStride_dst_D
                            );
                            SET_FLAG(FIX, M, pingpong_flag);  
                        }
                        FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY);
                        kv_pingpong_flag = 1 - kv_pingpong_flag;
                        kv_pingpong_offset = kv_pingpong_flag * KV_DB_SIZE;
                    }
                    if (n_idx >= launch_delay) {
                        uint32_t l0c_pingpong_flag = (n_idx + 1) % 2;
                        uint32_t l0c_offset = l0c_pingpong_flag * L0AB_HALF_BUF_SIZE;
                        uint32_t sv_n_triu = n_end * pp_n_scalar;
                        int32_t block_id0 = (int32_t)(*((__gm__ int32_t *)block_table_gm + stride_table + vIdx));
                        int64_t v_offset = (int64_t)block_id0 * kv_block_size * kv_heads * embd + (int64_t)(head_idx / group_num) * embd;
                        if (n_idx + s_block_stack > n_end + launch_delay - 1) {
                            sv_n = sv_n_triu > kv_seqlen ? kv_seqlen - (n_idx - launch_delay) * pp_n_scalar : sv_n_triu - (n_idx - launch_delay) * pp_n_scalar;
                        } else {
                            sv_n = pp_n_scalar * s_block_stack;
                        }
                        uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        uint32_t n_slice = pp_n_scalar * ((s_block_stack + 1) / 2);
                        // n_slice = 256;
                        uint32_t l1_split_loop = (sv_n + n_slice - 1) / n_slice;
                        WAIT_FLAG(MTE1, MTE2, kv_pingpong_flag * 2);
                        WAIT_FLAG(MTE1, MTE2, kv_pingpong_flag * 2 + 1);
                        uint32_t v_split_cnt = sv_n / pp_n_scalar;
                        for (uint32_t v_split_idx = 0; v_split_idx < v_split_cnt; v_split_idx++) {
                            gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                                l1v_buf_addr_tensor[kv_pingpong_offset + pp_n_scalar * BLOCK_SIZE * v_split_idx],
                                v_gm_tensor[v_offset],
                                pp_n_scalar,       // nValue
                                sv_round_n, // dstNzC0Stride
                                0,          // dstNzMatrixStride, unused
                                __k,        // dValue
                                0,          // dstNzMatrixStride, unused
                                stride_kv   // srcDValue
                            );
                            int32_t block_id1 = (int32_t)(*((__gm__ int32_t *)block_table_gm + stride_table + vIdx + v_split_idx + 1));
                            int64_t v_offset1 = (int64_t)block_id1 * kv_block_size * kv_heads * embd + (int64_t)(head_idx / group_num) * embd;
                            v_offset = v_offset1;
                        }
                        if (sv_n % pp_n_scalar != 0) {
                            gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                                l1v_buf_addr_tensor[kv_pingpong_offset + pp_n_scalar * BLOCK_SIZE * v_split_cnt],
                                v_gm_tensor[v_offset],
                                sv_n % pp_n_scalar,       // nValue
                                sv_round_n, // dstNzC0Stride
                                0,          // dstNzMatrixStride, unused
                                __k,        // dValue
                                0,          // dstNzMatrixStride, unused
                                stride_kv   // srcDValue
                            );
                        }
                        vIdx += (sv_n + pp_n_scalar - 1) / pp_n_scalar;
                        
                        WaitFlagDev(SOFTMAX_READY);
                        WAIT_FLAG(FIX, M, l0c_pingpong_flag);
                        for (uint32_t l1_k_split_idx = 0; l1_k_split_idx < l1_split_loop; l1_k_split_idx++) {
                            uint32_t l1_pingpong_flag = l1_k_split_idx % 2;
                            uint32_t l1_offset = l1_pingpong_flag * 128 * 256;
                            bool l1_last_split = l1_k_split_idx == l1_split_loop - 1;
                            uint32_t d = l1_last_split ? sv_n - l1_k_split_idx * n_slice : n_slice;
                            WAIT_FLAG(MTE1, MTE2, l1_pingpong_flag + 4);
                            if (qk_m == 1) {
                                gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::ND>(
                                    l1p_buf_addr_tensor[l1_offset],
                                    p_gm_tensor[((uint64_t)block_idx * TMP_SIZE * tmp_times +
                                                (n_idx - launch_delay) % vect_mod * TMP_SIZE * tmp_times / vect_mod) *
                                                2 / sizeof(QKV_DT) + l1_k_split_idx * n_slice],
                                    1, 0, 0, RoundUp<uint64_t>(sv_round_n, BlockSize<QKV_DT>()), // lenBurst
                                    0, 0);
                            } else {
                                gm_to_l1<ArchType::ASCEND_V220, QKV_DT, DataFormat::ND, DataFormat::NZ>(
                                    l1p_buf_addr_tensor[l1_offset],
                                    p_gm_tensor[((uint64_t)block_idx * TMP_SIZE * tmp_times +
                                                (n_idx - launch_delay) % vect_mod * TMP_SIZE * tmp_times / vect_mod) *
                                                2 / sizeof(QKV_DT) + l1_k_split_idx * n_slice],
                                    qk_m,                           // nValue
                                    qk_round_m,                     // dstNzC0Stride
                                    0,                              // dstNzMatrixStride, unused
                                    d,                           // dValue
                                    0,                              // dstNzMatrixStride, unused
                                    sv_round_n * 2 / sizeof(QKV_DT) // srcDValue
                                );
                            }
                            SET_FLAG(MTE2, MTE1, l1_pingpong_flag + 4);
                            WAIT_FLAG(MTE2, MTE1, l1_pingpong_flag + 4);
                            uint32_t d_split_loop = (d + 127) / 128;
                            for (uint32_t l0_k_split_idx = 0; l0_k_split_idx < d_split_loop; l0_k_split_idx++) {
                                uint32_t l0_pingpong_flag = l0_k_split_idx % 2;
                                uint32_t l0_offset = l0_pingpong_flag * 128 * 128;
                                bool l0_last_split = l0_k_split_idx == d_split_loop - 1;
                                int32_t l0_p_offset = qk_m == 1 ? l0_k_split_idx * 128 : l0_k_split_idx * 128 * qk_round_m;
                                bool initC = l0_k_split_idx== 0 && l1_k_split_idx == 0;
                                uint32_t reduce_d = l0_last_split ? d - l0_k_split_idx * 128 : 128;
                                uint32_t round_reduce_d = (reduce_d + 15) / 16 * 16;
                                WAIT_FLAG(M, MTE1, l0_pingpong_flag);
                                LoadDataToCa(l0a_buf_tensor[l0_offset], l1p_buf_addr_tensor[l1_offset + l0_p_offset],
                                            RoundUp<uint64_t>(round_reduce_d, BlockSize<QKV_DT>()), qk_round_m, qk_m);
                                if (l0_last_split){
                                    SET_FLAG(MTE1, MTE2, l1_pingpong_flag + 4);
                                }
                                WAIT_FLAG(M, MTE1, l0_pingpong_flag + 2);
                                for (uint32_t l0b_load_idx = 0; l0b_load_idx < 128 / BLOCK_SIZE; ++l0b_load_idx) {
                                    l1_to_l0_b<ArchType::ASCEND_V220, QKV_DT, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                                        l0b_buf_tensor[l0_offset + l0b_load_idx * round_k * BLOCK_SIZE],
                                        l1v_buf_addr_tensor[kv_pingpong_offset + l0b_load_idx * CUBE_MATRIX_SIZE + 
                                                            l1_k_split_idx * n_slice * BLOCK_SIZE +
                                                            l0_k_split_idx * 128 * BLOCK_SIZE],
                                        0,
                                        round_k / BLOCK_SIZE, // repeat
                                        0,
                                        sv_round_n / BLOCK_SIZE, // srcStride
                                        0,
                                        0 // dstStride
                                    );
                                }
                                if (l0_last_split && l1_last_split){
                                    SET_FLAG(MTE1, MTE2, kv_pingpong_flag * 2);
                                    SET_FLAG(MTE1, MTE2, kv_pingpong_flag * 2 + 1);
                                }
                                SET_FLAG(MTE1, M, l0_pingpong_flag + 6);
                                WAIT_FLAG(MTE1, M, l0_pingpong_flag + 6);
                                if constexpr (int8_flag) {
                                    mmad<ArchType::ASCEND_V220, QKV_DT, QKV_DT, int32_t, false>(
                                        l0c_buf_tensor.template ReinterpretCast<int32_t>()[l0c_offset],
                                        l0a_buf_tensor[l0_offset],
                                        l0b_buf_tensor[l0_offset],
                                        qk_m, // m
                                        __k,  // n
                                        sv_n, // k
                                        1     // cmatrixInitVal
                                    );
                                } else { 
                                    mmad<ArchType::ASCEND_V220, QKV_DT, QKV_DT, float, false>(
                                        l0c_buf_tensor[l0c_offset],
                                        l0a_buf_tensor[l0_offset],
                                        l0b_buf_tensor[l0_offset],
                                        qk_m, // m
                                        __k,  // n
                                        reduce_d, // k
                                        initC     // cmatrixInitVal
                                    );
                                }
                                PIPE_BARRIER(M);
                                SET_FLAG(M, MTE1, l0_pingpong_flag);
                                SET_FLAG(M, MTE1, l0_pingpong_flag + 2);
                            }
                        }
                        SET_FLAG(M, FIX, l0c_pingpong_flag);
                        WAIT_FLAG(M, FIX, l0c_pingpong_flag);
                        // copy O to gm
                        l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                            o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE * tmp_times + (n_idx - launch_delay) % vect_mod * TMP_SIZE * tmp_times / vect_mod],
                            l0c_buf_tensor[l0c_offset],
                            qk_m,        // MSize
                            round_k,     // NSize
                            qk_round_m,  // srcStride
                            round_k     // dstStride_dst_D
                        );
                        SET_FLAG(FIX, M, l0c_pingpong_flag);  
                        FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY);
                        kv_pingpong_flag = 1 - kv_pingpong_flag;
                        kv_pingpong_offset = kv_pingpong_flag * KV_DB_SIZE;
                    }
                }
            }
        }
    }

private:
    __gm__ uint8_t *__restrict__ q_gm{nullptr};
    __gm__ uint8_t *__restrict__ k_gm{nullptr};
    __gm__ uint8_t *__restrict__ v_gm{nullptr};
    __gm__ uint8_t *__restrict__ s_gm{nullptr};
    __gm__ uint8_t *__restrict__ p_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_para_gm{nullptr};
    __gm__ uint8_t *__restrict__ block_table_gm{nullptr};

    const uint32_t l1q_buf_addr_offset = 0;
    const uint32_t l1k_buf_addr_offset = 4 * L0AB_UINT8_BLOCK_SIZE;
    const uint32_t l1v_buf_addr_offset = 4 * L0AB_UINT8_BLOCK_SIZE;
    const uint32_t l1p_buf_addr_offset = 12 * L0AB_UINT8_BLOCK_SIZE;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;

    AscendC::LocalTensor<QKV_DT> l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, QKV_DT>(l1q_buf_addr_offset);
    AscendC::LocalTensor<QKV_DT> l1k_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, QKV_DT>(l1k_buf_addr_offset);
    AscendC::LocalTensor<QKV_DT> l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, QKV_DT>(l1p_buf_addr_offset);
    AscendC::LocalTensor<QKV_DT> l1v_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, QKV_DT>(l1v_buf_addr_offset);

    AscendC::GlobalTensor<QKV_DT> q_gm_tensor;
    AscendC::GlobalTensor<QKV_DT> k_gm_tensor;
    AscendC::GlobalTensor<QKV_DT> v_gm_tensor;
    AscendC::GlobalTensor<float> s_gm_tensor;
    AscendC::GlobalTensor<QKV_DT> p_gm_tensor;
    AscendC::GlobalTensor<float> o_tmp_gm_tensor;

    AscendC::LocalTensor<QKV_DT> l0a_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0A, QKV_DT>(0);
    AscendC::LocalTensor<QKV_DT> l0b_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0B, QKV_DT>(0);
    AscendC::LocalTensor<float> l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, float>(0);

    uint32_t batch_size{0};
    uint32_t max_seqlen{0};
    uint32_t max_kv_seqlen{0};
    uint32_t max_q_seqlen{0};
    uint32_t q_heads{0};
    uint32_t embd{0};
    uint32_t kv_heads{0};
    uint32_t is_triu_mask{0};
    uint32_t total_q_blk_num{0};
    uint32_t group_num{0};
    uint64_t stride_qo{0};
    uint64_t stride_kv{0};
    uint64_t batch_stride_kv{0};
    uint32_t __k{0};
    uint32_t round_k{0};
    uint32_t window_size{0};
    uint32_t data_shape_type{0};
    uint32_t tmp_times{1};
    uint32_t tilingKey{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t max_num_blocks_per_query{0};
};

#elif __DAV_C220_VEC__


constexpr int32_t UB_UINT8_BLOCK_SIZE = 16384;  // 64 * 128 * 2B
constexpr int32_t UB_HALF_BUF_SIZE = 8192;
constexpr int32_t UB_bf16_BUF_SIZE = 8192;      // 64 * 128
constexpr int32_t UB_UINT8_LINE_SIZE = 512;     // 128 * 4B
constexpr int32_t UB_FLOAT_LINE_SIZE = 64;     // 128
constexpr int32_t UB_HALF_LINE_SIZE = 128;       // UB_FLOAT_LINE_SIZE * 2
constexpr int32_t BASE_MASK_SIZE = 128;
constexpr int32_t COMPRESS_MASK_SIZE = 8192; // 64 * 128
constexpr float BASE_Y = 128;

__aicore__ __attribute__((always_inline)) inline void __set_mask(int32_t len)
{
    uint64_t mask = 0;
    uint64_t one = 1;
    uint64_t temp = len % FLOAT_VECTOR_SIZE;
    for (int64_t i = 0; i < temp; i++) {
        mask |= one << i;
    }

    if (len == VECTOR_SIZE || len == 0) {
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
    } else if (len >= FLOAT_VECTOR_SIZE) {
        SetVectorMask<int8_t>(mask, (uint64_t)-1);
    } else {
        SetVectorMask<int8_t>(0x0, mask);
    }
}

__aicore__ __attribute__((always_inline)) inline void __set_vcg_mask(int32_t len)
{
    if (len > 16 || len < 1) {
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        return;
    }
    uint64_t subMask = ((uint64_t) 1 << len) - 1;
    uint64_t maskValue = (subMask << 48) + (subMask << 32) + (subMask << 16) + subMask +
                            (subMask << 56) + (subMask << 40) + (subMask << 24) + (subMask << 8);
    SetVectorMask<int8_t>(maskValue, maskValue);
}

template<typename IN_DATA_TYPE, typename QKV_DT = IN_DATA_TYPE, typename O_DT = IN_DATA_TYPE, MaskType maskType = MaskType::MASK_TYPE_ALIBI_COMPRESS>
class FlashAttentionEncoderHighPrecisionVec {
public:
    __aicore__ __attribute__((always_inline)) inline FlashAttentionEncoderHighPrecisionVec(
        __gm__ uint8_t *__restrict__ sync, __gm__ uint8_t *__restrict__ mask_gm,
        __gm__ uint8_t *__restrict__ alibi_coeff_gm, __gm__ uint8_t *__restrict__ o_gm,
        __gm__ uint8_t *__restrict__ s_gm, __gm__ uint8_t *__restrict__ p_gm, __gm__ uint8_t *__restrict__ o_tmp_gm,
        __gm__ uint8_t *__restrict__ tiling_para_gm)
        : mask_gm(mask_gm), o_gm(o_gm), alibi_coeff_gm(alibi_coeff_gm), s_gm(s_gm), p_gm(p_gm), o_tmp_gm(o_tmp_gm),
          tiling_para_gm(tiling_para_gm)
    {
        SetFftsBaseAddr((unsigned long)sync);
        SetAtomicnone();
        SetMasknorm();
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);

        this->sub_block_idx = GetSubBlockidx();
        this->batch_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm));
        this->max_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1));
        this->q_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2));
        this->embd = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3));
        this->tor = (float)(*((__gm__ float *)tiling_para_gm + 5));
        this->head_stride = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 6));
        this->mask_stride = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 7));
        this->is_triu_mask = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 8));
        this->total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9));
        this->tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 14));
        this->tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 15));
        this->tilingKey = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 16));
        this->long_seq = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 17));
        this->alibi_compress_offset = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 21));
        this->alibi_left_align = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 22));
        this->quantType = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 24));
        this->window_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 27));

        this->stride_qo = q_heads * embd;


        this->__k = embd;
        this->round_k = (__k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
        this->scaleType = (ScaleType)(*((__gm__ int32_t *)tiling_para_gm + 26));
        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID1);
        SET_FLAG(MTE3, MTE2, EVENT_ID2);
        SET_FLAG(V, MTE2, EVENT_ID0);
        SET_FLAG(V, MTE2, EVENT_ID1);
        SET_FLAG(V, MTE2, EVENT_ID2);
        SET_FLAG(MTE3, V, EVENT_ID0);
        SET_FLAG(V, MTE2, EVENT_ID7);
    }
    __aicore__ __attribute__((always_inline)) inline ~FlashAttentionEncoderHighPrecisionVec()
    {
        WAIT_FLAG(V, MTE2, EVENT_ID7);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        WAIT_FLAG(V, MTE2, EVENT_ID1);
        WAIT_FLAG(V, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE3, V, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }

    __aicore__ __attribute__((always_inline)) inline uint32_t GetTilingKey()
    {
        return this->tilingKey;
    }

    template<typename Dtype>
    __aicore__ __attribute__((always_inline)) inline uint32_t VectorSize()
    {
        return 256 / sizeof(Dtype);
    }

    template<typename Dtype>
    __aicore__ __attribute__((always_inline)) inline uint64_t NumVectorsRoundUp(uint64_t num)
    {
        return (num + VectorSize<Dtype>() - 1) / VectorSize<Dtype>();
    }
    template<bool IS_BF16 = true>
    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        uint64_t cur_batch = 0;
        uint32_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 13 + offset_tiling));
        uint32_t process_num = total_q_blk_num * q_heads;
        float alibi_coeff = 1;

        mask_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(mask_gm));
        o_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ O_DT *>(o_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(s_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(p_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(o_tmp_gm));
        uint32_t next_process = 0;
        for (uint32_t process = block_idx; process < process_num; process = next_process) {
            while (process >= cur_total_q_blk_num * q_heads) {
                cur_batch++;
                pre_total_q_blk_num = cur_total_q_blk_num;
                offset_tiling += tiling_para_size;
                cur_total_q_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 13 + offset_tiling));
            }
            next_process = process + block_num;
            if (is_triu_mask) {
                uint32_t curr_iter = process / block_num;
                next_process = curr_iter % 2 == 1 ? (curr_iter + 1) * block_num + block_idx : (curr_iter + 2) * block_num - 1 - block_idx;
            }

            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));

            if (q_seqlen == 0 || kv_seqlen == 0) {
                continue;
            }
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_o_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 10 + offset_tiling));
            uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 11 + offset_tiling));
            uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
            uint32_t prefixLen = kv_seqlen - q_seqlen;
            uint32_t prefixIdx = prefixLen / pp_n_scalar;

            uint32_t process_idx = process - pre_total_q_blk_num * q_heads;
            uint32_t m_idx = process_idx / q_heads;
            uint64_t head_idx = process_idx % q_heads;
            if (alibi_coeff_gm !=nullptr) {
                alibi_coeff = (float)(*((__gm__ float *)alibi_coeff_gm + head_idx));
            }

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qk_m = (m_idx == (m_loop - 1)) ? (q_seqlen - m_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t sub_m = (sub_block_idx == 1) ? (qk_m - qk_m / 2) : qk_m / 2;
            uint32_t sub_m_d128 = (sub_m + VECTOR_SIZE - 1) / VECTOR_SIZE;            // up aligned to 128
            uint32_t sub_m_d64 = (sub_m + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE; // up aligned to 64
            uint32_t round_sub_m = (sub_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t qk_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t delta_uint = 0;
            float base_y = -128;
            float delta = 0;
            uint64_t mask_batch_offset = cur_batch * mask_stride * max_seqlen;
            uint64_t mask_head_offset = head_idx * head_stride * max_seqlen;
            uint64_t mask_offset = mask_batch_offset + mask_head_offset;
            uint64_t o_offset = addr_o_scalar + head_idx * embd + m_idx * pp_m_scalar * stride_qo;
            if (long_seq == 0) {
                mask_offset += (m_idx + prefixIdx) * pp_m_scalar * max_seqlen;
            } else {
                gm_to_ub<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                    mask16_ubuf_tensor,
                    mask_gm_tensor[(uint64_t)sub_block_idx * qk_m / 2 * VECTOR_SIZE],
                    0,                                // sid
                    sub_m,                                // nBurst
                    VECTOR_SIZE / BLOCK_SIZE,  // lenBurst
                    0,                                // srcGap
                    0                                 // dstGap
                );
                SET_FLAG(MTE2, V, EVENT_ID0);
                WAIT_FLAG(MTE2, V, EVENT_ID0);
                conv_v<ArchType::ASCEND_V220, IN_DATA_TYPE, float>(
                    mask_ubuf_tensor,
                    mask16_ubuf_tensor,
                    sub_m * VECTOR_SIZE / FLOAT_VECTOR_SIZE, // repeat
                    1, // dstBlockStride
                    1, // srcBlockStride
                    8, // dstRepeatStride
                    4  // srcRepeatStride
                );
                PIPE_BARRIER(V);
                if (IS_BF16) { 
                    muls_v<ArchType::ASCEND_V220, float>(
                        mask_ubuf_tensor,
                        mask_ubuf_tensor, (float)-3e38,
                        sub_m * VECTOR_SIZE / FLOAT_VECTOR_SIZE, // repeat
                        1,                                                                // dstBlockStride
                        1,                                                                // srcBlockStride
                        8,                                                                // dstRepeatStride
                        8                                                                 // srcRepeatStride
                    );
                    PIPE_BARRIER(V);
                }
            }
            uint32_t n_end = n_loop;
            if (is_triu_mask && pp_m_scalar >= pp_n_scalar) {
                n_end = m_idx + prefixIdx + 1;
            }
            uint32_t window_start = 0;
            uint32_t n_start = 0;

            uint32_t qk_n_triu = n_end * pp_n_scalar;
            uint32_t s_block_stack = n_end > 4 ? 2 : 1;
            // PV is in stage 3, which means the 1st PV block corresponding to the 3th QK in our pipeline strategy

            uint32_t pv_stage = 3;
            uint32_t launch_delay = s_block_stack * 2;
            uint32_t vect_mod = 2 * launch_delay;
            uint32_t m_slice = sub_m > 32 ? 32 : 0; // s_block_stack=2时，UB可以放下
            uint32_t m_end = sub_m > 32 ? 2 : 1;
            for (uint32_t n_idx = n_start; n_idx < n_end + launch_delay; n_idx += s_block_stack) {
                if (n_idx < n_end) {
                    if (n_idx + s_block_stack > n_end - 1) {
                        qk_n = qk_n_triu > kv_seqlen ? kv_seqlen - n_idx * pp_n_scalar : qk_n_triu - n_idx * pp_n_scalar;
                    } else {
                        qk_n = pp_n_scalar * s_block_stack;
                    }
                    qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                    if (qk_n <= VECTOR_SIZE) {
                        if (sub_m > 0 && maskType != MaskType::MASK_TYPE_NONE) {
                            if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_128) {
                                if (n_idx == n_end - 1) {
                                    mask_offset = head_idx * alibi_compress_offset * BASE_MASK_SIZE;
                                } else {
                                    delta_uint = m_idx * pp_m_scalar - n_idx * pp_n_scalar + prefixLen;
                                    mask_offset = BASE_MASK_SIZE * delta_uint + head_idx * alibi_compress_offset * BASE_MASK_SIZE;
                                }
                            } else if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS || maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT) {
                                if (n_idx == n_end - 1) {
                                    mask_offset = 0;
                                    delta_uint = 0;
                                    delta = 0;
                                } else {
                                    mask_offset = BASE_MASK_SIZE * max_seqlen;
                                    delta_uint = m_idx * pp_m_scalar - n_idx * pp_n_scalar + prefixLen;
                                    delta = base_y + delta_uint;
                                }
                            } else if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_LEFT_ALIGN) {
                                if (n_idx == n_end - 1) {
                                    mask_offset = 0;
                                } else {
                                    mask_offset = BASE_MASK_SIZE * max_seqlen; // 128*256
                                }
                                delta = -base_y * n_idx ;
                            }
                            if (long_seq == 0) {
                                WAIT_FLAG(V, MTE2, EVENT_ID1);
                                gm_to_ub_align<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                    mask16_ubuf_tensor,
                                    mask_gm_tensor[mask_offset + sub_block_idx * qk_m / 2 * max_seqlen],
                                    0,                       // sid
                                    sub_m,                   // nBurst
                                    qk_n * 2,                // lenBurst
                                    0,                       // leftPaddingNum
                                    0,                       // rightPaddingNum
                                    (max_seqlen - qk_n) * 2, // srcGap
                                    0                        // dstGap
                                );
                                SET_FLAG(MTE2, V, EVENT_ID1);
                                mask_offset += qk_n;
                                WAIT_FLAG(MTE2, V, EVENT_ID1);
                                conv_v<ArchType::ASCEND_V220, IN_DATA_TYPE, float>(
                                    mask_ubuf_tensor, mask16_ubuf_tensor,
                                    (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1,                                                                // dstBlockStride
                                    1,                                                                // srcBlockStride
                                    8,                                                                // dstRepeatStride
                                    4                                                                 // srcRepeatStride
                                );
                                if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS || maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT || maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_LEFT_ALIGN) {
                                    PIPE_BARRIER(V);
                                    if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT && (m_idx + prefixIdx) != n_idx) {
                                        mul_v<ArchType::ASCEND_V220, float>(
                                            mask_ubuf_tensor,
                                            mask_ubuf_tensor,
                                            mask_ubuf_tensor,
                                            (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                            1,         // dstBlockStride
                                            1,         // src0BlockStride
                                            1,         // src1BlockStride
                                            8,         // dstRepeatStride
                                            8,         // src0RepeatStride
                                            8          // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                    }
                                    adds_v<ArchType::ASCEND_V220, float>(
                                        mask_ubuf_tensor,
                                        mask_ubuf_tensor,
                                        (float)delta,
                                        (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1,                                                                // dstBlockStride
                                        1,                                                                // srcBlockStride
                                        8,                                                                // dstRepeatStride
                                        8                                                                 // srcRepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                    if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT && (m_idx + prefixIdx) != n_idx) {
                                        sqrt_v<ArchType::ASCEND_V220, float>(
                                            mask_ubuf_tensor,
                                            mask_ubuf_tensor,
                                            (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                            1,                                                                // dstBlockStride
                                            1,                                                                // srcBlockStride
                                            8,                                                                // dstRepeatStride
                                            8                                                                 // srcRepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                    }
                                    muls_v<ArchType::ASCEND_V220, float>(mask_ubuf_tensor, mask_ubuf_tensor, (float)alibi_coeff,
                                        (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1,                                                                // dstBlockStride
                                        1,                                                                // srcBlockStride
                                        8,                                                                // dstRepeatStride
                                        8                                                                 // srcRepeatStride
                                    );
                                }
                                PIPE_BARRIER(V);
                                if (IS_BF16 && head_stride == 0 && maskType != MaskType::MASK_TYPE_ALIBI && maskType != MaskType::MASK_TYPE_ALIBI_COMPRESS && maskType != MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT && maskType != MaskType::MASK_TYPE_ALIBI_COMPRESS_LEFT_ALIGN) {
                                    muls_v<ArchType::ASCEND_V220, float>(
                                            mask_ubuf_tensor,
                                            mask_ubuf_tensor,
                                            (float)-3e38,
                                            (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                            1, // dstBlockStride
                                            1, // srcBlockStride
                                            8, // dstRepeatStride
                                            8  // srcRepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                }
                            }
                        }
                        WaitFlagDev(QK_READY);
                        if (sub_m > 0) {
                            WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
                            // input QK
                            gm_to_ub<ArchType::ASCEND_V220, float>(
                                ls_ubuf_tensor,
                                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx - n_start) % vect_mod * TMP_SIZE / vect_mod +
                                    (uint64_t)sub_block_idx * qk_m / 2 * qk_round_n],
                                0,                                // sid
                                sub_m,                                // nBurst
                                qk_round_n / FLOAT_BLOCK_SIZE,  // lenBurst
                                0,                                // srcGap
                                0                                 // dstGap
                            );
                            SET_FLAG(MTE2, V, EVENT_ID0);
                            WAIT_FLAG(MTE2, V, EVENT_ID0);
                            // *** ls = tor * ls
                            muls_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor, ls_ubuf_tensor, tor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                                // dstBlockStride
                                1,                                                                // srcBlockStride
                                8,                                                                // dstRepeatStride
                                8                                                                 // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** ls = ls + mask
                            if (maskType != MaskType::MASK_TYPE_NONE) {
                                if (long_seq == 0) {
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls_ubuf_tensor,
                                        ls_ubuf_tensor,
                                        mask_ubuf_tensor,
                                        (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1,                                                                // dstBlockStride
                                        1,                                                                // src0BlockStride
                                        1,                                                                // src1BlockStride
                                        8,                                                                // dstRepeatStride
                                        8,                                                                // src0RepeatStride
                                        8                                                                 // src1RepeatStride
                                    );
                                    SET_FLAG(V, MTE2, EVENT_ID1);
                                } else if (pp_n_scalar == FLOAT_VECTOR_SIZE && s_block_stack == 2 && n_idx == n_end - 2) {
                                    __set_mask(qk_n - FLOAT_VECTOR_SIZE);
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls_ubuf_tensor[FLOAT_VECTOR_SIZE],
                                        ls_ubuf_tensor[FLOAT_VECTOR_SIZE],
                                        mask_ubuf_tensor,
                                        sub_m, // repeat
                                        1,                                                                // dstBlockStride
                                        1,                                                                // src0BlockStride
                                        1,                                                                // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE,                                                                // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE,                                                                // src0RepeatStride
                                        16                                                                 // src1RepeatStride
                                    );
                                } else if (n_idx == n_end - 1) {
                                    if (qk_n < FLOAT_VECTOR_SIZE){
                                        __set_mask(qk_n);
                                    } else {
                                        __set_mask(FLOAT_VECTOR_SIZE);
                                    }
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls_ubuf_tensor,
                                        ls_ubuf_tensor,
                                        mask_ubuf_tensor,
                                        sub_m, // repeat
                                        1,                                                                // dstBlockStride
                                        1,                                                                // src0BlockStride
                                        1,                                                                // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE,                                                                // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE,                                                                // src0RepeatStride
                                        16                                                                 // src1RepeatStride
                                    );
                                    if (qk_n > FLOAT_VECTOR_SIZE) {
                                        __set_mask(qk_n - FLOAT_VECTOR_SIZE);
                                        add_v<ArchType::ASCEND_V220, float>(
                                            ls_ubuf_tensor[FLOAT_VECTOR_SIZE],
                                            ls_ubuf_tensor[FLOAT_VECTOR_SIZE],
                                            mask_ubuf_tensor[FLOAT_VECTOR_SIZE],
                                            sub_m, // repeat
                                            1,                                                                // dstBlockStride
                                            1,                                                                // src0BlockStride
                                            1,                                                                // src1BlockStride
                                            qk_round_n / FLOAT_BLOCK_SIZE,                                                                // dstRepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE,                                                                // src0RepeatStride
                                            16                                                                 // src1RepeatStride
                                        );
                                    }
                                }
                                PIPE_BARRIER(V);
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            // *** lm = rowmax(ls)
                            if (qk_n <= FLOAT_VECTOR_SIZE) {
                                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                cgmax_v<ArchType::ASCEND_V220, float>(
                                    tv_ubuf_tensor,
                                    ls_ubuf_tensor,
                                    sub_m,
                                    1,
                                    1,
                                    qk_round_n / FLOAT_BLOCK_SIZE
                                );
                                PIPE_BARRIER(V);
                                __set_vcg_mask((qk_n + FLOAT_BLOCK_SIZE - 1)/ FLOAT_BLOCK_SIZE);
                                cgmax_v<ArchType::ASCEND_V220, float>(
                                    lm_ubuf_tensor,
                                    tv_ubuf_tensor,
                                    (sub_m * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                    1,
                                    1,
                                    8
                                );
                                PIPE_BARRIER(V);
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            } else {
                                cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, ls_ubuf_tensor,
                                                sub_m,
                                                1,
                                                1,
                                                qk_round_n / 8
                                );
                                PIPE_BARRIER(V);
                                cgmax_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, tv_ubuf_tensor,
                                                round_sub_m * 8 / 64,
                                                1,
                                                1,
                                                8
                                );
                                PIPE_BARRIER(V);
                                for (uint32_t rowMaxIdx = 1; rowMaxIdx < qk_n / FLOAT_VECTOR_SIZE; ++rowMaxIdx) {
                                    cgmax_v<ArchType::ASCEND_V220, float>(
                                        tv_ubuf_tensor,
                                        ls_ubuf_tensor[rowMaxIdx * FLOAT_VECTOR_SIZE],
                                        sub_m,
                                        1,
                                        1,
                                        qk_round_n / 8
                                    );
                                    PIPE_BARRIER(V);
                                    cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                                            round_sub_m * 8 / 64,
                                            1,
                                            1,
                                            8
                                    );
                                    PIPE_BARRIER(V);
                                    __set_mask(sub_m);
                                    max_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, lm_ubuf_tensor,
                                        tv_ubuf_tensor,
                                        1,                        // repeat
                                        1,                            // dstBlockStride
                                        1,                            // src0BlockStride
                                        1,                            // src1BlockStride
                                        8,                            // dstRepeatStride
                                        8,                            // src0RepeatStride
                                        8 // src1RepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                }
                                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                    cgmax_v<ArchType::ASCEND_V220, float>(
                                        tv_ubuf_tensor,
                                        ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        sub_m,
                                        1,
                                        1,
                                        qk_round_n / 8
                                    );
                                    PIPE_BARRIER(V);
                                    __set_vcg_mask((qk_n % FLOAT_VECTOR_SIZE + FLOAT_BLOCK_SIZE - 1)/ FLOAT_BLOCK_SIZE);
                                    cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                                                round_sub_m * 8 / 64,
                                                1,
                                                1,
                                                8
                                    );
                                    PIPE_BARRIER(V);
                                    __set_mask(sub_m);
                                    max_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, lm_ubuf_tensor,
                                        tv_ubuf_tensor,
                                        1,                        // repeat
                                        1,                            // dstBlockStride
                                        1,                            // src0BlockStride
                                        1,                            // src1BlockStride
                                        8,                            // dstRepeatStride
                                        8,                            // src0RepeatStride
                                        8 // src1RepeatStride
                                    );
                                }
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            if (n_idx == n_start) {
                                // *** hm = lm
                                ub_to_ub<ArchType::ASCEND_V220, float>(
                                    hm_ubuf_tensor,
                                    lm_ubuf_tensor,
                                    0,                              // sid
                                    1,                              // nBurst
                                    round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                    0,                              // srcGap
                                    0                               // dstGap
                                );
                                PIPE_BARRIER(V);
                            } else {
                                // *** hm = vmax(lm, gm)
                                max_v<ArchType::ASCEND_V220, float>(
                                    hm_ubuf_tensor,
                                    lm_ubuf_tensor,
                                    gm_ubuf_tensor,
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm = gm - hm
                                sub_v<ArchType::ASCEND_V220, float>(
                                    dm_ubuf_tensor[((n_idx - n_start) / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    gm_ubuf_tensor,
                                    hm_ubuf_tensor,
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm = exp(dm)
                                exp_v<ArchType::ASCEND_V220, float>(
                                    dm_ubuf_tensor[((n_idx - n_start) / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    dm_ubuf_tensor[((n_idx - n_start) / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // srcBlockStride
                                    8,         // dstRepeatStride
                                    8          // srcRepeatStride
                                );
                            }
                            // *** gm = hm
                            ub_to_ub<ArchType::ASCEND_V220, float>(
                                gm_ubuf_tensor,
                                hm_ubuf_tensor,
                                0,                              // sid
                                1,                              // nBurst
                                round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                0,                              // srcGap
                                0                               // dstGap
                            );
                            PIPE_BARRIER(V);
                            // *** hm_block = expand_to_block(hm), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                hm_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                1,                             // dstBlockStride
                                8,                             // dstRepeatStride
                                round_sub_m / FLOAT_BLOCK_SIZE // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** ls = ls - hm_block
                            for (uint32_t vsub_idx = 0; vsub_idx < qk_n / FLOAT_VECTOR_SIZE; ++vsub_idx) {
                                sub_v<ArchType::ASCEND_V220, float>(
                                    ls_ubuf_tensor[vsub_idx * FLOAT_VECTOR_SIZE],
                                    ls_ubuf_tensor[vsub_idx * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                         // repeat
                                    1,                             // dstBlockStride
                                    1,                             // src0BlockStride
                                    0,                             // src1BlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                              // src1RepeatStride
                                );
                            }
                            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                sub_v<ArchType::ASCEND_V220, float>(
                                    ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                         // repeat
                                    1,                             // dstBlockStride
                                    1,                             // src0BlockStride
                                    0,                             // src1BlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                              // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            // *** ls = exp(ls)
                            exp_v<ArchType::ASCEND_V220, float>(
                                ls32_ubuf_tensor,
                                ls_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                                // dstBlockStride
                                1,                                                                // srcBlockStride
                                8,                                                                // dstRepeatStride
                                8                                                                 // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** lp = castfp32to16(ls)
                            conv_v<ArchType::ASCEND_V220, float, IN_DATA_TYPE>(
                                lp_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(), ls32_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1, // dstBlockStride
                                1, // srcBlockStride
                                4, // dstRepeatStride
                                8  // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            SET_FLAG(V, MTE3, EVENT_ID0);
                            WAIT_FLAG(V, MTE3, EVENT_ID0);
                            ub_to_gm<ArchType::ASCEND_V220, QKV_DT>(
                                p_gm_tensor[((uint64_t)block_idx * TMP_SIZE + (n_idx - n_start) % vect_mod * TMP_SIZE / vect_mod +
                                            (uint64_t)sub_block_idx * qk_m / 2 * qk_round_n) * 2 / sizeof(QKV_DT)],
                                lp_ubuf_tensor.ReinterpretCast<QKV_DT>(),
                                0,                                            // sid
                                1,                                            // nBurst
                                sub_m * qk_round_n * 2 / BlockSize<int8_t>(), // lenBurst
                                0,                                            // srcGap
                                0                                             // dstGap
                            );
                            SET_FLAG(MTE3, MTE2, EVENT_ID0);
                            // *** ll = rowsum(ls32)
                            if (qk_n <= FLOAT_VECTOR_SIZE) {
                                __set_mask(qk_n);
                                cadd_v<ArchType::ASCEND_V220, float>(
                                    ll_ubuf_tensor,
                                    ls32_ubuf_tensor,
                                    sub_m,                         // repeat
                                    1,                             // dstRepeatStride
                                    1,                             // srcBlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            } else {
                                for (uint32_t rowSumIdx = 1; rowSumIdx < qk_n / FLOAT_VECTOR_SIZE; ++rowSumIdx) {
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor[rowSumIdx * FLOAT_VECTOR_SIZE],
                                        sub_m,                         // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        1,                             // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                }
                                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        sub_m,                         // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        1,                             // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                    );
                                }
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                PIPE_BARRIER(V);
                                cadd_v<ArchType::ASCEND_V220, float>(
                                    ll_ubuf_tensor,
                                    ls32_ubuf_tensor,
                                    sub_m,                         // repeat
                                    1,                             // dstRepeatStride
                                    1,                             // srcBlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                                );
                            }
                            PIPE_BARRIER(V);
                            if (n_idx == n_start) {
                                // *** gl = ll
                                ub_to_ub<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor,
                                    ll_ubuf_tensor,
                                    0,                              // sid
                                    1,                              // nBurst
                                    round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                    0,                              // srcGap
                                    0                               // dstGap
                                );
                                PIPE_BARRIER(V);
                            } else {
                                // *** gl = dm * gl
                                mul_v<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor,
                                    dm_ubuf_tensor[((n_idx - n_start) / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    gl_ubuf_tensor,
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** gl = ll + gl
                                add_v<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor,
                                    gl_ubuf_tensor,
                                    ll_ubuf_tensor,
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                            }
                        }
                    } else {
                        bool last_n_loop = n_idx + s_block_stack > n_end - 1;
                        for (uint32_t split_idx = 0; split_idx < m_end; split_idx++) {
                            bool last_m_loop = split_idx == m_end - 1;
                            uint32_t m_split =  last_m_loop ? sub_m - split_idx * m_slice : m_slice;
                            uint32_t round_m_split = (m_split + FLOAT_BLOCK_SIZE - 1) / FLOAT_BLOCK_SIZE * FLOAT_BLOCK_SIZE;
                            if (sub_m > 0 && maskType != MaskType::MASK_TYPE_NONE && long_seq == 0) {
                                WAIT_FLAG(V, MTE2, EVENT_ID1);
                                uint64_t mask_offset_tail = 0;
                                if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS || maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT || maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_LEFT_ALIGN) {
                                    mask_offset = BASE_MASK_SIZE * SOFTMAX_MAX_LENGTH;
                                    if (alibi_left_align == 0) {
                                        delta = base_y * (n_idx + 1 - m_idx - prefixIdx);
                                    } else {
                                        delta = -base_y * n_idx;
                                    }
                                    gm_to_ub<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                        mask16_ubuf_tensor,
                                        mask_gm_tensor[mask_offset + (sub_block_idx * qk_m / 2 + split_idx * m_slice) * SOFTMAX_MAX_LENGTH],
                                        0,                                // sid
                                        m_split,                                // nBurst
                                        qk_round_n / BLOCK_SIZE,  // lenBurst
                                        (SOFTMAX_MAX_LENGTH - qk_round_n) / BLOCK_SIZE, // srcGap
                                        0                                 // dstGap
                                    );
                                } else if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_128) {
                                    delta_uint = m_idx * pp_m_scalar - n_idx * pp_n_scalar + prefixLen;
                                    mask_offset = BASE_MASK_SIZE * delta_uint + head_idx * alibi_compress_offset * BASE_MASK_SIZE;
                                    mask_offset_tail = mask_offset - BASE_MASK_SIZE * pp_n_scalar;
                                    if (n_idx == n_end - 2) {
                                        mask_offset_tail = head_idx * alibi_compress_offset * BASE_MASK_SIZE;
                                    }
                                    gm_to_ub<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                        mask16_ubuf_tensor,
                                        mask_gm_tensor[mask_offset + (sub_block_idx * qk_m / 2 + split_idx * m_slice) * VECTOR_SIZE],
                                        0,                                // sid
                                        m_split,                                // nBurst
                                        8,  // lenBurst
                                        0, // srcGap
                                        (qk_round_n - VECTOR_SIZE)/ BLOCK_SIZE                                 // dstGap
                                    );
                                    gm_to_ub<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                        mask16_ubuf_tensor[VECTOR_SIZE],
                                        mask_gm_tensor[mask_offset_tail + (sub_block_idx * qk_m / 2 + split_idx * m_slice) * VECTOR_SIZE],
                                        0,                                // sid
                                        m_split,                                // nBurst
                                        (qk_round_n - VECTOR_SIZE)/ BLOCK_SIZE,  // lenBurst
                                        (SOFTMAX_MAX_LENGTH - qk_round_n)/ BLOCK_SIZE, // srcGap
                                        8                                 // dstGap
                                    );
                                } else {
                                    gm_to_ub_align<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                        mask16_ubuf_tensor,
                                        mask_gm_tensor[mask_offset + (sub_block_idx * qk_m / 2 + split_idx * m_slice) * max_seqlen],
                                        0,                       // sid
                                        m_split,                   // nBurst
                                        qk_n * 2,                // lenBurst
                                        0,                       // leftPaddingNum
                                        0,                       // rightPaddingNum
                                        (max_seqlen - qk_n) * 2, // srcGap
                                        0                        // dstGap
                                    );
                                }
                                SET_FLAG(MTE2, V, EVENT_ID1);
                            }
                            if (split_idx == 0) {
                                WaitFlagDev(QK_READY);
                            }
                            if (sub_m > 0) {
                                if (m_split > 0) {
                                    if (maskType != MaskType::MASK_TYPE_NONE && long_seq == 0) {
                                        WAIT_FLAG(MTE2, V, EVENT_ID1);
                                        conv_v<ArchType::ASCEND_V220, IN_DATA_TYPE, float>(
                                            mask_ubuf_tensor,
                                            mask16_ubuf_tensor,
                                            (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                            1,                                                                // dstBlockStride
                                            1,                                                                // srcBlockStride
                                            8,                                                                // dstRepeatStride
                                            4                                                                 // srcRepeatStride
                                        );
                                        SET_FLAG(V, MTE2, EVENT_ID1);
                                        if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS || maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT || maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_LEFT_ALIGN) {
                                            PIPE_BARRIER(V);
                                            if (n_idx != n_end - 2) {
                                                if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT) {
                                                    mul_v<ArchType::ASCEND_V220, float>(
                                                        mask_ubuf_tensor,
                                                        mask_ubuf_tensor,
                                                        mask_ubuf_tensor,
                                                        (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                                        1,         // dstBlockStride
                                                        1,         // src0BlockStride
                                                        1,         // src1BlockStride
                                                        8,         // dstRepeatStride
                                                        8,         // src0RepeatStride
                                                        8          // src1RepeatStride
                                                    );
                                                    PIPE_BARRIER(V);
                                                }
                                                adds_v<ArchType::ASCEND_V220, float>(
                                                    mask_ubuf_tensor,
                                                    mask_ubuf_tensor,
                                                    (float)delta,
                                                    (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                                    1,                                                                // dstBlockStride
                                                    1,                                                                // srcBlockStride
                                                    8,                                                                // dstRepeatStride
                                                    8                                                                 // srcRepeatStride
                                                );
                                                PIPE_BARRIER(V);
                                                if (alibi_left_align == 1) {
                                                    adds_v<ArchType::ASCEND_V220, float>(
                                                        mask_ubuf_tensor[128],
                                                        mask_ubuf_tensor,
                                                        (float)-base_y,
                                                        m_split, // repeat
                                                        1,                                                                // dstBlockStride
                                                        1,                                                                // srcBlockStride
                                                        qk_round_n / 8,                                                                // dstRepeatStride
                                                        qk_round_n / 8                                                                 // srcRepeatStride
                                                    );
                                                    adds_v<ArchType::ASCEND_V220, float>(
                                                        mask_ubuf_tensor[128 + FLOAT_VECTOR_SIZE],
                                                        mask_ubuf_tensor[FLOAT_VECTOR_SIZE],
                                                        (float)-base_y,
                                                        m_split, // repeat
                                                        1,                                                                // dstBlockStride
                                                        1,                                                                // srcBlockStride
                                                        qk_round_n / 8,                                                                // dstRepeatStride
                                                        qk_round_n / 8                                                                 // srcRepeatStride
                                                    );
                                                } else {
                                                    adds_v<ArchType::ASCEND_V220, float>(
                                                        mask_ubuf_tensor[128],
                                                        mask_ubuf_tensor,
                                                        base_y,
                                                        m_split, // repeat
                                                        1,                                                                // dstBlockStride
                                                        1,                                                                // srcBlockStride
                                                        qk_round_n / 8,                                                                // dstRepeatStride
                                                        qk_round_n / 8                                                                 // srcRepeatStride
                                                    );
                                                    adds_v<ArchType::ASCEND_V220, float>(
                                                        mask_ubuf_tensor[128 + FLOAT_VECTOR_SIZE],
                                                        mask_ubuf_tensor[FLOAT_VECTOR_SIZE],
                                                        base_y,
                                                        m_split, // repeat
                                                        1,                                                                // dstBlockStride
                                                        1,                                                                // srcBlockStride
                                                        qk_round_n / 8,                                                                // dstRepeatStride
                                                        qk_round_n / 8                                                                 // srcRepeatStride
                                                    );
                                                }
                                                PIPE_BARRIER(V);
                                                if (maskType == MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT) {
                                                    sqrt_v<ArchType::ASCEND_V220, float>(
                                                        mask_ubuf_tensor,
                                                        mask_ubuf_tensor,
                                                        (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                                        1,                                                                // dstBlockStride
                                                        1,                                                                // srcBlockStride
                                                        8,                                                                // dstRepeatStride
                                                        8                                                                 // srcRepeatStride
                                                    );
                                                    PIPE_BARRIER(V);
                                                }
                                            } else if (alibi_left_align == 1) {
                                                adds_v<ArchType::ASCEND_V220, float>(
                                                    mask_ubuf_tensor,
                                                    mask_ubuf_tensor,
                                                    (float)delta,
                                                    (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                                    1,                                                                // dstBlockStride
                                                    1,                                                                // srcBlockStride
                                                    8,                                                                // dstRepeatStride
                                                    8                                                                 // srcRepeatStride
                                                );
                                                PIPE_BARRIER(V);
                                            }

                                            muls_v<ArchType::ASCEND_V220, float>(
                                                mask_ubuf_tensor,
                                                mask_ubuf_tensor,
                                                (float)alibi_coeff,
                                                (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                                1,                                                                // dstBlockStride
                                                1,                                                                // srcBlockStride
                                                8,                                                                // dstRepeatStride
                                                8                                                                 // srcRepeatStride
                                            );
                                        }
                                        PIPE_BARRIER(V);
                                        if (IS_BF16 && head_stride == 0 && maskType != MaskType::MASK_TYPE_ALIBI && maskType != MaskType::MASK_TYPE_ALIBI_COMPRESS && maskType != MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT && maskType != MaskType::MASK_TYPE_ALIBI_COMPRESS_LEFT_ALIGN) {
                                            muls_v<ArchType::ASCEND_V220, float>(
                                                    mask_ubuf_tensor,
                                                    mask_ubuf_tensor,
                                                    (float)-3e38,
                                                    (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                                    1, // dstBlockStride
                                                    1, // srcBlockStride
                                                    8, // dstRepeatStride
                                                    8  // srcRepeatStride
                                            );
                                            PIPE_BARRIER(V);
                                        }
                                    }
                                    WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
                                    // input QK
                                    gm_to_ub<ArchType::ASCEND_V220, float>(
                                        ls_ubuf_tensor,
                                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx - n_start) % vect_mod * TMP_SIZE / vect_mod +
                                            (uint64_t)(sub_block_idx * qk_m / 2 + split_idx * m_slice) * qk_round_n],
                                        0,                                // sid
                                        m_split,                                // nBurst
                                        qk_round_n / FLOAT_BLOCK_SIZE,  // lenBurst
                                        0,                                // srcGap
                                        0                                 // dstGap
                                    );
                                    SET_FLAG(MTE2, V, EVENT_ID0);
                                    WAIT_FLAG(MTE2, V, EVENT_ID0);
                                    // *** ls = tor * ls
                                    muls_v<ArchType::ASCEND_V220, float>(
                                        ls_ubuf_tensor,
                                        ls_ubuf_tensor,
                                        tor,
                                        (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1,                                                                // dstBlockStride
                                        1,                                                                // srcBlockStride
                                        8,                                                                // dstRepeatStride
                                        8                                                                 // srcRepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                    // *** ls = ls + mask
                                    if (maskType != MaskType::MASK_TYPE_NONE) {
                                        if (long_seq == 0) {
                                            add_v<ArchType::ASCEND_V220, float>(
                                                ls_ubuf_tensor,
                                                ls_ubuf_tensor,
                                                mask_ubuf_tensor,
                                                (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                                1,                                                                // dstBlockStride
                                                1,                                                                // src0BlockStride
                                                1,                                                                // src1BlockStride
                                                8,                                                                // dstRepeatStride
                                                8,                                                                // src0RepeatStride
                                                8                                                                 // src1RepeatStride
                                            );
                                        } else if (n_idx == n_end - 2) {
                                            if (qk_n - pp_n_scalar < FLOAT_VECTOR_SIZE){
                                                __set_mask(qk_n - pp_n_scalar);
                                            } else {
                                                __set_mask(FLOAT_VECTOR_SIZE);
                                            }
                                            add_v<ArchType::ASCEND_V220, float>(
                                                ls_ubuf_tensor[pp_n_scalar],
                                                ls_ubuf_tensor[pp_n_scalar],
                                                mask_ubuf_tensor[split_idx * m_slice * VECTOR_SIZE],
                                                m_split, // repeat
                                                1,                                                                // dstBlockStride
                                                1,                                                                // src0BlockStride
                                                1,                                                                // src1BlockStride
                                                qk_round_n / FLOAT_BLOCK_SIZE,                                                                // dstRepeatStride
                                                qk_round_n / FLOAT_BLOCK_SIZE,                                                                // src0RepeatStride
                                                16                                                                 // src1RepeatStride
                                            );
                                            if (qk_n - pp_n_scalar > FLOAT_VECTOR_SIZE) {
                                                __set_mask(qk_n - pp_n_scalar - FLOAT_VECTOR_SIZE);
                                                add_v<ArchType::ASCEND_V220, float>(
                                                    ls_ubuf_tensor[pp_n_scalar + FLOAT_VECTOR_SIZE],
                                                    ls_ubuf_tensor[pp_n_scalar + FLOAT_VECTOR_SIZE],
                                                    mask_ubuf_tensor[FLOAT_VECTOR_SIZE + split_idx * m_slice * VECTOR_SIZE],
                                                    m_split, // repeat
                                                    1,                                                                // dstBlockStride
                                                    1,                                                                // src0BlockStride
                                                    1,                                                                // src1BlockStride
                                                    qk_round_n / FLOAT_BLOCK_SIZE,                                                                // dstRepeatStride
                                                    qk_round_n / FLOAT_BLOCK_SIZE,                                                                // src0RepeatStride
                                                    16                                                                 // src1RepeatStride
                                                );
                                            }
                                        }
                                        PIPE_BARRIER(V);
                                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    }
                                    if (qk_n == SOFTMAX_MAX_LENGTH) {
                                        cgmax_v<ArchType::ASCEND_V220, float>(
                                            tv_ubuf_tensor,
                                            ls_ubuf_tensor,
                                            m_split * qk_n / FLOAT_VECTOR_SIZE,
                                            1,
                                            1,
                                            8
                                        );
                                        PIPE_BARRIER(V);
                                        __set_mask(32);
                                        cgmax_v<ArchType::ASCEND_V220, float>(
                                            tv_ubuf_tensor,
                                            tv_ubuf_tensor,
                                            m_split,
                                            1,
                                            1,
                                            4
                                        );
                                        PIPE_BARRIER(V);
                                        __set_vcg_mask(4);
                                        cgmax_v<ArchType::ASCEND_V220, float>(
                                            lm_ubuf_tensor,
                                            tv_ubuf_tensor,
                                            (m_split * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                            1,
                                            1,
                                            8
                                        );
                                        PIPE_BARRIER(V);
                                        __set_mask(m_split);
                                    } else {
                                        cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, ls_ubuf_tensor,
                                                        m_split,
                                                        1,
                                                        1,
                                                        qk_round_n / FLOAT_BLOCK_SIZE
                                        );
                                        PIPE_BARRIER(V);
                                        cgmax_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, tv_ubuf_tensor,
                                                        (m_split * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                                        1,
                                                        1,
                                                        8
                                        );
                                        PIPE_BARRIER(V);
                                        for (uint64_t rowmax_idx = 1; rowmax_idx < (uint64_t)qk_n / FLOAT_VECTOR_SIZE; ++rowmax_idx) {
                                            cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, ls_ubuf_tensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                                                        m_split,
                                                        1,
                                                        1,
                                                        qk_round_n / FLOAT_BLOCK_SIZE
                                            );
                                            PIPE_BARRIER(V);
                                            cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                                                        (m_split * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                                        1,
                                                        1,
                                                        8
                                            );
                                            PIPE_BARRIER(V);
                                            __set_mask(m_split);
                                            max_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, lm_ubuf_tensor,
                                                tv_ubuf_tensor,
                                                1,                        // repeat
                                                1,                            // dstBlockStride
                                                1,                            // src0BlockStride
                                                1,                            // src1BlockStride
                                                8,                            // dstRepeatStride
                                                8,                            // src0RepeatStride
                                                8 // src1RepeatStride
                                            );
                                            PIPE_BARRIER(V);
                                            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                        }
                                        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                            cgmax_v<ArchType::ASCEND_V220, float>(
                                                tv_ubuf_tensor,
                                                ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                                m_split,
                                                1,
                                                1,
                                                qk_round_n / FLOAT_BLOCK_SIZE
                                            );
                                            PIPE_BARRIER(V);
                                            __set_vcg_mask((qk_n % FLOAT_VECTOR_SIZE + FLOAT_BLOCK_SIZE - 1)/ FLOAT_BLOCK_SIZE);
                                            cgmax_v<ArchType::ASCEND_V220, float>(
                                                tv_ubuf_tensor,
                                                tv_ubuf_tensor,
                                                (m_split * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                                1,
                                                1,
                                                8
                                            );
                                            PIPE_BARRIER(V);
                                            __set_mask(m_split);
                                            max_v<ArchType::ASCEND_V220, float>(
                                                lm_ubuf_tensor,
                                                lm_ubuf_tensor,
                                                tv_ubuf_tensor,
                                                1,                        // repeat
                                                1,                            // dstBlockStride
                                                1,                            // src0BlockStride
                                                1,                            // src1BlockStride
                                                8,                            // dstRepeatStride
                                                8,                            // src0RepeatStride
                                                8 // src1RepeatStride
                                            );
                                        }
                                    }
                                    PIPE_BARRIER(V);
                                    if (n_idx == n_start) {
                                        // *** hm = lm
                                        ub_to_ub<ArchType::ASCEND_V220, float>(
                                            hm_ubuf_tensor[split_idx * m_slice],
                                            lm_ubuf_tensor,
                                            0,                              // sid
                                            1,                              // nBurst
                                            round_m_split / FLOAT_BLOCK_SIZE, // lenBurst
                                            0,                              // srcGap
                                            0                               // dstGap
                                        );
                                        PIPE_BARRIER(V);
                                    } else {
                                        // *** hm = vmax(lm, gm)
                                        max_v<ArchType::ASCEND_V220, float>(
                                            hm_ubuf_tensor[split_idx * m_slice],
                                            lm_ubuf_tensor,
                                            gm_ubuf_tensor[split_idx * m_slice],
                                            1,         // repeat
                                            1,         // dstBlockStride
                                            1,         // src0BlockStride
                                            1,         // src1BlockStride
                                            8,         // dstRepeatStride
                                            8,         // src0RepeatStride
                                            8          // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                        // *** dm = gm - hm
                                        sub_v<ArchType::ASCEND_V220, float>(
                                            dm_ubuf_tensor[((n_idx - n_start) / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE + split_idx * m_slice],
                                            gm_ubuf_tensor[split_idx * m_slice],
                                            hm_ubuf_tensor[split_idx * m_slice],
                                            1,         // repeat
                                            1,         // dstBlockStride
                                            1,         // src0BlockStride
                                            1,         // src1BlockStride
                                            8,         // dstRepeatStride
                                            8,         // src0RepeatStride
                                            8          // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                        // *** dm = exp(dm)
                                        exp_v<ArchType::ASCEND_V220, float>(
                                            dm_ubuf_tensor[((n_idx - n_start) / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE + split_idx * m_slice],
                                            dm_ubuf_tensor[((n_idx - n_start) / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE + split_idx * m_slice],
                                            1,         // repeat
                                            1,         // dstBlockStride
                                            1,         // srcBlockStride
                                            8,         // dstRepeatStride
                                            8          // srcRepeatStride
                                        );
                                    }
                                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    PIPE_BARRIER(V);
                                    // *** gm = hm
                                    ub_to_ub<ArchType::ASCEND_V220, float>(
                                        gm_ubuf_tensor[split_idx * m_slice],
                                        hm_ubuf_tensor[split_idx * m_slice],
                                        0,                              // sid
                                        1,                              // nBurst
                                        round_m_split / FLOAT_BLOCK_SIZE, // lenBurst
                                        0,                              // srcGap
                                        0                               // dstGap
                                    );
                                    PIPE_BARRIER(V);
                                    // *** hm_block = expand_to_block(hm), 存放于 tv
                                    brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                        tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                        hm_ubuf_tensor.ReinterpretCast<uint32_t>()[split_idx * m_slice],
                                        1,                             // dstBlockStride
                                        8,                             // dstRepeatStride
                                        round_m_split / FLOAT_BLOCK_SIZE // repeat
                                    );
                                    PIPE_BARRIER(V);
                                    // *** ls = ls - hm_block
                                    for (uint32_t vsub_idx = 0; vsub_idx < qk_n / FLOAT_VECTOR_SIZE; ++vsub_idx) {
                                        sub_v<ArchType::ASCEND_V220, float>(
                                            ls_ubuf_tensor[vsub_idx * FLOAT_VECTOR_SIZE],
                                            ls_ubuf_tensor[vsub_idx * FLOAT_VECTOR_SIZE],
                                            tv_ubuf_tensor,
                                            m_split,                         // repeat
                                            1,                             // dstBlockStride
                                            1,                             // src0BlockStride
                                            0,                             // src1BlockStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                            1                              // src1RepeatStride
                                        );
                                    }
                                    if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                        __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                        sub_v<ArchType::ASCEND_V220, float>(
                                            ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                            ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                            tv_ubuf_tensor,
                                            m_split,                         // repeat
                                            1,                             // dstBlockStride
                                            1,                             // src0BlockStride
                                            0,                             // src1BlockStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                            1                              // src1RepeatStride
                                        );
                                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    }
                                    PIPE_BARRIER(V);
                                    // *** ls = exp(ls)
                                    exp_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor,
                                        ls_ubuf_tensor,
                                        (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1,                                                                // dstBlockStride
                                        1,                                                                // srcBlockStride
                                        8,                                                                // dstRepeatStride
                                        8                                                                 // srcRepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                    conv_v<ArchType::ASCEND_V220, float, IN_DATA_TYPE>(
                                        lp_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(), ls32_ubuf_tensor,
                                        (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) /
                                            FLOAT_VECTOR_SIZE, // repeat
                                        1,                     // dstBlockStride
                                        1,                     // srcBlockStride
                                        4,                     // dstRepeatStride
                                        8                      // srcRepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                    SET_FLAG(V, MTE3, EVENT_ID0);
                                    WAIT_FLAG(V, MTE3, EVENT_ID0);
                                    ub_to_gm<ArchType::ASCEND_V220, QKV_DT>(
                                        p_gm_tensor[((uint64_t)block_idx * TMP_SIZE +
                                                    (n_idx - n_start) % vect_mod * TMP_SIZE / vect_mod +
                                                    ((uint64_t)sub_block_idx * qk_m / 2 + split_idx * m_slice) *
                                                        qk_round_n) * 2 / sizeof(QKV_DT)],
                                        lp_ubuf_tensor.ReinterpretCast<QKV_DT>(),
                                        0,                                    // sid
                                        m_split,                              // nBurst
                                        qk_round_n * 2 / BlockSize<int8_t>(), // lenBurst
                                        0,                                    // srcGap
                                        0                                     // dstGap
                                    );
                                    SET_FLAG(MTE3, MTE2, EVENT_ID0);
                                    // *** ll = rowsum(ls32)
                                    for (uint32_t rowsum_idx = 1; rowsum_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowsum_idx) {
                                        add_v<ArchType::ASCEND_V220, float>(
                                            ls32_ubuf_tensor,
                                            ls32_ubuf_tensor,
                                            ls32_ubuf_tensor[rowsum_idx * FLOAT_VECTOR_SIZE],
                                            m_split,                         // repeat
                                            1,                             // dstBlockStride
                                            1,                             // src0BlockStride
                                            1,                             // src1BlockStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                    }
                                    if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                        __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                        add_v<ArchType::ASCEND_V220, float>(
                                            ls32_ubuf_tensor,
                                            ls32_ubuf_tensor,
                                            ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                            m_split,                         // repeat
                                            1,                             // dstBlockStride
                                            1,                             // src0BlockStride
                                            1,                             // src1BlockStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                        );
                                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    }
                                    PIPE_BARRIER(V);
                                    cadd_v<ArchType::ASCEND_V220, float>(
                                        ll_ubuf_tensor,
                                        ls32_ubuf_tensor,
                                        m_split,                         // repeat
                                        1,                             // dstRepeatStride
                                        1,                             // srcBlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                    if (n_idx == n_start) {
                                        // *** gl = ll
                                        ub_to_ub<ArchType::ASCEND_V220, float>(
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            ll_ubuf_tensor,
                                            0,                              // sid
                                            1,                              // nBurst
                                            round_m_split / FLOAT_BLOCK_SIZE, // lenBurst
                                            0,                              // srcGap
                                            0                               // dstGap
                                        );
                                        PIPE_BARRIER(V);
                                    } else {
                                        __set_mask(m_split);
                                        // *** gl = dm * gl
                                        mul_v<ArchType::ASCEND_V220, float>(
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            dm_ubuf_tensor[((n_idx - n_start) / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE + split_idx * m_slice],
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            1, // repeat
                                            1,         // dstBlockStride
                                            1,         // src0BlockStride
                                            1,         // src1BlockStride
                                            8,         // dstRepeatStride
                                            8,         // src0RepeatStride
                                            8          // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                        // *** gl = ll + gl
                                        add_v<ArchType::ASCEND_V220, float>(
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            ll_ubuf_tensor,
                                            1, // repeat
                                            1,         // dstBlockStride
                                            1,         // src0BlockStride
                                            1,         // src1BlockStride
                                            8,         // dstRepeatStride
                                            8,         // src0RepeatStride
                                            8          // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    }
                                }
                            }
                        }
                        mask_offset += qk_n;
                    }
                    FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY);
                }
                if (n_idx >= launch_delay + n_start) {
                    WaitFlagDev(UPDATE_READY); // 4
                    if (sub_m > 0) {
                        // *** 更新 L 和 O
                        if (n_idx != launch_delay + n_start) {
                            WAIT_FLAG(V, MTE2, EVENT_ID2);
                            gm_to_ub<ArchType::ASCEND_V220, float>(
                                lo_ubuf_tensor,
                                o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx - launch_delay - n_start) % vect_mod * TMP_SIZE / vect_mod +
                                                (uint64_t)sub_block_idx * qk_m / 2 * round_k],
                                0,                                  // sid
                                1,                                  // nBurst
                                sub_m * round_k / FLOAT_BLOCK_SIZE, // lenBurst
                                0,                                  // srcGap
                                0                                   // dstGap
                            );
                            SET_FLAG(MTE2, V, EVENT_ID2);
                            // *** dm_block = expand_to_block(dm), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                dm_ubuf_tensor[((n_idx - launch_delay - n_start) / s_block_stack % 4) * UB_FLOAT_LINE_SIZE].ReinterpretCast<uint32_t>(),
                                1,                             // dstBlockStride
                                8,                             // dstRepeatStride
                                round_sub_m / FLOAT_BLOCK_SIZE // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** go = go * dm_block
                            for (uint32_t vmul_idx = 0; vmul_idx < __k / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                                mul_v<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                      // repeat
                                    1,                          // dstBlockStride
                                    1,                          // src0BlockStride
                                    0,                          // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                           // src1RepeatStride
                                );
                            }
                            if (__k % FLOAT_VECTOR_SIZE > 0) {
                                __set_mask(__k % FLOAT_VECTOR_SIZE);
                                mul_v<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                      // repeat
                                    1,                          // dstBlockStride
                                    1,                          // src0BlockStride
                                    0,                          // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                           // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            // *** go = lo + go
                            WAIT_FLAG(MTE2, V, EVENT_ID2);
                            add_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor, go_ubuf_tensor, lo_ubuf_tensor,
                                (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                             // dstBlockStride
                                1,                                                             // src0BlockStride
                                1,                                                             // src1BlockStride
                                8,                                                             // dstRepeatStride
                                8,                                                             // src0RepeatStride
                                8                                                              // src1RepeatStride
                            );
                            PIPE_BARRIER(V);
                            SET_FLAG(V, MTE2, EVENT_ID2);
                        } else {
                            WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
                            gm_to_ub<ArchType::ASCEND_V220, float>(
                                go_ubuf_tensor,
                                o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx - launch_delay - n_start) % vect_mod * TMP_SIZE / vect_mod +
                                    (uint64_t)sub_block_idx * qk_m / 2 * round_k],
                                0,                                  // sid
                                1,                                  // nBurst
                                sub_m * round_k / FLOAT_BLOCK_SIZE, // lenBurst
                                0,                                  // srcGap
                                0                                   // dstGap
                            );
                            SET_FLAG(MTE2, V, EVENT_ID3);
                            WAIT_FLAG(MTE2, V, EVENT_ID3);
                        }
                        if (n_idx + s_block_stack > n_end + launch_delay - 1)  {
                            // *** gl_block = expand_to_block(gl), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                gl_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                1,                             // dstBlockStride
                                8,                             // dstRepeatStride
                                round_sub_m / FLOAT_BLOCK_SIZE // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** go = go / gl_block
                            for (uint32_t vdiv_idx = 0; vdiv_idx < __k / FLOAT_VECTOR_SIZE; ++vdiv_idx) {
                                div_v<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                      // repeat
                                    1,                          // dstBlockStride
                                    1,                          // src0BlockStride
                                    0,                          // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                           // src1RepeatStride
                                );
                            }
                            if (__k % FLOAT_VECTOR_SIZE > 0) {
                                __set_mask(__k % FLOAT_VECTOR_SIZE);
                                div_v<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                      // repeat
                                    1,                          // dstBlockStride
                                    1,                          // src0BlockStride
                                    0,                          // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                           // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            conv_v<ArchType::ASCEND_V220, float, IN_DATA_TYPE>(
                                go_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(),
                                go_ubuf_tensor,
                                (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1, // dstBlockStride
                                1, // srcBlockStride
                                4, // dstRepeatStride
                                8  // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // ********************* move O to GM ************************
                            SET_FLAG(V, MTE3, EVENT_ID0);
                            WAIT_FLAG(V, MTE3, EVENT_ID0);
                            ub_to_gm_align<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                o_gm_tensor[o_offset + (uint64_t)sub_block_idx * qk_m / 2 * stride_qo],
                                go_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(),
                                0,                    // sid
                                sub_m,                // nBurst
                                __k * 2,              // lenBurst
                                0,                    // leftPaddingNum
                                0,                    // rightPaddingNum
                                0,                    // srcGap
                                (stride_qo - __k) * 2 // dstGap
                            );
                            SET_FLAG(MTE3, MTE2, EVENT_ID2);
                        }
                    }
                }
            }
        }
    }

private:
    __gm__ uint8_t *__restrict__ mask_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_gm{nullptr};
    __gm__ uint8_t *__restrict__ s_gm{nullptr};
    __gm__ uint8_t *__restrict__ p_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_para_gm{nullptr};
    __gm__ uint8_t *__restrict__ alibi_coeff_gm{nullptr};

    const uint32_t ls_ubuf_offset = 0;
    const uint32_t lp_ubuf_offset = 0;
    const uint32_t ls32_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE;
    const uint32_t mask_ubuf_offset = 4 * UB_UINT8_BLOCK_SIZE;
    const uint32_t lo_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE;
    const uint32_t lm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE;
    const uint32_t hm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 1 * UB_UINT8_LINE_SIZE;
    const uint32_t gm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 2 * UB_UINT8_LINE_SIZE;
    const uint32_t dm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 4 * UB_UINT8_LINE_SIZE;
    const uint32_t ll_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 8 * UB_UINT8_LINE_SIZE;
    const uint32_t gl_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 12 * UB_UINT8_LINE_SIZE;
    const uint32_t tv_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 13 * UB_UINT8_LINE_SIZE;
    const uint32_t p_scale_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 21 * UB_UINT8_LINE_SIZE;
    const uint32_t log_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 30 * UB_UINT8_LINE_SIZE;
    const uint32_t log_ubuf_float_offset = 8 * UB_UINT8_BLOCK_SIZE + 30 * UB_UINT8_LINE_SIZE;
    const uint32_t go_ubuf_offset = 9 * UB_UINT8_BLOCK_SIZE;
    const uint32_t mask16_ubuf_offset = 11 * UB_UINT8_BLOCK_SIZE;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<float> ls_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ls_ubuf_offset);
    AscendC::LocalTensor<float> lp_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lp_ubuf_offset);
    AscendC::LocalTensor<float> ls32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ls32_ubuf_offset);
    AscendC::LocalTensor<float> mask_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(mask_ubuf_offset);
    AscendC::LocalTensor<float> lo_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lo_ubuf_offset);
    AscendC::LocalTensor<float> lm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lm_ubuf_offset);
    AscendC::LocalTensor<float> hm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(hm_ubuf_offset);
    AscendC::LocalTensor<float> gm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gm_ubuf_offset);
    AscendC::LocalTensor<float> dm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(dm_ubuf_offset);
    AscendC::LocalTensor<float> ll_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_offset);
    AscendC::LocalTensor<float> gl_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gl_ubuf_offset);
    AscendC::LocalTensor<float> tv_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tv_ubuf_offset);
    AscendC::LocalTensor<float> p_scale_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(p_scale_ubuf_offset);
    AscendC::LocalTensor<float> go_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(go_ubuf_offset);
    AscendC::LocalTensor<IN_DATA_TYPE> mask16_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(mask16_ubuf_offset);

    AscendC::LocalTensor<IN_DATA_TYPE> log_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(log_ubuf_offset);
    AscendC::LocalTensor<float> log_ubuf_float_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(log_ubuf_float_offset);


    AscendC::GlobalTensor<IN_DATA_TYPE> mask_gm_tensor;
    AscendC::GlobalTensor<O_DT> o_gm_tensor;
    AscendC::GlobalTensor<float> s_gm_tensor;
    AscendC::GlobalTensor<QKV_DT> p_gm_tensor;
    AscendC::GlobalTensor<float> o_tmp_gm_tensor;
    ScaleType scaleType = ScaleType::SCALE_TOR;
    uint32_t batch_size{0};
    uint32_t max_seqlen{0};
    uint32_t q_heads{0};
    uint32_t embd{0};
    float tor{0};
    uint32_t head_stride{0};
    uint32_t mask_stride{0};
    uint32_t is_triu_mask{0};
    uint32_t total_q_blk_num{0};
    uint64_t stride_qo{0};
    uint32_t __k{0};
    uint32_t round_k{0};
    uint32_t go_flag_scalar{1};

    int32_t sub_block_idx{-1};
    uint32_t tilingKey{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t long_seq{0};
    uint32_t alibi_compress_offset{0};
    uint32_t alibi_left_align{0};
    uint32_t quantType{0};
	uint32_t window_size{0};
};

template<typename S_DTYPE, typename P_DTYPE, typename MASK_DTYPE, typename O_DTYPE, typename EXP_DTYPE, MaskType MASK_TYPE, ScaleType SCALE_TYPE, bool splitm = false>
class FlashAttentionEncoderHighPrecisionVecOpt {
public:
    __aicore__ __attribute__((always_inline)) inline FlashAttentionEncoderHighPrecisionVecOpt(
        __gm__ uint8_t *__restrict__ sync, __gm__ uint8_t *__restrict__ mask_gm,
        __gm__ uint8_t *__restrict__ alibi_coeff_gm, __gm__ uint8_t *__restrict__ o_gm,
        __gm__ uint8_t *__restrict__ s_gm, __gm__ uint8_t *__restrict__ p_gm, __gm__ uint8_t *__restrict__ o_tmp_gm,
        __gm__ uint8_t *__restrict__ tiling_para_gm)
        : mask_gm(mask_gm), o_gm(o_gm), alibi_coeff_gm(alibi_coeff_gm), s_gm(s_gm), p_gm(p_gm), o_tmp_gm(o_tmp_gm),
          tiling_para_gm(tiling_para_gm)
    {
        SetFftsBaseAddr((unsigned long)sync);
        SetAtomicnone();
        SetMasknorm();
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);

        this->sub_block_idx = GetSubBlockidx();
        this->batch_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm));
        this->max_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1));
        this->q_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2));
        this->embd = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3));
        this->tor = (float)(*((__gm__ float *)tiling_para_gm + 5));
        this->head_stride = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 6));
        this->mask_stride = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 7));
        this->is_triu_mask = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 8));
        this->total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9));
        this->isClamp = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 10));
        this->clampMin = (float)(*((__gm__ float *)tiling_para_gm + 11));
        this->clampMax = (float)(*((__gm__ float *)tiling_para_gm + 12));
        this->tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 14));
        this->tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 15));
        this->tilingKey = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 16));
        this->long_seq = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 17));
        this->is_sqrt = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 19));
        this->mask_type = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 20));
        this->alibi_compress_offset = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 21));
        this->alibi_left_align = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 22));
        this->quantType = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 24));
        this->data_shape_type = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 25));
        this->max_q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 29));
        this->stride_qo = (uint64_t)q_heads * embd;
        if (this->data_shape_type == 1) {
            this->stride_qo = embd;
        }
        if constexpr (splitm) {
            this->tmp_times = 2;
        }
        this->__k = embd;
        this->round_k = (__k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID1);
        SET_FLAG(MTE3, MTE2, EVENT_ID2);
        SET_FLAG(V, MTE2, EVENT_ID0);
        SET_FLAG(V, MTE2, EVENT_ID1);
        SET_FLAG(V, MTE2, EVENT_ID2);
        SET_FLAG(V, MTE2, EVENT_ID6);
        SET_FLAG(V, MTE2, EVENT_ID7);
        SET_FLAG(MTE3, V, EVENT_ID0);
    }
    __aicore__ __attribute__((always_inline)) inline ~FlashAttentionEncoderHighPrecisionVecOpt()
    {
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        WAIT_FLAG(V, MTE2, EVENT_ID1);
        WAIT_FLAG(V, MTE2, EVENT_ID2);
        WAIT_FLAG(V, MTE2, EVENT_ID6);
        WAIT_FLAG(V, MTE2, EVENT_ID7);
        WAIT_FLAG(MTE3, V, EVENT_ID0);
        PIPE_BARRIER(ALL);
        PIPE_BARRIER(ALL);
    }
    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        uint64_t cur_batch = 0;
        uint64_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 13 + offset_tiling));
        uint64_t process_num = (uint64_t)total_q_blk_num * q_heads;
        float mask_coff = std::is_same_v<MASK_DTYPE, half> ? HALF_MASK_COFF : BF16_MASK_COFF;
        mask_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ MASK_DTYPE *>(mask_gm));
        o_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ O_DTYPE *>(o_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ S_DTYPE *>(s_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ P_DTYPE *>(p_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(o_tmp_gm));
        if constexpr (MASK_TYPE == MaskType::MASK_TYPE_CAUSAL_MASK) {
            GenMaskMatrix(mask16_ubuf_tensor.template ReinterpretCast<float>(), mask_coff);
        }
        uint64_t next_process = 0;
        for (uint64_t process = block_idx; process < process_num; process = next_process) {
            while (process >= (uint64_t)cur_total_q_blk_num * q_heads) {
                cur_batch++;
                pre_total_q_blk_num = cur_total_q_blk_num;
                offset_tiling += tiling_para_size;
                cur_total_q_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 13 + offset_tiling));
            }
            next_process = process + block_num;
            if (is_triu_mask) {
                uint32_t curr_iter = process / block_num;
                next_process = curr_iter % 2 == 1 ? (curr_iter + 1) * block_num + block_idx : (curr_iter + 2) * block_num - 1 - block_idx;
            }
            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            if (q_seqlen == 0 || kv_seqlen == 0) {
                continue;
            }
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_o_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 10 + offset_tiling));
            uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 11 + offset_tiling));
            uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);

            uint64_t process_idx = process - pre_total_q_blk_num * q_heads;
            uint32_t m_idx = process_idx / q_heads;
            uint64_t head_idx = process_idx % q_heads;

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qk_m = (m_idx == (m_loop - 1)) ? (q_seqlen - m_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t sub_m1 = (qk_m <= BLOCK_SIZE) ? qk_m : ((qk_m / 2 + BLOCK_SIZE - 1) / BLOCK_SIZE) * BLOCK_SIZE;
            uint32_t sub_m = (sub_block_idx == 1) ? (qk_m - sub_m1) : sub_m1;
            uint32_t sub_m_d128 = (sub_m + VECTOR_SIZE - 1) / VECTOR_SIZE;            // up aligned to 128
            uint32_t sub_m_d64 = (sub_m + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE; // up aligned to 64
            uint32_t round_sub_m = (sub_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
            uint32_t prefixLen = kv_seqlen - q_seqlen;
            uint32_t prefixIdx = prefixLen / pp_n_scalar;

            /******** pre_load *******/
            uint32_t qk_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint64_t o_offset = addr_o_scalar + head_idx * embd + m_idx * pp_m_scalar * stride_qo;
            if (data_shape_type == 1) {
                o_offset = addr_o_scalar + head_idx * embd * max_q_seqlen + m_idx * pp_m_scalar * stride_qo;
            }

            uint32_t n_end = n_loop;
            if (is_triu_mask && pp_m_scalar >= pp_n_scalar) {
                n_end = m_idx + prefixIdx + 1;
            }
            uint32_t qk_n_triu = n_end * pp_n_scalar; 
            uint32_t s_block_stack = n_end > 8 ? 4 : (n_end > 4 ? 2 : 1);
            uint32_t launch_delay = s_block_stack * 2;
            uint32_t vect_mod = 2 * launch_delay;
            uint32_t m_slice = FLOAT_VECTOR_SIZE / s_block_stack;
            uint32_t m_end = (sub_m + m_slice - 1) / m_slice;
            for (uint32_t n_idx = 0; n_idx < n_end + launch_delay; n_idx += s_block_stack) {
                if (n_idx < n_end) {
                    bool last_n_loop = n_idx + s_block_stack > n_end - 1;
                    if (last_n_loop) {
                        qk_n = qk_n_triu > kv_seqlen ? kv_seqlen - n_idx * pp_n_scalar : qk_n_triu - n_idx * pp_n_scalar;
                    } else {
                        qk_n = pp_n_scalar * s_block_stack;
                    }
                    qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                    if (sub_m == 0) {
                        WaitFlagDev(QK_READY);
                    }
                    uint32_t pingpong_flag = 0;
                    for (uint32_t m_ind = 0; m_ind < m_end; m_ind++) {
                        uint32_t row_offset = m_ind * m_slice;
                        uint32_t curr_m = m_ind == m_end - 1 ? sub_m - row_offset : m_slice;
                        uint32_t s_ub_offset = pingpong_flag * S_DB_SIZE;
                        uint64_t sp_gm_offset = (uint64_t)block_idx * TMP_SIZE * tmp_times + n_idx % vect_mod * TMP_SIZE * tmp_times / vect_mod +
                                    (uint64_t)sub_block_idx * sub_m1 * qk_round_n + row_offset * qk_round_n;
                        if (m_ind == 0) {
                            WaitFlagDev(QK_READY);
                        }
                        if (curr_m == 0) {
                            continue;
                        }
                        /* int32_t div_m = 6;
                        if constexpr (splitm) {
                            div_m = 4;
                        } */
                        int32_t div_m = 4;
                        if constexpr (MASK_TYPE == MaskType::MASK_TYPE_CAUSAL_MASK) {
                            OnlineSoftmaxStage1<S_DTYPE, EXP_DTYPE, P_DTYPE, MASK_DTYPE, MASK_TYPE> (
                                ls_ubuf_tensor[s_ub_offset], 
                                mask16_ubuf_tensor.template ReinterpretCast<float>(),
                                lm_ubuf_tensor[row_offset],
                                hm_ubuf_tensor[row_offset],
                                gm_ubuf_tensor[row_offset],
                                dm_ubuf_tensor[((n_idx / s_block_stack) % div_m) * UB_FLOAT_LINE_SIZE * tmp_times + row_offset],
                                ls_ubuf_tensor[s_ub_offset], 
                                ll_ubuf_tensor[row_offset],
                                gl_ubuf_tensor[row_offset],
                                lp_ubuf_tensor[s_ub_offset * 2], 
                                tv_ubuf_tensor,
                                s_gm_tensor[sp_gm_offset],
                                p_gm_tensor[sp_gm_offset],
                                n_idx == 0, this->tor, mask_coff, curr_m, qk_n, qk_round_n, pp_n_scalar,
                                last_n_loop, row_offset, sub_m1, pingpong_flag
                            );
                        }
                        pingpong_flag = 1 - pingpong_flag;
                    }
                    // 等不到核间同步信号
                    FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY);
                }
                if (n_idx >= launch_delay) {
                    WaitFlagDev(UPDATE_READY); // 4
                    if (sub_m == 0) {
                        continue;
                    }
                    // // *** 更新 L 和 O
                    if constexpr (splitm) {
                       // 预留splitm
                    } else {
                        if (n_idx != launch_delay) {
                            WAIT_FLAG(V, MTE2, EVENT_ID2);
                            gm_to_ub<ArchType::ASCEND_V220, float>(
                                lo_ubuf_tensor,
                                o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx - launch_delay) % vect_mod * TMP_SIZE / vect_mod +
                                                (uint64_t)sub_block_idx * sub_m1 * round_k],
                                0,                                  // sid
                                1,                                  // nBurst
                                sub_m * round_k / FLOAT_BLOCK_SIZE, // lenBurst
                                0,                                  // srcGap
                                0                                   // dstGap
                            );
                            SET_FLAG(MTE2, V, EVENT_ID2);
                            // *** dm_block = expand_to_block(dm), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                dm_ubuf_tensor[((n_idx - launch_delay) / s_block_stack % 4) * UB_FLOAT_LINE_SIZE].ReinterpretCast<uint32_t>(),
                                1,                             // dstBlockStride
                                8,                             // dstRepeatStride
                                round_sub_m / FLOAT_BLOCK_SIZE // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** go = go * dm_block
                            for (uint32_t vmul_idx = 0; vmul_idx < __k / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                                mul_v<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                      // repeat
                                    1,                          // dstBlockStride
                                    1,                          // src0BlockStride
                                    0,                          // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                           // src1RepeatStride
                                );
                            }
                            if (__k % FLOAT_VECTOR_SIZE > 0) {
                                SetVecMask(__k % FLOAT_VECTOR_SIZE);
                                mul_v<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                      // repeat
                                    1,                          // dstBlockStride
                                    1,                          // src0BlockStride
                                    0,                          // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                           // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            // *** go = lo + go
                            WAIT_FLAG(MTE2, V, EVENT_ID2);
                            add_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor, go_ubuf_tensor, lo_ubuf_tensor,
                                (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                             // dstBlockStride
                                1,                                                             // src0BlockStride
                                1,                                                             // src1BlockStride
                                8,                                                             // dstRepeatStride
                                8,                                                             // src0RepeatStride
                                8                                                              // src1RepeatStride
                            );
                            PIPE_BARRIER(V);
                            SET_FLAG(V, MTE2, EVENT_ID2);
                        } else {
                            WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
                            gm_to_ub<ArchType::ASCEND_V220, float>(
                                go_ubuf_tensor,
                                o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx - launch_delay) % vect_mod * TMP_SIZE / vect_mod +
                                    (uint64_t)sub_block_idx * sub_m1 * round_k],
                                0,                                  // sid
                                1,                                  // nBurst
                                sub_m * round_k / FLOAT_BLOCK_SIZE, // lenBurst
                                0,                                  // srcGap
                                0                                   // dstGap
                            );
                            SET_FLAG(MTE2, V, EVENT_ID3);
                            WAIT_FLAG(MTE2, V, EVENT_ID3);
                        }
                        if (n_idx + s_block_stack > n_end + launch_delay - 1)  {
                            // *** gl_block = expand_to_block(gl), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                gl_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                1,                             // dstBlockStride
                                8,                             // dstRepeatStride
                                round_sub_m / FLOAT_BLOCK_SIZE // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** go = go / gl_block
                            for (uint32_t vdiv_idx = 0; vdiv_idx < __k / FLOAT_VECTOR_SIZE; ++vdiv_idx) {
                                div_v<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                      // repeat
                                    1,                          // dstBlockStride
                                    1,                          // src0BlockStride
                                    0,                          // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                           // src1RepeatStride
                                );
                            }
                            if (__k % FLOAT_VECTOR_SIZE > 0) {
                                SetVecMask(__k % FLOAT_VECTOR_SIZE);
                                div_v<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                      // repeat
                                    1,                          // dstBlockStride
                                    1,                          // src0BlockStride
                                    0,                          // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                           // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            conv_v<ArchType::ASCEND_V220, float, O_DTYPE>(
                                go_ubuf_tensor.ReinterpretCast<O_DTYPE>(),
                                go_ubuf_tensor,
                                (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1, // dstBlockStride
                                1, // srcBlockStride
                                4, // dstRepeatStride
                                8  // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // ********************* move O to GM ************************
                            SET_FLAG(V, MTE3, EVENT_ID0);
                            WAIT_FLAG(V, MTE3, EVENT_ID0);
                            ub_to_gm_align<ArchType::ASCEND_V220, O_DTYPE>(
                                o_gm_tensor[o_offset + (uint64_t)sub_block_idx * sub_m1 * stride_qo],
                                go_ubuf_tensor.ReinterpretCast<O_DTYPE>(),
                                0,                    // sid
                                sub_m,                // nBurst
                                __k * 2,              // lenBurst
                                0,                    // leftPaddingNum
                                0,                    // rightPaddingNum
                                0,                    // srcGap
                                (stride_qo - __k) * 2 // dstGap
                            );
                            SET_FLAG(MTE3, MTE2, EVENT_ID2);
                        }
                    }
                }
            }
        }
    }

private:
    __gm__ uint8_t *__restrict__ mask_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_gm{nullptr};
    __gm__ uint8_t *__restrict__ s_gm{nullptr};
    __gm__ uint8_t *__restrict__ p_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_para_gm{nullptr};
    __gm__ uint8_t *__restrict__ alibi_coeff_gm{nullptr};
    __gm__ uint8_t *__restrict__ deq_qk_gm{nullptr};
    __gm__ uint8_t *__restrict__ off_qk_gm{nullptr};
    __gm__ uint8_t *__restrict__ quant_p_gm{nullptr};
    __gm__ uint8_t *__restrict__ deq_pv_gm{nullptr};
    __gm__ uint8_t *__restrict__ off_pv_gm{nullptr};
    __gm__ uint8_t *__restrict__ logN_gm{nullptr};

    const uint32_t ls_ubuf_offset = 0;
    const uint32_t lp_ubuf_offset = 0;
    const uint32_t ls32_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE;
    const uint32_t mask_ubuf_offset = 4 * UB_UINT8_BLOCK_SIZE;
    const uint32_t go_ubuf_offset = 4 * UB_UINT8_BLOCK_SIZE;
    const uint32_t lm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE;
    const uint32_t hm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 1 * UB_UINT8_LINE_SIZE;
    const uint32_t gm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 2 * UB_UINT8_LINE_SIZE;
    const uint32_t dm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE;
    const uint32_t ll_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 7 * UB_UINT8_LINE_SIZE;
    const uint32_t gl_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 8 * UB_UINT8_LINE_SIZE;
    const uint32_t tv_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 9 * UB_UINT8_LINE_SIZE;
    const uint32_t p_scale_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 21 * UB_UINT8_LINE_SIZE;
    const uint32_t log_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 30 * UB_UINT8_LINE_SIZE;
    const uint32_t log_ubuf_float_offset = 8 * UB_UINT8_BLOCK_SIZE + 30 * UB_UINT8_LINE_SIZE;
    const uint32_t lo_ubuf_offset = 9 * UB_UINT8_BLOCK_SIZE;
    const uint32_t mask16_ubuf_offset = 11 * UB_UINT8_BLOCK_SIZE;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<S_DTYPE> ls_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, S_DTYPE>(ls_ubuf_offset);
    AscendC::LocalTensor<P_DTYPE> lp_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, P_DTYPE>(lp_ubuf_offset);
    AscendC::LocalTensor<EXP_DTYPE> ls32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, EXP_DTYPE>(ls32_ubuf_offset);
    AscendC::LocalTensor<S_DTYPE> mask_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, S_DTYPE>(mask_ubuf_offset);
    AscendC::LocalTensor<float> lo_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lo_ubuf_offset);
    AscendC::LocalTensor<float> lm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lm_ubuf_offset);
    AscendC::LocalTensor<float> hm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(hm_ubuf_offset);
    AscendC::LocalTensor<float> gm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gm_ubuf_offset);
    AscendC::LocalTensor<float> dm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(dm_ubuf_offset);
    AscendC::LocalTensor<float> ll_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_offset);
    AscendC::LocalTensor<float> gl_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gl_ubuf_offset);
    AscendC::LocalTensor<float> tv_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tv_ubuf_offset);
    AscendC::LocalTensor<float> p_scale_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(p_scale_ubuf_offset);
    AscendC::LocalTensor<float> go_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(go_ubuf_offset);
    AscendC::LocalTensor<MASK_DTYPE> mask16_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, MASK_DTYPE>(mask16_ubuf_offset);

    AscendC::LocalTensor<half> log_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(log_ubuf_offset);
    AscendC::LocalTensor<float> log_ubuf_float_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(log_ubuf_float_offset);


    AscendC::GlobalTensor<MASK_DTYPE> mask_gm_tensor;
    AscendC::GlobalTensor<O_DTYPE> o_gm_tensor;
    AscendC::GlobalTensor<S_DTYPE> s_gm_tensor;
    AscendC::GlobalTensor<P_DTYPE> p_gm_tensor;
    AscendC::GlobalTensor<float> o_tmp_gm_tensor;
    AscendC::GlobalTensor<S_DTYPE> logN_gm_tensor;
    AscendC::GlobalTensor<float> logN_float_gm_tensor;

    uint32_t batch_size{0};
    uint32_t max_seqlen{0};
    uint32_t max_q_seqlen{0};
    uint32_t q_heads{0};
    uint32_t embd{0};
    float tor{0};
    uint32_t head_stride{0};
    uint32_t mask_stride{0};
    uint32_t is_triu_mask{0};
    uint32_t total_q_blk_num{0};
    uint32_t isClamp{0};
    float clampMin;
    float clampMax;
    uint64_t stride_qo{0};
    uint32_t __k{0};
    uint32_t round_k{0};
    uint32_t tmp_times{1};
    int32_t sub_block_idx{-1};
    uint32_t tilingKey{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t long_seq{0};
    uint32_t is_sqrt{0};
    uint32_t mask_type{0};
    uint32_t alibi_compress_offset{0};
    uint32_t alibi_left_align{0};
    uint32_t data_shape_type{0};
    uint32_t quantType{0};
};

#endif

extern "C" __global__ __aicore__ void unpad_flashattention_bf16_prefix_cache(
    __gm__ uint8_t *__restrict__ sync,
    __gm__ uint8_t *__restrict__ q_gm,
    __gm__ uint8_t *__restrict__ k_gm,
    __gm__ uint8_t *__restrict__ v_gm,
    __gm__ uint8_t *__restrict__ block_table_gm,
    __gm__ uint8_t *__restrict__ mask_gm,
    __gm__ uint8_t *__restrict__ alibi_coeff_gm,
    __gm__ uint8_t *__restrict__ o_gm,
    __gm__ uint8_t *__restrict__ s_gm,
    __gm__ uint8_t *__restrict__ p_gm,
    __gm__ uint8_t *__restrict__ o_tmp_gm,
    __gm__ uint8_t *__restrict__ upo_tmp_gm,
    __gm__ uint8_t *__restrict__ tiling_para_gm)
{
    if (TILING_KEY_IS(26)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<__bf16> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<__bf16, __bf16, __bf16, MaskType::MASK_TYPE_ALIBI_COMPRESS> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                             tiling_para_gm);
        fa_vec.Run();
#endif
    } else if (TILING_KEY_IS(30)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<__bf16> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<__bf16, __bf16, __bf16, MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run();
#endif
    } else if (TILING_KEY_IS(38)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<__bf16> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<__bf16, __bf16, __bf16, MaskType::MASK_TYPE_ALIBI_COMPRESS_128> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run();
#endif
    } else if (TILING_KEY_IS(24)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<half> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<half, half, half, MaskType::MASK_TYPE_ALIBI_COMPRESS> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run<false>();
#endif
    } else if (TILING_KEY_IS(28)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<half> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<half, half, half, MaskType::MASK_TYPE_ALIBI_COMPRESS_SQRT> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run<false>();
#endif
    } else if (TILING_KEY_IS(36)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<half> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<half, half, half, MaskType::MASK_TYPE_ALIBI_COMPRESS_128> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run<false>();
#endif
    } else if (TILING_KEY_IS(34)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<__bf16> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<__bf16, __bf16, __bf16, MaskType::MASK_TYPE_ALIBI_COMPRESS_LEFT_ALIGN> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run();
#endif
    } else if (TILING_KEY_IS(32)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<half> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<half, half, half, MaskType::MASK_TYPE_ALIBI_COMPRESS_LEFT_ALIGN> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run<false>();
#endif
    } else if (TILING_KEY_IS(10)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<__bf16> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<__bf16, __bf16, __bf16, MaskType::MASK_TYPE_ALIBI> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run();
#endif
    } else if (TILING_KEY_IS(8)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<half> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<half, half, half, MaskType::MASK_TYPE_ALIBI> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run<false>();
#endif
    } else if (TILING_KEY_IS(6)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<__bf16> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<__bf16, __bf16, __bf16, MaskType::MASK_TYPE_TRIU> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run();
#endif
    } else if (TILING_KEY_IS(4)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<half> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<half, half, half, MaskType::MASK_TYPE_TRIU> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run<false>();
#endif
    } else if (TILING_KEY_IS(2)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<__bf16> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<__bf16, __bf16, __bf16, MaskType::MASK_TYPE_NONE> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run();
#endif
    } else if (TILING_KEY_IS(0)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecision<half> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVec<half, half, half, MaskType::MASK_TYPE_NONE> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                           tiling_para_gm);
        fa_vec.Run<false>();
#endif
    } else if (TILING_KEY_IS(1048577)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecisionCubeOpt<half> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVecOpt<float, half, half, half, float, MaskType::MASK_TYPE_CAUSAL_MASK, ScaleType::SCALE_TOR> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                             tiling_para_gm);
        fa_vec.Run();
#endif
    } else if (TILING_KEY_IS(1048576)) {
#ifdef __DAV_C220_CUBE__
        FlashAttentionEncoderHighPrecisionCubeOpt<__bf16> fa_cube(sync, q_gm, k_gm, v_gm, block_table_gm, s_gm, p_gm, o_tmp_gm, tiling_para_gm);
        fa_cube.Run();
#elif __DAV_C220_VEC__
        FlashAttentionEncoderHighPrecisionVecOpt<float, __bf16, __bf16, __bf16, float, MaskType::MASK_TYPE_CAUSAL_MASK, ScaleType::SCALE_TOR> fa_vec(sync, mask_gm, alibi_coeff_gm, o_gm, s_gm, p_gm, o_tmp_gm,
                                                             tiling_para_gm);
        fa_vec.Run();
#endif
    }
}
