/*
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */
#include "collectives.cce"

template<typename T>
__attribute__((always_inline)) inline __aicore__ void LcalReduceScatterBigDataOrigin(
    __gm__ T* buff[8], __gm__ T *input, __gm__ T *output, uint64_t processedNum, int64_t blockNumPerGroup, uint32_t rank,
    uint32_t rankSize, int64_t allLen, int64_t len, int64_t magic, __ubuf__ int64_t* ctrlFlagsUB, __ubuf__ int64_t* ctrlFlagsUB1,
    __ubuf__ int64_t* ctrlFlagsUB2, __ubuf__ T *inputUB[2], int64_t dataOffsetNum, int64_t flagOffset1st, int64_t flagOffset2nd,
    int64_t x, int64_t corePerRank, int64_t coreSegmentedIdx, int op)
{
    const int64_t inputNum = len * rankSize;
    const int64_t avgNumDMAPerCore = len / corePerRank;
    int64_t dataNumRemain = avgNumDMAPerCore;

    int64_t inputOffsetNum = coreSegmentedIdx * avgNumDMAPerCore;
    if (coreSegmentedIdx == corePerRank - 1) {
        dataNumRemain = len - inputOffsetNum;
    }
    if (dataNumRemain <= 0) {
        return;
    }

    if (GetBlockIdx() < blockNumPerGroup) {
        __gm__ int64_t* ctrlFlagsGM = (__gm__ int64_t*)buff[rank] + flagOffset1st;
        __gm__ T *receiveBuff = (__gm__ T*)((__gm__ int64_t*)buff[rank] + dataOffsetNum);
        int64_t ipcBuffOffsetNum = x * len + inputOffsetNum;
        input2BuffRankMagic(dataNumRemain * sizeof(T), inputUB[0], receiveBuff, ipcBuffOffsetNum,
                            input, inputOffsetNum, ctrlFlagsUB, ctrlFlagsGM, magic);
        return;
    }

    if (x == rank) {
        __gm__ int64_t* ctrlFlagsGM = (__gm__ int64_t*)buff[rank] + flagOffset2nd;
        input2BuffRankMagic(dataNumRemain * sizeof(T), inputUB[0], output, inputOffsetNum,
                            input, inputOffsetNum, ctrlFlagsUB, ctrlFlagsGM, magic);
        return;
    }

    *ctrlFlagsUB = 0;
    *ctrlFlagsUB1 = 0;
    *ctrlFlagsUB2 = 0;
    __gm__ int64_t* ctrlFlagsGM = (__gm__ int64_t*)buff[rank] + (coreSegmentedIdx + rank * corePerRank + GetLcalBlockNum()) * MEM_DMA_UNIT_INT_NUM;
    __gm__ int64_t* ctrlFlagsGMX = (__gm__ int64_t*)buff[x] + (coreSegmentedIdx + rank * corePerRank) * MEM_DMA_UNIT_INT_NUM;
    int64_t allDataSizeNeed2Add = dataNumRemain * sizeof(T);
    AscendC::PipeBarrier<PIPE_ALL>();
    while (true) {
        if (*ctrlFlagsUB * DMA_SIZE_PER_FLAG >= allDataSizeNeed2Add) {
            break;
        }

        CpGM2UB(ctrlFlagsUB1, ctrlFlagsGM, sizeof(int64_t));
        CpGM2UB(ctrlFlagsUB2, ctrlFlagsGMX, sizeof(int64_t));
        AscendC::PipeBarrier<PIPE_ALL>();

        if (((*ctrlFlagsUB1 >> 10) != (magic >> 10)) || ((*ctrlFlagsUB2 >> 10) != (magic >> 10))) {
            continue;
        }
        if (*ctrlFlagsUB1 == 0 || *ctrlFlagsUB2 == 0) {
            continue;
        }

        int64_t preparedDataGroupCount = (*ctrlFlagsUB1 <= *ctrlFlagsUB2) ? (*ctrlFlagsUB1 - magic) : (*ctrlFlagsUB2 - magic);
        if (preparedDataGroupCount <= 0 || *ctrlFlagsUB >= preparedDataGroupCount) {
            continue;
        }

        int64_t dataSizeRemain = (preparedDataGroupCount - *ctrlFlagsUB) * DMA_SIZE_PER_FLAG;
        if (preparedDataGroupCount * DMA_SIZE_PER_FLAG > allDataSizeNeed2Add) {
            dataSizeRemain = allDataSizeNeed2Add - *ctrlFlagsUB * DMA_SIZE_PER_FLAG;
        }
        int64_t ipcBuffOffsetNum = rank * len + inputOffsetNum + (*ctrlFlagsUB) * DMA_SIZE_PER_FLAG / sizeof(T);
        int64_t outputOffsetNum = inputOffsetNum + (*ctrlFlagsUB) * DMA_SIZE_PER_FLAG / sizeof(T);

        ProcessDataNew<T>(dataSizeRemain, inputUB, buff[x], dataOffsetNum, ipcBuffOffsetNum, output, outputOffsetNum, op);
        AscendC::PipeBarrier<PIPE_ALL>();

        *ctrlFlagsUB = preparedDataGroupCount;
        AscendC::PipeBarrier<PIPE_ALL>();
    }
}

template<typename T>
__attribute__((always_inline)) inline __aicore__ void LcalReduceScatterBigData(ALLREDUCE_ARGS_FUN(T))
{
    DumpLcclLogInfo(dumpAddr, LogId::INIT, static_cast<Op>(op));
    magic *= 1024;
    const int64_t dataOffsetNum = GetLcalBlockNum() * 2 * MEM_DMA_UNIT_INT_NUM;
    int64_t flagOffset1st = MEM_DMA_UNIT_INT_NUM * GetBlockIdx();
    __gm__ T* buff[8] = {
        buff0, buff1, buff2, buff3,
        buff4, buff5, buff6, buff7
    };
    __ubuf__ int64_t* ctrlFlagsUB = (__ubuf__ int64_t*)(0);
    __ubuf__ int64_t* ctrlFlagsUB1 = (__ubuf__ int64_t*)(32);
    __ubuf__ int64_t* ctrlFlagsUB2 = (__ubuf__ int64_t*)(64);
    __ubuf__ T* inputUB[2] = {(__ubuf__ T*)(96), (__ubuf__ T*)(97440)};

    int64_t blockNumPerGroup = GetLcalBlockNum() >> 1;
    int64_t corePerRank = blockNumPerGroup / rankSize;
    int64_t coreSegmentedIdx = GetBlockIdx() % corePerRank;
    int64_t x = GetBlockIdx() / corePerRank;
    if (GetBlockIdx() >= blockNumPerGroup) {
        x = (GetBlockIdx() - blockNumPerGroup) / corePerRank;
        flagOffset1st = (GetBlockIdx() - blockNumPerGroup) * MEM_DMA_UNIT_INT_NUM;
    }
    int64_t flagOffset2nd = GetLcalBlockNum() * MEM_DMA_UNIT_INT_NUM + flagOffset1st;

    int64_t ipcBuffMaxNum = IPC_BUFF_MAX_SIZE / sizeof(T);
    int64_t ipcBuffMaxNumPerRank = ipcBuffMaxNum / rankSize;
    int64_t dataLen = len;

    DumpLcclLogInfo(dumpAddr, LogId::INIT, static_cast<Op>(op));
    DumpLcclLogInfo(dumpAddr, LogId::PROCESS, static_cast<Op>(op));
    for (int64_t i = 0; i < CeilDiv(dataLen, ipcBuffMaxNumPerRank); i++) {
        *ctrlFlagsUB = 0;
        AscendC::PipeBarrier<PIPE_ALL>();

        int64_t processedNum = i * ipcBuffMaxNumPerRank;
        int64_t remainNum = (dataLen - processedNum < ipcBuffMaxNumPerRank) ? dataLen - processedNum : ipcBuffMaxNumPerRank;

        PostSyncBigData<T>(ctrlFlagsUB, buff, rank, rankSize, dataOffsetNum, ipcBuffMaxNum, magic, i);
        LcalReduceScatterBigDataOrigin<T>(
            buff, input + len * x + processedNum, output + processedNum, processedNum, blockNumPerGroup, rank, rankSize,
            len, remainNum, (magic + i) * 1024, ctrlFlagsUB, ctrlFlagsUB1, ctrlFlagsUB2, inputUB, dataOffsetNum,
            flagOffset1st, flagOffset2nd, x, corePerRank, coreSegmentedIdx, op);
        AscendC::PipeBarrier<PIPE_ALL>();
    }
    DumpLcclLogInfo(dumpAddr, LogId::PROCESS, static_cast<Op>(op));
}