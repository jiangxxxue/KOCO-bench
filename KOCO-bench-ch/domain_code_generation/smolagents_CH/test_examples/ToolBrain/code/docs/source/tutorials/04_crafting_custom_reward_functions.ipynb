{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025a9950-430f-47f5-8613-2c19428ff31c",
   "metadata": {},
   "source": [
    "# Tutorial 3: Crafting Custom Reward Functions\n",
    "\n",
    "Reward functions are the heart of reinforcement learning in ToolBrain. They are the mechanism by which you teach an agent what constitutes a \"good\" or \"bad\" outcome. A well-crafted reward function is critical for shaping the agent's behavior toward your desired goal.\n",
    "\n",
    "This tutorial covers ToolBrain's built-in rewards, the power of LLM-as-a-Judge, and how to create your own custom reward function.\n",
    "\n",
    "## Built-in Reward Functions\n",
    "\n",
    "ToolBrain comes with several pre-built reward functions that cover common use cases. You can find them in `toolbrain.rewards`.\n",
    "\n",
    "- **`reward_exact_match`**: This is the simplest and most common reward function. It gives a reward of `1.0` if the agent's `final_answer` is identical to the `gold_answer` you provide, and `0.0` otherwise. It's great for tasks with a single, definitive correct answer.\n",
    "\n",
    "    ```python\n",
    "    # Usage\n",
    "    brain = Brain(agent, reward_func=reward_exact_match)\n",
    "    ```\n",
    "\n",
    "- **`reward_tool_execution_success`**: This function rewards the agent simply for using a tool *without causing an error*. It returns `1.0` if the `tool_output` in a turn does not contain an error message, and `0.0` if it does. This is useful for encouraging the agent to learn the correct syntax and parameters for its tools, even if the final answer isn't perfect yet.\n",
    "\n",
    "- **`reward_combined`**: This powerful function allows you to mix multiple reward functions together, each with its own weight. This is perfect for balancing different objectives. For example, you might want to reward the agent for both getting the right answer *and* using its tools efficiently.\n",
    "\n",
    "    ```python\n",
    "    from toolbrain.rewards import reward_combined\n",
    "\n",
    "    def custom_combined_reward(trace, **kwargs):\n",
    "        # 70% weight on correctness, 30% on successful tool use\n",
    "        weights = {\n",
    "            \"exact_match\": 0.7,\n",
    "            \"tool_success\": 0.3,\n",
    "        }\n",
    "        kwargs[\"weights\"] = weights\n",
    "        return reward_combined(trace, **kwargs)\n",
    "\n",
    "    brain = Brain(agent, reward_func=custom_combined_reward)\n",
    "    ```\n",
    "\n",
    "### A Special Mention: LLM-as-a-Judge\n",
    "\n",
    "For complex tasks where a simple `exact_match` is insufficient, you can use another powerful LLM to act as a \"judge.\" The **`reward_llm_judge_via_ranking`** function does exactly this.\n",
    "\n",
    "When using this reward, the `Brain` collects multiple traces from the agent for the same query. It then presents these traces to a judge model (e.g., GPT-4, Gemini), which ranks them from best to worst. The rewards are then assigned based on this ranking. This is useful for tasks that require nuanced evaluation of quality, relevance, or style.\n",
    "\n",
    "```python\n",
    "# From examples/10_llm_as_judge.py\n",
    "brain = Brain(\n",
    "    agent,\n",
    "    algorithm=\"GRPO\",\n",
    "    reward_func=reward_llm_judge_via_ranking,\n",
    "    judge_model_id=\"gemini/gemini-1.5-flash\", # Specify the judge model\n",
    "    num_group_members=3 # Collect 3 traces to be ranked\n",
    ")\n",
    "```\n",
    "\n",
    "## Creating a Custom Reward Function\n",
    "\n",
    "Creating your own reward function is easy. All you need to do is define a Python function that adheres to the `RewardFunction` protocol.\n",
    "\n",
    "### The `RewardFunction` Protocol\n",
    "\n",
    "A valid reward function must:\n",
    "1.  Accept a `trace: Trace` as its first argument.\n",
    "2.  Accept `**kwargs: Any` to catch any other data passed by the `Brain` (like `gold_answer`).\n",
    "3.  Return a `float` value. By convention, this score is often between 0.0 and 1.0, but any real-valued score will work.\n",
    "\n",
    "### Hands-On: Rewarding Accuracy in HPO\n",
    "\n",
    "Let's look at the example from `examples/02_lightgbm_hpo_training_with_grpo/run_hpo_training.py`. In this task, the agent performs hyperparameter optimization (HPO) by calling a `run_lightgbm` tool. The tool returns the model's prediction accuracy.\n",
    "\n",
    "We want to reward the agent for finding parameters that lead to higher accuracy. A simple `exact_match` won't work here. We need a custom reward function.\n",
    "\n",
    "Here is the function from the example:\n",
    "\n",
    "```python\n",
    "from toolbrain import Trace\n",
    "from typing import Any\n",
    "\n",
    "# Customised reward function\n",
    "def reward_accuracy(trace: Trace, **kwargs: Any) -> float:\n",
    "    for turn in trace:\n",
    "        try:\n",
    "            # The tool's output is the accuracy score\n",
    "            reward = float(turn[\"action_output\"]) \n",
    "            return reward\n",
    "        except:\n",
    "            # If the tool call failed, the agent gets no reward\n",
    "            reward = 0.0\n",
    "    return reward\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1.  It iterates through the `trace`.\n",
    "2.  It inspects the `action_output` (which is the same as `tool_output`) of each `Turn`.\n",
    "3.  It tries to convert the output to a float. If successful, it means the tool ran correctly and returned an accuracy score. This score is directly used as the reward.\n",
    "4.  If the `action_output` cannot be converted to a float (e.g., it's an error message), it means the tool call failed, and the agent receives a reward of `0.0`.\n",
    "\n",
    "### Integrating the Custom Reward\n",
    "\n",
    "Using your new function is as simple as passing it to the `Brain` during initialization:\n",
    "\n",
    "```python\n",
    "brain = Brain(\n",
    "    agent=my_agent,\n",
    "    reward_func=reward_accuracy, # Pass the custom function here\n",
    "    algorithm=\"GRPO\"\n",
    ")\n",
    "\n",
    "brain.train(training_dataset, num_iterations=10)\n",
    "```\n",
    "\n",
    "With this setup, the GRPO algorithm will favor agent behaviors (i.e., choices of `feature_fraction`) that result in higher accuracy scores, effectively teaching the agent to perform HPO.\n",
    "\n",
    "---\n",
    "\n",
    "By mastering custom reward functions, you unlock the full potential of ToolBrain to shape agent behavior for virtually any task you can imagine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f392f71-eae9-43fd-a7aa-7a0fb19c634f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
