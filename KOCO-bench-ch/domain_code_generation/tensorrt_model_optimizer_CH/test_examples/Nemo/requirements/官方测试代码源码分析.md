# NeMo æ ¸å¿ƒå‡½æ•°å®˜æ–¹æµ‹è¯•ä»£ç æºç åˆ†æ

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†ææ¯ä¸ªå®˜æ–¹æµ‹è¯•ä»£ç æ˜¯å¦‚ä½•è°ƒç”¨ç›®æ ‡æ ¸å¿ƒå‡½æ•°çš„ï¼ŒåŒ…æ‹¬æºç è¯æ®å’Œè°ƒç”¨é“¾åˆ†æã€‚

---

## 1. FUNCTION: Quantizer.quantize

### æµ‹è¯•ä»£ç 1: tests/export/test_quantizer.py

**è°ƒç”¨ä½ç½®**: ç¬¬78è¡Œ

**æºç ç‰‡æ®µ**:
```python
# ç¬¬67-81è¡Œ
@patch('nemo.export.quantize.quantizer.dist')
def test_quantize_method(self, mock_dist, basic_quantization_config, basic_export_config):
    mock_dist.get_rank.return_value = 0

    # Create mock model and forward loop
    mock_model = MagicMock()
    mock_forward_loop = MagicMock()

    quantizer = Quantizer(basic_quantization_config, basic_export_config)

    with patch('modelopt.torch.quantization.quantize') as mock_quantize:
        with patch('modelopt.torch.quantization.print_quant_summary'):
            quantizer.quantize(mock_model, mock_forward_loop)  # â† ç¬¬78è¡Œï¼šç›´æ¥è°ƒç”¨

            # Verify quantize was called with correct arguments
            mock_quantize.assert_called_once_with(mock_model, QUANT_CFG_CHOICES['int8'], mock_forward_loop)
```

**åˆ†æç»“è®º**: âœ… **ç¡®è®¤æœ‰æ•ˆ**
- è¿™æ˜¯ä¸€ä¸ªå•å…ƒæµ‹è¯•ï¼Œç›´æ¥æµ‹è¯• `Quantizer.quantize()` æ–¹æ³•
- ä½¿ç”¨ mock å¯¹è±¡æ¨¡æ‹Ÿæ¨¡å‹å’Œ forward_loop
- éªŒè¯äº† quantize æ–¹æ³•çš„è°ƒç”¨å‚æ•°

---

### æµ‹è¯•ä»£ç 2: examples/nlp/language_modeling/megatron_gpt_ptq.py

**è°ƒç”¨ä½ç½®**: ç¬¬105è¡Œ

**æºç ç‰‡æ®µ**:
```python
# ç¬¬73-107è¡Œ
# Initialize quantizer
quantizer = Quantizer(cfg.quantization, cfg.export)

# ... æ¨¡å‹åŠ è½½ä»£ç  ...

if cfg.quantization.algorithm is not None:
    data_iter = get_calib_data_iter(...)
    dataloader = [data for data in data_iter]

    def forward_loop(model):
        model.set_inference_config(OmegaConf.to_container(cfg.inference))
        for i, batch in enumerate(tqdm(dataloader, desc="Calibrating")):
            model.predict_step(batch, i)

    model = quantizer.quantize(model, forward_loop)  # â† ç¬¬105è¡Œï¼šå®é™…è°ƒç”¨

quantizer.export(model)
```

**åˆ†æç»“è®º**: âœ… **ç¡®è®¤æœ‰æ•ˆ**
- è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯é‡åŒ–ç¤ºä¾‹è„šæœ¬
- çœŸå®è°ƒç”¨ `quantizer.quantize()` æ–¹æ³•è¿›è¡Œæ¨¡å‹é‡åŒ–
- æä¾›äº†å®é™…çš„æ ¡å‡†æ•°æ®å’Œ forward_loop å®ç°
- æœ€é€‚åˆä½œä¸ºé›†æˆæµ‹è¯•å‚è€ƒ

---

## 2. FUNCTION: prune_language_model

### æµ‹è¯•ä»£ç 1: examples/nlp/language_modeling/megatron_gpt_prune.py

**è°ƒç”¨ä½ç½®**: ç¬¬164ã€171è¡Œ

**æºç ç‰‡æ®µ**:
```python
# ç¬¬159-177è¡Œ
drop_layers = OmegaConf.to_object(cfg.prune.drop_layers)
if drop_layers:
    assert not export_config
    mtp.plugins.megatron.drop_mcore_gpt_layers(model.model, layers_to_drop=drop_layers)  # â† ç¬¬164è¡Œ
    setattr(model.cfg, "num_layers", model.model.config.num_layers)
else:
    assert cfg.model.tensor_model_parallel_size == 1
    
    mtp.prune(  # â† ç¬¬171è¡Œï¼šè°ƒç”¨ mtp.pruneï¼Œä¸æ˜¯ prune_language_model
        model,
        mode="mcore_gpt_minitron",
        constraints={"export_config": export_config},
        dummy_input=None,
        config={"forward_loop": forward_loop},
    )
```

**åˆ†æç»“è®º**: âŒ **æ— ç›´æ¥è°ƒç”¨**
- è¿™ä¸ªè„šæœ¬ä½¿ç”¨çš„æ˜¯ **NeMo 1.0 é£æ ¼çš„ API**
- ç›´æ¥è°ƒç”¨äº†åº•å±‚çš„ `mtp.plugins.megatron.drop_mcore_gpt_layers` å’Œ `mtp.prune`
- **ä¸æ˜¯**è°ƒç”¨ç›®æ ‡å‡½æ•° `prune_language_model`
- ä½†æ˜¯å®ç°é€»è¾‘ä¸ `prune_language_model` å®Œå…¨ä¸€è‡´ï¼ˆå¯ä»¥ä½œä¸ºå‚è€ƒå®ç°ï¼‰

---

### æµ‹è¯•ä»£ç 2: scripts/llm/gpt_prune.py

**è°ƒç”¨ä½ç½®**: ç¬¬119è¡Œ

**æºç ç‰‡æ®µ**:
```python
# ç¬¬104-133è¡Œ
def main(args):
    pruning_config = PruningConfig(
        target_ffn_hidden_size=args.target_ffn_hidden_size,
        # ... å…¶ä»–é…ç½® ...
    )

    data_module = get_data_module(args) if not args.drop_layers else None

    llm.prune(  # â† ç¬¬119è¡Œï¼šè°ƒç”¨ llm.pruneï¼ˆé«˜çº§APIï¼‰
        nemo_checkpoint=args.restore_path,
        save_path=args.save_path,
        pruning_config=pruning_config,
        devices=args.devices,
        # ... å…¶ä»–å‚æ•° ...
    )
```

**è°ƒç”¨é“¾åˆ†æ**:
```
scripts/llm/gpt_prune.py:119
  â””â”€> llm.prune() 
      â””â”€> nemo/collections/llm/api.py:381
          â””â”€> prune_language_model(model, pruning_config, data, trainer)  â† ç›®æ ‡å‡½æ•°
```

**åˆ†æç»“è®º**: âœ… **é—´æ¥è°ƒç”¨ï¼ˆé€šè¿‡é«˜çº§APIï¼‰**
- è¿™æ˜¯ **NeMo 2.0 é£æ ¼çš„é«˜çº§ API** è„šæœ¬
- è°ƒç”¨ `llm.prune()` åï¼Œå†…éƒ¨ä¼šè°ƒç”¨ `prune_language_model`
- é€‚åˆä½œä¸º**ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•**å‚è€ƒ

---

### æµ‹è¯•ä»£ç 3: nemo/collections/llm/api.py

**è°ƒç”¨ä½ç½®**: ç¬¬381è¡Œ

**æºç ç‰‡æ®µ**:
```python
# ç¬¬366-387è¡Œ
def prune(
    nemo_checkpoint: str,
    save_path: str,
    pruning_config: PruningConfig,
    # ... å‚æ•° ...
) -> str:
    # ... è®¾ç½®ä»£ç  ...
    
    model, trainer = setup_trainer_and_restore_model_with_modelopt_spec(...)
    
    prune_language_model(model, pruning_config, data, trainer)  # â† ç¬¬381è¡Œï¼šç›´æ¥è°ƒç”¨
    save_pruned_model(trainer, save_path)
    
    console = Console()
    console.print(f"[green]âœ“ Pruning succeded, pruned checkpoint saved to {save_path}[/green]")
    
    return save_path
```

**åˆ†æç»“è®º**: âœ… **ç¡®è®¤æœ‰æ•ˆ**
- è¿™æ˜¯ `llm.prune()` é«˜çº§ API çš„å†…éƒ¨å®ç°
- **ç›´æ¥è°ƒç”¨**äº†ç›®æ ‡å‡½æ•° `prune_language_model`
- è¿™æ˜¯æŸ¥çœ‹ `prune_language_model` å¦‚ä½•è¢«è°ƒç”¨çš„æœ€ä½³ä½ç½®

---

### æµ‹è¯•ä»£ç 4: tests/functional_tests/L2_NeMo_2_Prune_Llama_TP1PP2.sh

**è°ƒç”¨ä½ç½®**: Shell è„šæœ¬è°ƒç”¨ gpt_prune.py

**æºç ç‰‡æ®µ**:
```bash
coverage run -a --data-file=/workspace/.coverage --source=/workspace/nemo scripts/llm/gpt_prune.py \
  --restore_path /home/TestData/nemo2_ckpt/llama_68M_v4 \
  --target_hidden_size 64 \
  --target_ffn_hidden_size 128 \
  --target_num_attention_heads 4 \
  --target_num_query_groups 4 \
  --target_num_layers 2 \
  --save_path /tmp/pruned-llama
```

**åˆ†æç»“è®º**: âœ… **é—´æ¥è°ƒç”¨ï¼ˆL2çº§åˆ«åŠŸèƒ½æµ‹è¯•ï¼‰**
- è¿™æ˜¯ä¸€ä¸ª **L2 çº§åˆ«çš„åŠŸèƒ½æµ‹è¯•è„šæœ¬**
- é€šè¿‡è°ƒç”¨ `gpt_prune.py` æ¥è§¦å‘ `prune_language_model`
- é€‚åˆéªŒè¯ç«¯åˆ°ç«¯çš„å‰ªææµç¨‹

---

## 3. FUNCTION: adjust_distillation_model_for_mcore

### æµ‹è¯•ä»£ç 1: examples/nlp/language_modeling/megatron_gpt_distillation.py

**è°ƒç”¨ä½ç½®**: ç¬¬167è¡Œ

**æºç ç‰‡æ®µ**:
```python
# ç¬¬156-169è¡Œï¼ˆmodel_provider_func æ–¹æ³•å†…ï¼‰
# [ModelOpt] Distillation mode.
distill_cfg = load_distillation_config(self.transformer_config)

# Intialize DistillationModel.
kd_config = {
    "teacher_model": (_teacher_provider, [self.cfg, copy.deepcopy(self.trainer)], {}),
    "criterion": distill_cfg["criterion"],
    "loss_balancer": distill_cfg["loss_balancer"],
}
model = mtd.convert(model, mode=[("kd_loss", kd_config)])

# Additional tweaks needed for MCore/Nemo.
adjust_distillation_model_for_mcore(model, distill_cfg)  # â† ç¬¬167è¡Œï¼šç›´æ¥è°ƒç”¨

return model
```

**åˆ†æç»“è®º**: âœ… **ç¡®è®¤æœ‰æ•ˆ**
- è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„è’¸é¦è®­ç»ƒç¤ºä¾‹
- åœ¨ `DistillationMegatronGPTModel.model_provider_func()` ä¸­è°ƒç”¨
- åœ¨åˆ›å»ºè’¸é¦æ¨¡å‹åï¼Œè°ƒç”¨æ­¤å‡½æ•°è¿›è¡Œ MCore æ¶æ„é€‚é…
- æœ€ä½³çš„**ç«¯åˆ°ç«¯è’¸é¦ç¤ºä¾‹**

---

### æµ‹è¯•ä»£ç 2: tests/functional_tests/L2_NeMo_2_Distill_Llama3_TP1PP2.sh

**è°ƒç”¨ä½ç½®**: Shell è„šæœ¬è°ƒç”¨è®­ç»ƒè„šæœ¬

**æºç ç‰‡æ®µ**:
```bash
coverage run -a --data-file=/workspace/.coverage --source=/workspace/nemo scripts/llm/gpt_train.py \
  --name nemo2_llama_distill \
  --teacher_path /home/TestData/nemo2_ckpt/llama_68M_v4 \
  --model_path /home/TestData/nemo2_ckpt/llama_68M_v4 \
  --kd_config /tmp/distill-config.yaml \
  --tp_size 1 \
  --pp_size 2 \
  --max_steps 5
```

**è°ƒç”¨é“¾åˆ†æ**:
```
L2_NeMo_2_Distill_Llama3_TP1PP2.sh
  â””â”€> scripts/llm/gpt_train.py (å¸¦ --kd_config)
      â””â”€> åˆ›å»º DistillationMegatronGPTModel
          â””â”€> model_provider_func()
              â””â”€> adjust_distillation_model_for_mcore()  â† ç›®æ ‡å‡½æ•°
```

**åˆ†æç»“è®º**: âœ… **é—´æ¥è°ƒç”¨ï¼ˆL2çº§åˆ«åŠŸèƒ½æµ‹è¯•ï¼‰**
- è¿™æ˜¯ **L2 çº§åˆ«çš„è’¸é¦åŠŸèƒ½æµ‹è¯•**
- é€šè¿‡è’¸é¦è®­ç»ƒæµç¨‹è§¦å‘å‡½æ•°è°ƒç”¨
- éªŒè¯äº†å®Œæ•´çš„è’¸é¦å·¥ä½œæµ

---

## 4. FUNCTION: teacher_provider

### æµ‹è¯•ä»£ç 1: examples/nlp/language_modeling/megatron_gpt_distillation.py

**è°ƒç”¨ä½ç½®**: ç¬¬503è¡Œï¼ˆå‡½æ•°å®šä¹‰ï¼‰ã€ç¬¬160è¡Œï¼ˆè¢«å¼•ç”¨ï¼‰

**æºç ç‰‡æ®µ**:
```python
# ç¬¬503-515è¡Œï¼š_teacher_provider å‡½æ•°å®šä¹‰
def _teacher_provider(cfg: DictConfig, trainer: Trainer) -> MCoreGPTModel:
    """Teacher model factory (must be a non-local function to pickle)."""
    logging.info("Distillation: Loading teacher weights...")
    teacher_model_cfg = _merge_model_arch_fields(cfg, cfg.kd_teacher_restore_from_path)

    model = MegatronGPTModel.restore_from(
        cfg.kd_teacher_restore_from_path,
        override_config_path=teacher_model_cfg,
        trainer=trainer,
    )
    teacher_model_module_list = model.get_model_module_list()
    logging.info("Distillation: ... teacher weights loaded.")
    return teacher_model_module_list[0]

# ç¬¬156-164è¡Œï¼š_teacher_provider è¢«å¼•ç”¨
distill_cfg = load_distillation_config(self.transformer_config)
kd_config = {
    "teacher_model": (_teacher_provider, [self.cfg, copy.deepcopy(self.trainer)], {}),  # â† è¢«å¼•ç”¨
    "criterion": distill_cfg["criterion"],
    "loss_balancer": distill_cfg["loss_balancer"],
}
model = mtd.convert(model, mode=[("kd_loss", kd_config)])
```

**åˆ†æç»“è®º**: âš ï¸ **å‚è€ƒå®ç°ï¼ˆä¸æ˜¯ç›´æ¥è°ƒç”¨ç›®æ ‡å‡½æ•°ï¼‰**
- è¿™é‡Œå®šä¹‰çš„æ˜¯ `_teacher_provider`ï¼Œä¸æ˜¯ç›®æ ‡å‡½æ•° `teacher_provider`
- ä½†æ˜¯**åŠŸèƒ½å®Œå…¨ç›¸åŒ**ï¼Œå®ç°é€»è¾‘ä¸€è‡´
- æ–‡ä»¶è·¯å¾„ä¸åŒï¼š
  - ç›®æ ‡å‡½æ•°ï¼š`nemo/collections/llm/modelopt/distill/utils.py`
  - è¿™é‡Œçš„å‡½æ•°ï¼š`examples/nlp/language_modeling/megatron_gpt_distillation.py`
- å¯ä»¥ä½œä¸º**å‚è€ƒå®ç°**ï¼Œç†è§£ teacher_provider çš„ç”¨æ³•

---

### æµ‹è¯•ä»£ç 2: tests/functional_tests/L2_NeMo_2_Distill_Llama3_TP1PP2.sh

**è°ƒç”¨ä½ç½®**: Shell è„šæœ¬è§¦å‘è’¸é¦è®­ç»ƒ

**åˆ†æç»“è®º**: âœ… **é—´æ¥è°ƒç”¨ï¼ˆé€šè¿‡è’¸é¦æµç¨‹ï¼‰**
- ä¸ `adjust_distillation_model_for_mcore` å…±äº«åŒä¸€ä¸ªæµ‹è¯•è„šæœ¬
- åœ¨è’¸é¦æ¨¡å‹åˆå§‹åŒ–æ—¶ä¼šè°ƒç”¨ teacher_provider åŠ è½½æ•™å¸ˆæ¨¡å‹

---

## 5. FUNCTION: get_tensor_shapes_adjust_fn_for_distillation

### æµ‹è¯•ä»£ç 1: nemo/lightning/megatron_parallel.py

**è°ƒç”¨ä½ç½®**: ç¬¬1449-1457è¡Œ

**æºç ç‰‡æ®µ**:
```python
# ç¬¬1439-1457è¡Œ
@property
def adjust_tensor_shapes_fn(self) -> Union[Callable, None]:
    """
    Retrieves the function to adjust send and receive tensor shapes in Megatron-Core's forward pass.

    Currently only used during non-interleaved pipelining for Distillation.

    Returns:
        Union[Callable, None]: The function which takes in tensor shapes and returns updated shapes,
                               or None if not applicable.
    """
    from nemo.collections.llm.modelopt.distill.utils import get_tensor_shapes_adjust_fn_for_distillation

    return get_tensor_shapes_adjust_fn_for_distillation(  # â† ç¬¬1451è¡Œï¼šç›´æ¥è°ƒç”¨
        self.model,
        self.seq_length,
        self.micro_batch_size,
        self.decoder_seq_length,
        self.forward_only,
    )
```

**åˆ†æç»“è®º**: âœ… **ç¡®è®¤æœ‰æ•ˆï¼ˆæ¡†æ¶å†…éƒ¨è‡ªåŠ¨è°ƒç”¨ï¼‰**
- åœ¨ `MegatronStep.adjust_tensor_shapes_fn` å±æ€§ä¸­è¢«è°ƒç”¨
- è¿™æ˜¯ä¸€ä¸ª **property å±æ€§**ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è·å–
- å½“ä½¿ç”¨æµæ°´çº¿å¹¶è¡Œçš„è’¸é¦è®­ç»ƒæ—¶ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨è°ƒç”¨æ­¤å‡½æ•°
- è¿™æ˜¯æœ€çœŸå®çš„ä½¿ç”¨åœºæ™¯

---

### æµ‹è¯•ä»£ç 2: examples/nlp/language_modeling/megatron_gpt_distillation.py

**è°ƒç”¨ä½ç½®**: é—´æ¥è§¦å‘ï¼ˆé€šè¿‡æµæ°´çº¿å¹¶è¡Œè’¸é¦ï¼‰

**åˆ†æç»“è®º**: âœ… **é—´æ¥è°ƒç”¨ï¼ˆè¿è¡Œæ—¶è‡ªåŠ¨è§¦å‘ï¼‰**
- å½“è¿è¡Œè’¸é¦è„šæœ¬æ—¶ï¼Œå¦‚æœé…ç½®äº† `pipeline_model_parallel_size > 1`
- æ¡†æ¶ä¼šè‡ªåŠ¨è°ƒç”¨ `get_tensor_shapes_adjust_fn_for_distillation`
- ä¸éœ€è¦æ˜¾å¼è°ƒç”¨ï¼Œç”± `MegatronStep` è‡ªåŠ¨å¤„ç†

---

### æµ‹è¯•ä»£ç 3: tests/functional_tests/L2_NeMo_2_Distill_Llama3_TP1PP2.sh

**è°ƒç”¨ä½ç½®**: Shell è„šæœ¬ï¼ˆPP=2 è§¦å‘ï¼‰

**æºç ç‰‡æ®µ**:
```bash
--pp_size 2  # â† Pipeline Parallel = 2ï¼Œä¼šè§¦å‘å‡½æ•°è°ƒç”¨
```

**åˆ†æç»“è®º**: âœ… **é—´æ¥è°ƒç”¨ï¼ˆL2åŠŸèƒ½æµ‹è¯•è§¦å‘ï¼‰**
- è„šæœ¬ä¸­è®¾ç½® `--pp_size 2`ï¼Œå¯ç”¨æµæ°´çº¿å¹¶è¡Œ
- åœ¨ PP > 1 çš„è’¸é¦è®­ç»ƒä¸­ï¼Œæ­¤å‡½æ•°ä¼šè¢«è‡ªåŠ¨è°ƒç”¨
- éªŒè¯äº†æµæ°´çº¿å¹¶è¡Œè’¸é¦çš„å®Œæ•´æµç¨‹

---

## ğŸ“Š æ€»ç»“ä¸å»ºè®®

### æœ‰æ•ˆçš„å®˜æ–¹æµ‹è¯•ä»£ç ï¼ˆæ¨èä½¿ç”¨ï¼‰

| å‡½æ•° | æ¨èçš„å®˜æ–¹æµ‹è¯• | ç±»å‹ | æ¨èåº¦ |
|------|---------------|------|--------|
| `Quantizer.quantize` | `tests/export/test_quantizer.py` | å•å…ƒæµ‹è¯• | â­â­â­â­â­ |
| `Quantizer.quantize` | `examples/nlp/language_modeling/megatron_gpt_ptq.py` | é›†æˆç¤ºä¾‹ | â­â­â­â­â­ |
| `prune_language_model` | `nemo/collections/llm/api.py` | å®ç°ä½ç½® | â­â­â­â­â­ |
| `prune_language_model` | `scripts/llm/gpt_prune.py` | é›†æˆè„šæœ¬ | â­â­â­â­ |
| `prune_language_model` | `tests/functional_tests/L2_NeMo_2_Prune_Llama_TP1PP2.sh` | L2æµ‹è¯• | â­â­â­â­ |
| `adjust_distillation_model_for_mcore` | `examples/nlp/language_modeling/megatron_gpt_distillation.py` | é›†æˆç¤ºä¾‹ | â­â­â­â­â­ |
| `adjust_distillation_model_for_mcore` | `tests/functional_tests/L2_NeMo_2_Distill_Llama3_TP1PP2.sh` | L2æµ‹è¯• | â­â­â­â­ |
| `teacher_provider` | `examples/nlp/language_modeling/megatron_gpt_distillation.py` | å‚è€ƒå®ç° | â­â­â­â­ |
| `get_tensor_shapes_adjust_fn_for_distillation` | `nemo/lightning/megatron_parallel.py` | æ¡†æ¶è°ƒç”¨ | â­â­â­â­â­ |
| `get_tensor_shapes_adjust_fn_for_distillation` | `examples/nlp/language_modeling/megatron_gpt_distillation.py` | è¿è¡Œæ—¶è§¦å‘ | â­â­â­â­ |

### æœ€ä½³å­¦ä¹ è·¯å¾„

1. **Quantizer.quantize**: å…ˆçœ‹å•å…ƒæµ‹è¯•ç†è§£æ¥å£ â†’ å†çœ‹é›†æˆç¤ºä¾‹ç†è§£å®é™…ç”¨æ³•
2. **prune_language_model**: ç›´æ¥çœ‹ `api.py` çš„è°ƒç”¨æ–¹å¼ â†’ å†çœ‹ L2 æµ‹è¯•éªŒè¯æµç¨‹
3. **adjust_distillation_model_for_mcore**: çœ‹å®Œæ•´çš„è’¸é¦ç¤ºä¾‹
4. **teacher_provider**: å‚è€ƒè’¸é¦ç¤ºä¾‹ä¸­çš„ `_teacher_provider` å®ç°
5. **get_tensor_shapes_adjust_fn_for_distillation**: ç†è§£æ¡†æ¶è‡ªåŠ¨è°ƒç”¨æœºåˆ¶

